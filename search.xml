<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ip查询的一些网站]]></title>
    <url>%2F2020%2F06%2F19%2Fip%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[ip查询的一些网站 ip.cn ipinfo.io cip.cc ifconfig.me myip.ipip.net 123456789101112131415161718➜ ~ curl ipinfo.io&#123; "ip": "27.102.106.127", "city": "Seoul", "region": "Seoul", "country": "KR", "loc": "37.5660,126.9784", "org": "AS45996 DAOU TECHNOLOGY", "postal": "03186", "timezone": "Asia/Seoul", "readme": "https://ipinfo.io/missingauth"&#125;➜ ~ curl ifconfig.me27.103.116.127➜ ~ curl myip.ipip.net当前 IP：221.226.112.106 来自于：中国 北京 北京 联通]]></content>
  </entry>
  <entry>
    <title><![CDATA[开启Spring Initializr个性化之旅]]></title>
    <url>%2F2020%2F06%2F18%2F%E5%BC%80%E5%90%AFSpring%20Initializr%E4%B8%AA%E6%80%A7%E5%8C%96%E4%B9%8B%E6%97%85%2F</url>
    <content type="text"><![CDATA[Every good Spring Boot project usually starts at https://start.spring.io/ — Josh Long 背景介绍，自己的项目或者公司的项目一般需要维护很多定制化的模块时，都是上传到maven私服中方便使用，但存在一个问题，每次需要相关的package需要去翻文档或者看bom，不能在建项目的时间直接引入，参考了start.spring.io，尝试搭建自己的spring initializr服务，同时整合自己的一些package，提供个性化服务，快速开发。 目标基本框架 Spring Initializr 提供核心REST API，可以整合到UI或者IDE（如Intellij IDEA），直接生成项目 https://start.spring.io 提供web界面，强依赖于 Spring Initializr，显示数据来源于 Spring Properties，定制化主要是使用 Spring Initializr 提供的SPI 除此之外，Spring.io 提供 Spring Boot metadata endpoint，Spring Initializr 会使用metadata作为外部数据源，以确保生成的Spring Boot版本是最新的 个性化 https://start.spring.io/ 虽然已经提供了非常优秀的Spring Boot Start，但在某些场景下，仍然需要做一些定制化，比如： 由于网络限制，需要搭建一个自己的实例 定制化自己的UI界面 提供一些自己的配置或依赖，如公司内部的starter Spring Initializr 是一个使用Spring Boot搭建的模块化应用，所以还是很容易扩展的 版本由于官方Spring Initializr以及提供了bom，所以我们直接基于最新的bom版本搭建即可。 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.spring.initializr&lt;/groupId&gt; &lt;artifactId&gt;initializr-bom&lt;/artifactId&gt; &lt;version&gt;0.8.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 搭建流程准备两个组件 https://github.com/spring-io/initializr https://github.com/spring-io/start.spring.io initializr 是必须的，ui界面是可选的。 个性化定制配置文件可以基于 InitializrProperties 定义 application.yml，产出核心依赖。Spring Initializr 也允许我们使用 InitializrMetadataProvider 定义metadata，因此，我们可以创建一个 CustomInitializrProperties 类 来读取不同配置文件的配置项。 123456789101112131415161718192021@Configuration@EnableConfigurationProperties(CustomInitializrProperties.class)public class CustomInitializrConfiguration &#123; @Bean public DefaultInitializrMetadataProvider customInitializrMetadataProvider(InitializrProperties initializrProperties, CustomInitializrProperties customInitializrProperties, InitializrMetadataUpdateStrategy initializrMetadataUpdateStrategy) &#123; InitializrMetadata meta = InitializrMetadataBuilder.fromInitializrProperties(customInitializrProperties.getInitializr()) .withInitializrProperties(initializrProperties, true).build(); return new DefaultInitializrMetadataProvider(meta, initializrMetadataUpdateStrategy); &#125;&#125;@Data@ConfigurationProperties("custom")public class CustomInitializrProperties &#123; @NestedConfigurationProperty InitializrProperties initializr = new InitializrProperties();&#125; 配置是通过 StartApplication 来加载的，但由于应用并没有使用组件扫描，我们需要在配置文件里进行自定义设置： 1234567891011custom: initializr: dependencies: - name: Custom Dependencies content: - name: Custom dependency id: custom-dependency groupId: your.domain artifactId: custom-artifact starter: false description: My first custom dependency for the Spring Initializr 通过这种自定义的依赖配置，我们就可以控制配置项的合并和显示顺序。 Initializr扩展通过配置文件自定义依赖，并不是总能满足我们的需求，有时候我们还需要自定义一些代码片段，这个时候就需要使用 Spring Initializr 提供的一些扩展钩子： BuildCustomizer：定义Maven/Gradle构建过程，如增加maven build插件 ProjectContributor：定义一些个性化的项目目录或者文件 MainSourceCodeCustomizer, MainCompilationUnitCustomizer, MainApplicationTypeCustomizer, TestSourceCodeCustomizer, TestApplicationTypeCustomizer：项目的源码生成或修改，而不局限于项目语言 GitIgnoreCustomizer：定义gitignore文件 HelpDocumentCustomizer：定义 HELP.md文件 ProjectDescriptionCustomizer：通常用于适应项目描述，例如自动解决框架版本和语言级别的无效组合。 举例，如果我们需要在生成项目中增加maven插件，则需要使用一种所谓的“伪”依赖（ pseudo dependency）。首先我们需要定义一个像这样的依赖： 123456789101112custom: initializr: dependencies: - name: Custom Dependencies content: - name: Custom Maven Plugin id: custom-maven-plugin groupId: your.domain artifactId: custom-maven-plugin version: 1.0.0 starter: false description: Configures custom Maven plugin integration for project scans 接着，我们定义两个 BuildCustomizer：一个用来增加maven依赖插件，一个用来移除插件。 12345678910111213141516171819202122232425@ProjectGenerationConfiguration@ConditionalOnRequestedDependency("custom-maven-plugin")public class CustomMavenPluginConfiguration &#123; @Bean public BuildCustomizer&lt;MavenBuild&gt; customPluginConfigurer() &#123; return (MavenBuild build) -&gt; &#123; build.dependencies().ids().filter(it -&gt; it.equals("custom-maven-plugin")) .findFirst() .map(r -&gt; build.dependencies().get(r)).map(r -&gt; &#123; build.plugins().add(r.getGroupId(), r.getArtifactId(), (plugin) -&gt; plugin.execution("my-execution", (first) -&gt; first.goal("scan").configuration((conf) -&gt; &#123;conf.add("failOnSeverity", "MAJOR");&#125;) )); return build; &#125;).orElse(build); &#125;; &#125; @Bean public BuildCustomizer&lt;MavenBuild&gt; customPluginDependencyRemoval() &#123; return build -&gt; build.dependencies().remove("custom-maven-plugin"); &#125;&#125; 注意使用注解，Spring Initializr 自身并不会使用这些自动化配置，而是在生成项目时使用的，但需要spring.factories注册这些配置 123io.spring.initializr.generator.project.ProjectGenerationConfiguration=\ io.spring.start.site.extension.StartProjectGenerationConfiguration, \ io.spring.start.site.CustomMavenPluginConfiguration 最终产生的pom类似这样： 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;your.domain&lt;/groupId&gt; &lt;artifactId&gt;custom-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;my-execution&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;scan&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;failOnSeverity&gt;MAJOR&lt;/failOnSeverity&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 结语抛砖引玉，这篇文章只是简单介绍了Spring Initializr的一些定制化方法，更多更好的扩展方式还需要你去发现。 git地址：https://github.com/silloy/start.silloy.me References https://docs.spring.io/initializr/docs/current-SNAPSHOT/reference/html start.aliyun.com 正式上线 How to customize the Spring Initializr]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>Spring Initializr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis的数据类型]]></title>
    <url>%2F2020%2F06%2F06%2Fredis%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[简单介绍一下redis的几种数据类型： Strings，Lists，Sets， Hashe，Sorted sets，Bitmaps and HyperLogLogs StringsStrings是redis的最基础的类型，意味着reds stings可以包含任何数据类型，普通的string，integer…自不必说，JPEG 图片，Java对象，文件等等都可以序列化成strings。 最大限制：512M 常用操作： INCR, DECR, INCRBY：通过strings实现原子化操作 APPEND：改变字符串 GETRANGE , SETRANGE： 自由获取或者变更Strings GETBIT, SETBIT：对Strings进行位操作，如通过redis创建一个bloom filter 其他： available string commands ListsRedis Lists是strings的集合list，顺序为元素的插入顺序。当然也可以通过命令在head或者tail插入元素。即使list很大，在head或者tail增删元素的速度也很快，但是如果操作中间的元素，则复杂度为O(N)。 lists最大长度： 2^32 - 1 (4294967295， 也就是每个list可以存40亿个元素) 有意思的案例： 创建设计网络中的时间线，使用 LPUSH 增加元素，而使用LRANGE获取最新插入的几个元素 使用 LPUSH 和 LTRIM 创建一个固定容量的list，只记录最新的几个元素 使用list结构创建一个消息队列，如ruby的Resque - 用于创建后台任务 BLPOP 其他： available commands operating on lists SetsRedis Sets是strings的无序无重复集合。增删查的复杂度是0(1)，基于sets可以做 unions, intersections 等操作 sets最大长度： 2^32 - 1 (4294967295， 也就是每个set可以存40亿个元素) 一些案例： trace 一些唯一事件，比如说记录一篇blog的访问ip，仅仅只需要使用 SADD ，而不需要考虑是否重复。 创建一个标签系统，给对象标记不同的tag 使用 SPOP 或者 SRANDMEMBER 命令从一个set中随机抽取元素 其他： full list of Set commands HashesRedis Hashes 是用来存储一些键值对的，举个栗子： 12345@cliHMSET user:1000 username antirez password P1pp0 age 34HGETALL user:1000HSET user:1000 password 12345HGETALL user:1000 每一个hash都可以存储至多 2^32 - 1个 键值对 其他： full list of Hash commands Sorted setsredis Sorted Sets 和sets类型相似，但最大的不同点是 Sorted Set 的每一个元素都有一个分值，这个分值可以用作排序，虽然元素是不重复的，但分值可能相同。可以获取不同分值段内的一些元素。 sorted sets 可以做很多有意思的事情，与直接操作数据库相比，有很好的性能 比如设计一个在线游戏的选手积分榜，使用ZADD来增加一个新的记录，使用 ZRANGE 获取前几名的选手，也可以使用 ZRANGE来获取一个选手的排名。使用ZRANK 和 ZRANGE可以很快查询到一个元素附近的几个元素 Sorted Sets常常被用来做作redis中的索引数据，比如有很多用户的hash数据，可以使用用户的年龄作为分值，用户id作为值，使用ZRANGEBYSCORE就可以很快查询出一个年龄区间内的用户 其他：full list of Sorted Set commands Bitmaps and HyperLogLogsRedis 同时也支持Bitmaps 和 HyperLogLogs，基于Strings实现，但有他们自己的语义。 其他：introduction to Redis data types 推荐的redisdocs： http://doc.redisfans.com/ http://redisdoc.com/ References Data types]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由一次Base64编码异常引起的小bug]]></title>
    <url>%2F2020%2F05%2F04%2F%E7%94%B1%E4%B8%80%E6%AC%A1Base64%E7%BC%96%E7%A0%81%E5%BC%82%E5%B8%B8%E5%BC%95%E8%B5%B7%E7%9A%84%E5%B0%8Fbug%2F</url>
    <content type="text"><![CDATA[base64是一种基本的加密算法，在Java中可以使用java自带的base64编码，也可以用apache 的commons-codec包。最近在使用commons-codec 1.10 版本能正常解密微信的消息，升级为1.13后出现了不能正常decode，出现异常 123456java.lang.IllegalArgumentException: Last encoded character (before the paddings if any) is a valid base 64 alphabet but not a possible value at org.apache.commons.codec.binary.Base64.validateCharacter(Base64.java:798) at org.apache.commons.codec.binary.Base64.decode(Base64.java:477) at org.apache.commons.codec.binary.BaseNCodec.decode(BaseNCodec.java:411) at org.apache.commons.codec.binary.BaseNCodec.decode(BaseNCodec.java:395) 具体场景 处理微信公众号消息时，对消息内容进行必须的加解密，出现的问题是处理aesKey时出现的，具体demo如下 1234567891011121314public WxOpenCryptUtil(WxOpenConfigStorage wxOpenConfigStorage) &#123; /* * @param token 公众平台上，开发者设置的token * @param encodingAesKey 公众平台上，开发者设置的EncodingAESKey * @param appId 公众平台appid */ String encodingAesKey = wxOpenConfigStorage.getComponentAesKey(); String token = wxOpenConfigStorage.getComponentToken(); String appId = wxOpenConfigStorage.getComponentAppId(); this.token = token; this.appidOrCorpid = appId; this.aesKey = Base64.decodeBase64(encodingAesKey + "="); &#125; 当升级commons-codec版本为1.13及以上时，会出现上述异常 出现的原因 1.13出现异常的方法 1234567private long validateCharacter(final int numBitsToDrop, final Context context) &#123; if ((context.ibitWorkArea &amp; numBitsToDrop) != 0) &#123; throw new IllegalArgumentException( "Last encoded character (before the paddings if any) is a valid base 64 alphabet but not a possible value"); &#125; return context.ibitWorkArea &gt;&gt; numBitsToDrop;&#125; 仔细分析可以看出编解码是在BaseNCodec.java是Base64和Base32的基类 可以看出唯一的差别就是在解码时对参数做了校验。有必要了解下这个参数校验做了些什么？ 12&gt; context.ibitWorkArea &amp; numBitsToDrop&gt; ibitWorkArea: 位处理的基本位数 numBitsToDrop: 应该为空的低位数目 可以看出当 context.ibitWorkArea &amp; numBitsToDrop不为0时就会抛出异常，实际上只有base64严格模式编码下，才可能会为0，松散模式不会为0 解决办法 降低版本到1.12以下可以解决该问题，或者等commons-codec版本更新到1.15，最新的源码已经处理了该问题 1234567private void validateCharacter(final int emptyBitsMask, final Context context) &#123; if (isStrictDecoding() &amp;&amp; (context.ibitWorkArea &amp; emptyBitsMask) != 0) &#123; throw new IllegalArgumentException( "Strict decoding: Last encoded character (before the paddings if any) is a valid base 64 alphabet but not a possible encoding. " + "Expected the discarded bits from the character to be zero."); &#125; &#125; 扩展 base64的严格模式和松散模式定义，直接引用源码了 Lenient: Any trailing bits are composed into 8-bit bytes where possible.The remainder are discarded.Strict: The decoding will raise an {@link IllegalArgumentException} if trailing bitsare not part of a valid encoding. Any unused bits from the final character mustbe zero. Impossible counts of entire final characters are not allowed. References 使用java8的java.util.Base64报“java.lang.IllegalArgumentException: Illegal base64 character d”的问题 Base64笔记 Base64.decode fails on Java11 for certain valid base 64 encoded String]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java8之扩展函数式接口]]></title>
    <url>%2F2020%2F05%2F04%2FJava8%E4%B9%8B%E6%89%A9%E5%B1%95%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[函数式接口先看一下官方定义 Functional interfaces provide target types for lambda expressions and method references. 可以看出函数式接口主要用于lambda表达式，这类接口只定义了唯一的抽象方法的接口（除了隐含的Object对象的公共方法），一开始也称SAM类型接口(Single Abstract Method)。 简单使用1234List&lt;Integer&gt; list = Lists.newArrayList(1,2,3); list.forEach(r -&gt; &#123; System.out.println("re = " + Math.sqrt(r)); &#125;); 看一下 foreach 实现，在Iterable.java中 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125; &#125; 这里出现的Consumer就是一个函数式接口， java8 提供了一些常用的函数式接口 Predicate – 传入一个参数，返回一个bool结果， 方法为boolean test(T t) Consumer – 传入一个参数，无返回值，纯消费。 方法为void accept(T t) Function – 传入一个参数，返回一个结果，方法为R apply(T t) Supplier – 无参数传入，返回一个结果，方法为T get() UnaryOperator – 一元操作符， 继承Function,传入参数的类型和返回类型相同。 BinaryOperator – 二元操作符， 传入的两个参数的类型和返回类型相同， 继承BiFunction 这里就不一一列举了，具体请见 java.util.function 包 都很简单，不太清楚的自行google 扩展但是jdk提供的有时候不一定能满足需求，这个时候就需要我们自定义函数式接口 普通的 Function 或者 Consumer 只能就收一个参数，BiFuntion 和 BiConsumer 也只能接受连个参数，参数更多的情况就无法满足了 以 consumer 为例，先自定义一个接口 12345678910111213@FunctionalInterface public interface TriConsumer&lt;T, U, W&gt; &#123; void accept(T t, U u, W w); default TriConsumer&lt;T, U, W&gt; andThen(TriConsumer&lt;? super T, ? super U, ? super W&gt; after) &#123; Objects.requireNonNull(after); return (l, r, w) -&gt; &#123; accept(l, r, w); after.accept(l, r, w); &#125;; &#125; &#125; 函数式接口一般使用 @FunctionalInterface 注解注释，以申明该接口是一个函数式接口， 这里提供一个 andThen 方法以支持连续调用 使用方法 1234TriConsumer&lt;Integer, Integer, Integer&gt; consumer = (a, b, c) -&gt; &#123; System.out.println(a + b + c); &#125;; consumer.accept(5,6,7); funtion类似，这里就不举例了 异常捕获 FunctionalInterface 提供的接口一般是不抛出异常的，意味着我们在使用的时候需要在方法体内部捕获异常，这里定义一种可以抛出异常的接口 1234@FunctionalInterface public interface InterfaceException&lt;T&gt; &#123; void apply(T t) throws Exception; &#125; References Java 8函数式接口functional interface的秘密]]></content>
  </entry>
  <entry>
    <title><![CDATA[补码之求相反数]]></title>
    <url>%2F2020%2F05%2F02%2F%E8%A1%A5%E7%A0%81%E4%B9%8B%E6%B1%82%E7%9B%B8%E5%8F%8D%E6%95%B0%2F</url>
    <content type="text"><![CDATA[概念[来自wikipedia]原码(True form)：一个二进制数左边加上符号位后所得到的码，且当二进制数大于0时，符号位为0；二进制数小于0时，符号位为1；二进制数等于0时，符号位可以为0或1(+0/-0) 反码(One’s complement)：一种在计算机中数的机器码表示。对于单个数值（二进制的0和1）而言，对其进行取反操作就是将0变为1，1变为0 补码(2’s complement)：一种用二进制表示有号数的方法，也是一种将数字的正负号变号的方式，主要优点是 不需因为数字的正负而使用不同的计算方式 计算正数：原码、反码、补码相同 负数：反码 — 符号位不变化，其余位数取反*，补码 — *符号位不变化，其余各位原码取反+1， 即 反码+1 测试代码如下 123456789101112131415@Test public void byteTest() &#123; reverse(0); reverse(6); reverse(-6); reverse(128); reverse(-128); reverse(1280); reverse(-1280); &#125; private Integer reverse(Integer a) &#123; System.out.printf("%5d\t%32s\t%5d\t%32s\t%5d\t%32s%n", a, Integer.toBinaryString(a), ~a, Integer.toBinaryString((~a)), ~a + 1, Integer.toBinaryString((~a + 1))); return ~a + 1; &#125; 结果 12345670 0 -1 11111111111111111111111111111111 0 0 6 110 -7 11111111111111111111111111111001 -6 11111111111111111111111111111010 -6 11111111111111111111111111111010 5 101 6 110 128 10000000 -129 11111111111111111111111101111111 -128 11111111111111111111111110000000 -128 11111111111111111111111110000000 127 1111111 128 10000000 1280 10100000000 -1281 11111111111111111111101011111111 -1280 11111111111111111111101100000000-1280 11111111111111111111101100000000 1279 10011111111 1280 10100000000 可以看出： 对一个数字直接取反，并不是其相反数，需要+1 [+0]原码=0000 0000， [-0]原码=1000 0000 [+0]反码=0000 0000， [-0]反码=1111 1111 [+0]补码=0000 0000， [-0]补码=0000 0000]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源贡献协议介绍]]></title>
    <url>%2F2020%2F04%2F05%2F%E5%BC%80%E6%BA%90%E8%B4%A1%E7%8C%AE%E5%8D%8F%E8%AE%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[众所周知，开源代码都有其开源许可证，详细选择参考如何选择开源许可证？。这里不是要讨论开源许可证的问题，而是开源贡献协议。最近在参加一个开源项目时，提到需要使用DCO贡献协议，所以就去了解当下主流的开源贡献协议。 CLA（Contributor License Agreement）协议 CLA时对开源License的法律性质补充，多由企业或者组织自行定义，作为开源协议的补充，一次性签署，比如alibaba CLA协议： Alibaba Open Source Individual CLA 使用上来说，CLA只需要签署一次，如阿里巴巴个人CLA， Google CLA，Pivotal CLA，CNCF CLA 使用CLA的用户或者组织 Facebook Eclipse Go Google InfluxDB Python Elastic CNCF ….. DCO（Developer Certificate of Origin）协议 DCO时Linux Foundation提出的，只有四条简短条文， 具体内容如下: DCO，使用上只需要开发者提交信息时追加 Signed-off-by 即可。具体内容(version1.1)如下： 该贡献全部或部分由我创建，我有权根据文件中指明的开源许可提交；要么 该贡献是基于以前的工作，这些工作属于适当的开源许可，无论这些工作全部还是部分由我完成，我有权根据相同的开源许可证（除非我被允许根据不同的许可证提交）提交修改后的工作；要么 该贡献由1、2、或 3 证明的其他人直接提供给我，而我没有对其进行修改。 我理解并同意该项目和贡献是公开的，并且该贡献的记录（包括我随之提交的所有个人信息，包括我的签字）将无限期保留，并且可以与本项目或涉及的开源许可证保持一致或者重新分配。 使用方法详见 Probot: DCO， 其实就是提交代码(commit)时增加-s参数，然后你会在提交信息里看到 123This is my commit messageSigned-off-by: DEVELOPER &lt;random@developer.example.org&gt; 使用DCO的用户或者组织 Gitlab Chef TiKv Apache SkyWalking …… 总结如果你只是一个commiter，请遵从开源项目的贡献者协议，或者发起issue请求变更，如果是自己的开源项目，更看重法律风险，建议使用CLA，看重社区合作，可以使用DCO。 References CLA vs. DCO: What’s the difference? 为何《贡献者许可协议》不利于开源社区？ Move from CLA to DCO #2649 - github.com probot/dco]]></content>
  </entry>
  <entry>
    <title><![CDATA[支付宝和微信接口加密规范调研]]></title>
    <url>%2F2020%2F04%2F05%2F%E6%94%AF%E4%BB%98%E5%AE%9D%E5%92%8C%E5%BE%AE%E4%BF%A1%E6%8E%A5%E5%8F%A3%E5%8A%A0%E5%AF%86%E8%A7%84%E8%8C%83%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[支付宝(ISV)参考文档：https://docs.open.alipay.com/291/106115 签名方式：RSA2 具体实现 初始化 AlipayClient 1AlipayClient alipayClient = new DefaultAlipayClient(gateway,app_id,private_key,"json",charset,alipay_public_key,sign_type); 具体签名过程 获取encryptor，加密关键参数部分 12String encryptContent = getEncryptor().encrypt( appParams.get(AlipayConstants.BIZ_CONTENT_KEY), this.encryptType, this.charset) 加密算法 12345678Cipher cipher = Cipher.getInstance(AES_CBC_PCK_ALG); IvParameterSpec iv = new IvParameterSpec(AES_IV); cipher.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(Base64.decodeBase64(aesKey.getBytes()), AES_ALG), iv); byte[] encryptBytes = cipher.doFinal(content.getBytes(charset)); return new String(Base64.encodeBase64(encryptBytes)); 签名 123String signContent = AlipaySignature.getSignatureContent(requestHolder); protocalMustParams.put(AlipayConstants.SIGN, getSigner().sign(signContent, this.signType, charset)); 签名算法 1234567891011121314151617PrivateKey priKey = getPrivateKeyFromPKCS8(AlipayConstants.SIGN_TYPE_RSA, new ByteArrayInputStream(privateKey.getBytes())); java.security.Signature signature = java.security.Signature .getInstance(AlipayConstants.SIGN_SHA256RSA_ALGORITHMS); signature.initSign(priKey); if (StringUtils.isEmpty(charset)) &#123; signature.update(content.getBytes()); &#125; else &#123; signature.update(content.getBytes(charset)); &#125; byte[] signed = signature.sign(); return new String(Base64.encodeBase64(signed)); 增加accessToken等参数发起访问 微信(ISV)主要依靠token机制，支持post，get，消息体明文传输 消息内返回需要加密： 加密机制：aes 主要加密逻辑 1234567891011121314151617181920212223242526272829303132public String encrypt(String plainText) &#123; String encryptedXml = this.encrypt(genRandomStr(), plainText); String timeStamp = Long.toString(System.currentTimeMillis() / 1000L); String nonce = genRandomStr(); String signature = SHA1.gen(new String[]&#123;this.token, timeStamp, nonce, encryptedXml&#125;); return generateXml(encryptedXml, signature, timeStamp, nonce); &#125;protected String encrypt(String randomStr, String plainText) &#123; ByteGroup byteCollector = new ByteGroup(); byte[]a randomStringBytes = randomStr.getBytes(CHARSET); byte[] plainTextBytes = plainText.getBytes(CHARSET); byte[] bytesOfSizeInNetworkOrder = number2BytesInNetworkOrder(plainTextBytes.length); byte[] appIdBytes = this.appidOrCorpid.getBytes(CHARSET); byteCollector.addBytes(randomStringBytes); byteCollector.addBytes(bytesOfSizeInNetworkOrder); byteCollector.addBytes(plainTextBytes); byteCollector.addBytes(appIdBytes); byte[] padBytes = PKCS7Encoder.encode(byteCollector.size()); byteCollector.addBytes(padBytes); byte[] unencrypted = byteCollector.toBytes(); try &#123; Cipher cipher = Cipher.getInstance("AES/CBC/NoPadding"); SecretKeySpec keySpec = new SecretKeySpec(this.aesKey, "AES"); IvParameterSpec iv = new IvParameterSpec(this.aesKey, 0, 16); cipher.init(1, keySpec, iv); byte[] encrypted = cipher.doFinal(unencrypted); return BASE64.encodeToString(encrypted); &#125; catch (Exception var14) &#123; throw new RuntimeException(var14); &#125; &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[HashMap，ConcurrentHashMap 全解析]]></title>
    <url>%2F2019%2F12%2F25%2FHashMap%EF%BC%8CConcurrentHashMap%20%E5%85%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[[TOC] HashMap从1.7到1.8简介 数据结构：引入了红黑树主要介绍 红黑树 存储流程 数组元素&amp;链表中的实现类 HashMap中的数组元素&amp;链表节点采用Node类实现 与1.7相比（Entry），仅仅只是换了名字 源码见 jdk1.8 红黑树节点实现类 HashMap中的红黑树节点采用TreeNode实现 具体使用主要使用API和使用流程 基本与JDK1.7相同 基础知识：HashMap中的重要参数主要参数（与1.7相同） 容量（capacity）：必须是2的幂 &amp; &lt; 2^30 加载因子（load factor）：HashMap在其容量自动增加前可达到多满的一种尺度 扩容阈值（threshold）：当哈希表的大小 ≥ 扩容阈值时，就会扩容哈希表（即扩充HashMap的容量） 其他 Node - 存储数据的Node类型 数组，长度 = 2的幂；数组的每个元素 = 1个单链表 size - HashMap的大小，即 HashMap中存储的键值对的数量 其他参数（与红黑树相关） 桶的树化阈值：即 链表转成红黑树的阈值，在存储数据时，当链表长度 &gt; 该值时，则将链表转换成红黑树 static final int TREEIFY_THRESHOLD = 8; 桶的链表还原阈值：即 红黑树转为链表的阈值，当在扩容（resize（））时（此时HashMap的数据存储位置会重新计算），在重新计算存储位置后，当原有的红黑树内数量 &lt; 6时，则将 红黑树转换成链表 static final int UNTREEIFY_THRESHOLD = 6; 3. 最小树形化容量阈值：即 当哈希表中的容量 &gt; 该值时，才允许树形化链表 （即 将链表 转换成红黑树） // 否则，若桶内元素太多时，则直接扩容，而不是树形化 // 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD static final int MIN_TREEIFY_CAPACITY = 64; 加载因子 主要区别 版本 存储的数据结构 数组&amp;链表节点的实现类 红黑树的实现类 核心参数 JDK8 数组+链表+红黑树 Node TreeNode 主要参数相同，1.8增加了红黑树参数 JDK7 数组+链表 Entry / 1. 桶的树化阈值，即 链表转化为红黑树的阈值2. 桶的链表还原阈值，即红黑树转为链表的阈值3. 最小树形化容量阈值- 当哈希表中的容量&gt;该值时，才允许树形化链表- 否则当桶内元素太多时，则直接扩容，而不是树形化- 为了避免扩容，树形化选择的冲突，这个值不能小于4*THEEIFY_THRESHOLD 源码分析 声明HashMap对象 添加数据，put操作 hash操作 1.8 hash过程 计算过程 为了解决 “哈希码与数组大小范围不匹配” 的问题，HashMap给出了解决方案：哈希码 与运算（&amp;） （数组长度-1） 扰动处理：加大哈希码低位的随机性，使得分布更均匀，从而提高对应数组存储下标位置的随机性 &amp; 均匀性，最终减少Hash冲突 putVal操作 具体流程 扩容机制（resize()） 扩容机制对比， 结论 References Java源码分析：HashMap 1.8 相对于1.7 到底更新了什么？ HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你！ Java7/8 中的 HashMap 和 ConcurrentHashMap 全解析 HASHMAP之扰动函数和低位掩码 美团面试题：Hashmap的结构，1.7和1.8有哪些区别，史上最深入的分析 HashMap源码分析，基于1.8，对比1.7]]></content>
  </entry>
  <entry>
    <title><![CDATA[java浮点型精度丢失浅析]]></title>
    <url>%2F2019%2F11%2F23%2Fjava%E6%B5%AE%E7%82%B9%E5%9E%8B%E7%B2%BE%E5%BA%A6%E4%B8%A2%E5%A4%B1%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[java浮点型数值在运算中会出现精度损失的情况，在业务要求比较高比如交易等场景，一般使用BigDecimal来解决精度丢失的情况。最近一个同事在使用BigDecimal时仍然出现了精度损失，简略记录一下 测试用例代码如下 1234567@Test public void fd() &#123; double abc = 0.56D; System.out.println("abc: " + abc); System.out.println("new BigDecimal(abc): " + new BigDecimal(abc)); System.out.println("BigDecimal.valueOf(abc): " + BigDecimal.valueOf(abc)); &#125; 输出 123abc: 0.56new BigDecimal(abc): 0.560000000000000053290705182007513940334320068359375BigDecimal.valueOf(abc): 0.56 可以看到在使用BigDecimal构造器转化浮点型仍然会有损失，而使用valueOf方法则不会出现精度损失。 深入源码BigDecimal构造器，核心代码(BigDecimal(double val))如下 1234567891011public BigDecimal(double val, MathContext mc) &#123; ..... long valBits = Double.doubleToLongBits(val); int sign = ((valBits &gt;&gt; 63) == 0 ? 1 : -1); int exponent = (int) ((valBits &gt;&gt; 52) &amp; 0x7ffL); long significand = (exponent == 0 ? (valBits &amp; ((1L &lt;&lt; 52) - 1)) &lt;&lt; 1 : (valBits &amp; ((1L &lt;&lt; 52) - 1)) | (1L &lt;&lt; 52)); exponent -= 1075; ...&#125; 划重点， Double.doubleToLongBits返回根据IEEE754浮点“双精度格式”位布局，返回指定浮点值的表示 BigDecimal.valueOf核心代码 123456public static BigDecimal valueOf(double val) &#123; return new BigDecimal(Double.toString(val)); &#125;public BigDecimal(char[] in, int offset, int len, MathContext mc) &#123; ....&#125; 可以看到使用valueOf方法实际上是把double转为String，再调用string构造器的。 那么为什么使用Double.doubleToLongBits会出现精度损失，而使用string构造器不会呢。主要原因是BigDecimal使用十进制(BigInteger)+小数点(scale)位置来表示小数，而不是直接使用二进制，如101.001 = 101001 * 0.1^3，运算时会分成两部分，BigInteger间的运算以及小数点位置的更新，这里不再展开。 原理浅析Double.doubleToLongBits为什么会出现精度损失呢，主要原因是因为浮点型不能用精确的二进制来表述，就如十进制不能准确描述无穷小数一样。 浮点型转化为二进制的算法是乘以2直到没有了小数为止，举个栗子，0.8表示成二进制 0.8*2=1.6 取整数部分 1 0.6*2=1.2 取整数部分 1 0.2*2=0.4 取整数部分 0 0.4*2=0.8 取整数部分 0 可以看到上述的计算过程出现循环了，所以说浮点型转化为二进制有时是不可能精确的。 结论如果想要把浮点型转化为BigDecimal，尽量选择使用valueOf方法，而不是使用构造器。 References JAVA程序中Float和Double精度丢失问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logitech Options 在 Mac 下的自定义按键失灵问题]]></title>
    <url>%2F2019%2F11%2F23%2FLogitech%20Options%20%E5%9C%A8%20Mac%20%E4%B8%8B%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%89%E9%94%AE%E5%A4%B1%E7%81%B5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[罗技鼠标的自定义按键很好远，但自从升级系统到10.15 catalina后自定义按键就失灵了，重启，重装Logitech Options，重新授权…一番折腾后终于找到了彻底的解决办法 环境配置系统：MacOS 10.15 Catalina 鼠标：MX720 Logitech Options 版本：8.02.86 排查步骤 修改Security &amp; Privacy 里的 Logi Options Daemon 和 Logi Options 权限，发现已经勾选了 重新安装Logitech Options勾选权限，仍然无法使用 解决办法 删除Logi Options和Logi Options Daemon后，再次添加这两项 Security &amp; Privacy -&gt; Privacy 中添加Input Monitoring权限 如果需要自定义鼠标截图，还需要添加 Screen Recording权限 结论仔细看了Catalina的新特性，新系统对于安全和权限管理更加严格了，所以需要单独处理，至于Logitech Options需要的权限，可以参考Logitech Options permission prompts on macOS Catalina and macOS Mojave，新系统中其他软件遇到类似的问题，都可以通过这种方式解决 References Logitech Options 在 Mac 下的自定义按键经常会失灵 Logitech Options permission prompts on macOS Catalina and macOS Mojave]]></content>
  </entry>
  <entry>
    <title><![CDATA[Nacos 配置 MySQL 8.0 数据库]]></title>
    <url>%2F2019%2F11%2F15%2FNacos%20%E9%85%8D%E7%BD%AE%20MySQL%208.0%20%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Nacos 1.2.0版本以前是不支持MySQL8.0，如果出现配置保存不了，500的错误，多是由于数据源的问题，需要修改源码以支持MySQL8.0。 从github克隆源码 git clone https://github.com/alibaba/nacos.git 修改pom驱动版本（最外层pom） mysql mysql-connector-java 8.0.19 修改源码引用位置 nacos/naming/src/main/java/com/alibaba/nacos/naming/healthcheck/MysqlHealthCheckProcessor.java // import com.mysql.jdbc.jdbc2.optional.MysqlDataSource;原本引用的类import com.mysql.cj.jdbc.MysqlDataSource; 打包123cd nacos/mvn -Prelease-nacos -Dmaven.test.skip=true clean install -U ls -al distribution/target/nacos-server-1.2.0-SNAPSHOT/nacos 注意修改targeta下的jar包名为 nacos-server.jar 修改conf里的配置文件 启动进入bin目录，以官方提供的方式启动 sh startup.sh -m standalone 异常如果出现异常，可以通过logs/nacos.log查看具体的启动异常 References Nacos 配置 MySQL8数据库 Nacos 快速开始]]></content>
  </entry>
  <entry>
    <title><![CDATA[浅析Java8 CompletableFuture和Stream串行使用产生的一个问题]]></title>
    <url>%2F2019%2F11%2F02%2F%E6%B5%85%E6%9E%90Java8%20CompletableFuture%E5%92%8CStream%E4%B8%B2%E8%A1%8C%E4%BD%BF%E7%94%A8%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题来源问题来源于一次串行使用CompletableFuture和Stream导致CompletableFuture异步失效的问题，问题代码： 123456789101112int size = 50; List&lt;Double&gt; res = Stream.iterate(0, i -&gt; i + 1).limit(size).map(i -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; Double re = BigDecimal.valueOf(10 * (Math.sin(i) + 1)).setScale(2, RoundingMode.HALF_UP).stripTrailingZeros().doubleValue(); try &#123; TimeUnit.SECONDS.sleep(re.intValue()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return re; &#125;, ThreadUtil.fixed())) .map(CompletableFuture::join) .collect(Collectors.toList()); 其中ThreadUtil.fixed()是自己封装的线程工具方法，也可以使用Executors.newFixedThreadPool代替，原本是想通过CompletableFuture拿到异步执行的结果并进行处理，串联使用stream后反而未起到作用，先说解决方法： 先收集 future 结果到list，再调用新的流运算，即 .map(CompletableFuture::join)方法 limit(size).map 之间添加 parallel() 方法，形成 parallelStream()d的形式 原因分析：普通的stram可以理解为单纯的foreach循环，每生成一个future立即join，出现异步变同步的现象。 再进一步，既然CompletableFuture和parellStream都可以并行执行任务，有必要比较一下。 parallelStream先试用 parallelStream 重写上述方法 123List&lt;Double&gt; result = Stream.iterate(0, i -&gt; i + 1).limit(50).parallel().map(i -&gt; &#123; ///..... &#125;).collect(Collectors.toList()); 用时：54018ms 想要深入了解parallelStream，需要先了解ForkJoin框架和ForkJoinPool框架。这里简单介绍一下ForkJoinPool，真正了解 ParallelStream 还是需要先弄懂ForkJoinPool的，在此只是简单比较两者功能，不做深入探讨。 ForkJoinPool 使用分治法(Divide-and-Conquer Algorithm)来解决问题，实现了ExecutorService接口，线程数量可以通过构造器传入，默认使用机器的CPU数量。和ThreadPoolExecutor有一定区别，ForkJoinPool可以在运行线程中创建新的任务，并挂起当前的任务，此时线程就能够从队列中选择子任务执行，而ThreadPoolExecutor做不到这一点的。ForkJoinPool的核心算法是工作窃取算法，这样就可以在使用少量的线程来完成大量的任务。比如说ForkJoinPool 4个线程可以处理200完个任务，ThreadPoolExecutor显然是不可行的。 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 ParallelStreams中java8为ForkJoinPool添加了通用线程池，默认线程数量为机器的处理器数量。可以通过 -Djava.util.concurrent.ForkJoinPool.common.parallelism=N来设置ForkJoinPool的线程数量。 拆分成两步重写上述方法12345List&lt;CompletableFuture&lt;Double&gt;&gt; futures = Stream.iterate(0, i -&gt; i + 1).limit(50).map(i -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; ///..... &#125;, ThreadUtil.fixed())).collect(Collectors.toList()); List&lt;Double&gt; ids = futures.stream().map(CompletableFuture::join) .collect(Collectors.toList()); 用时：57111ms， 通过增加线程数量可以减少执行时间。 References Java 多线程中的任务分解机制-ForkJoinPool，以及CompletableFuture CompletableFuture 组合式异步编程]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入理解Java虚拟机》读书笔记]]></title>
    <url>%2F2019%2F09%2F12%2F%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[我与虚拟机小红小蓝的故事 《深入理解Java虚拟机》是JVM领域的经典之作，刚工作不久就有大佬强烈推荐，无奈基础较差，读的比较慢，之后短短续续读完了。在之后的工作中，偶尔也会拿出来翻一下，对于学习和工作帮助很大。 JVM是整个虚拟机提醒的底座，是Java的“平台无关性”的基础，了解了JVM才能深入理解到java程序“一次编写，到处运行”的真正原因。同时对于工作中程序性能调优，异常排查有很大的指导意义。 主要的内容包含以下几个方面： 走进Java 自动内存管理机制 虚拟机执行子系统 程序编译与代码优化 高效并发 作者层层递进，从了解java一致到指导编写高效简洁的程序给了我很多帮助。 走近JAVA 这一部分就不用说了吧，对java技术体系的一个大概介绍。说实话，这是我刚开始读这本书了解最深也是最有印象的一部分，因为其他篇章还读不太懂（逃）。 自动内存管理机制 说实话，很多人喜欢java可能就是因为JVM的自动内存回收机制，因为这一个机制，让我们不用考虑垃圾回收问题，专注于业务代码的编写，但是并不意味着不需要了解JVM的GC机制。程序计数器，堆栈，内存布局……等等对于工作的指导意义很大。刚开始工作的时候写代码比较随意，stackoverflow常有发生，这个时候就需要优化代码，同时调整jvm参数就可以理解。同时像数据库连接池，线程池，等等的配置也和JVM内存有很大的关系。同时这一部分也讲了很多案例，程序的部署策略，集群间的内存异常，堆外内存的溢出错误，JVM进程奔溃，外部命令导致系统缓慢，都有很大的实践意义。刚开始学习netty的时候，遇到了很多堆外内存的溢出错误，也是在实践案例里找到参考，让我顺利的解决了异常。同时这一部分也了解了JConsole的和VisualVM的使用，现在启动新的项目时，一般也会通过VisualVM观察本地的内存占用情况，同时，如有可能也可以通过连接远程来观察服务器上的内存占用情况，优化程序。 虚拟机执行子系统 了解class文件结构，字节码指令，类加载机制等等，实际业务中虽然用的不多，排查问题的时候用过。同时通过了解字节码指令结合javaagent编写一些小的工具，如监控系统等等，对于程序的良好运行都有很大意义。 程序编译与代码优化 了解了Java编译器，可以通过反编译了解代码的执行机制。像泛型，类型擦除，自动装箱、拆箱、与遍历循环，java8的lambda表达式，条件编译等等都可以其实现机制，让我可以编写更好的程序。 高效并发 并发程序的重要性不言而喻，但是如何编写高效的，线程安全的并发程序是一个长久的话题，这一部分从内存模型出发，深入解释了主内存与工作内存，内存交互，特别是线程优化与锁优化讲的很是深入。 总的来水，深入理解Java虚拟机 从底层开始讲起，让我了解了JVM的方方面面，之后的工作学习中也了解了其他的jvm技术，但基本都能在这本书中找到出处，让我受益良多。同时也期待第三版出版，让我有更大的收获。 ​ https://item.jd.com/12607299.html?dist=jd]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis底层原理-Strings]]></title>
    <url>%2F2019%2F08%2F21%2Fredis%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86-Strings%2F</url>
    <content type="text"><![CDATA[翻译自 Under the Hood of Redis: Strings 你知道简单string strings 在redis里占用了56 bytes的内存吗？ 我会试图告诉你为什么，了解redis运行原理时非常重要的。当你试图构建一个高负载的应用显的尤为重要，同时，你很快就会理解你的redis实例为什么会消费大量的内存？ 这篇文章主要介绍以下几个主题： strings 在redis里如何存储 strings 的内部结构是什么样的 redis 使用的优化机制 依据不同场景，如何有效的使用strings或者以此为基础的结构 Strings是redis里最常用的数据结构。 HSET/ZSET/LIST 在内部结构上都会增加一定的开销。过去一年，我在 stackoverflow 上浏览了大量关于redis的答案， 让我意识到大量的开发者并不理解reids的内存结构以及redis为高速所付出的代价。这是该系列的第一篇文章，讲解redis内部构造。redis数据结构会占用多少内存的问题实际和编译器，CPU以及redis使用的内存分配器相关（redis默认使用jemalloc）。以下的计算依赖64位 centos 上的redis 3.0.5 版本。 对于不编写或者不熟悉C/C++的开发者而言，理解可能上不太容易。在此我会简化概念以让你能理解计算过程。在C/C++语言里，当你声明unsigned int (4 bytes) 变量，编译器会分配8 bytes内存（64位架构）。jemalloc 内存分配器会优化查找新的内存块的速度，并对齐分配的内存。jemalloc 的内存分配策略运行良好，然而接下来我认为我应该使用简化的概念来描述。你请求24 bytes，分配32。你请求61，分配64。我做了深度的简化，希望你理解的更清楚。 Salvatore Sanfilippo’s (aka antirez）通过一种SDS的结构来解释strings： 12345+--------+-------------------------------+-----------+| Header | Binary safe C alike string... | Null term |+--------+-------------------------------+-----------+ | `-&gt; Pointer returned to the user. 这是一种简单的C结构，header 部分包含string数据部分和末尾0的实际大小和内存占用空间的信息。我们感兴趣的事sds strings header结构的成本，resize策略和内存分配的代价。 2015年7月4号，pull request a long history with the optimization of sds strings, 被引入Redis 3.2，使sds headers部分内存占用大幅度降低（从16%到200%不等）。移除了redis里关于redis string 最大512MB的限制。所有的这些可能性都归功于string长度变化时，header的动态变更。strings长度在256 bytes以下时，header仅占用3 bytes，65kb以下时占用5 bytes，512MB以下时占用9 bytes，uint64_t(64 bit unsigned integer)以下时占用17 bytes。而这种变化可以减少redis server farm 19.3%的内存（～42 GB）。然而，在Redis 3.0.x 中简化为 8 bytes 加 末端零占用的1 byte。让我们评估一下string strings的内存占用： 116 (header) + 7 (string length) + 1(trailing zero) = 24 bytes (16 bytes in the header, because the compiler will align 2 unsigned int for you). jemalloc 会分配32 bytes。Let’s take as long as it will not be taken into account （我希望你售后能理解为什么）。 当一个字符串大小变化时会引起什么变化？当你增加字符串长度，同时发现已分配的内存不足，redis 会将新长度和常量SDS_MAX_PREALLOC（sds.h中定义，值为1,048,576 bytes）比较。如果新长度比该值小，则会分配两倍的请求大小。如过请求长度大于 SDS_MAX_PREALLOC ，新增加的长度会增加到这个常量上。 这个特性对于主题-bitmaps使用中内存减少 这个问题非常重要。分配的内存通常是需要的两倍，是因为setbit实现的需要（参见 setbit 命令，bitops.c）。 现在你可以说 strings 会占用32 bytes（包括已分配的）。浏览过 hashedin.com (redis memory optimization guide) 的读者可能会想起他们被强烈建议不要使用少于100 bytes 的字符串，比如 set foo bar 会占用 ～96 bytes，其中 90 bytes 的开销（64位机器）。讲道理，让我们看一下为什么。 reids里所有的值都被命名为 redisObject， 内部结构如下： 123+------+----------+-----+----------+-------------------------+| Type | Encoding | LRU | RefCount | Pointer to data (ptr*) |+------+----------+-----+----------+-------------------------+ 稍后我们会计算字符串的大小，了解账户编译器和jemalloc特性。了解存储字符串的编码是非常重要的，redis会使用三种不同的存储策略： REDIS_ENCODING_INT. Strings can be stored in this form, if the value is cast to long value in the range LONG_MIN, LONG_MAX. For example, the string «dict» it will be stored in the form of this encoding, and will be the number 1952672100 (0x74636964). This encoding is also used for pre-selected range of special values in the range REDIS_SHARED_INTEGERS (defined in redis.h and the default is 10000). The values of this range are allocated immediately at the start of Redis. REDIS_ENCODING_EMBSTR used for strings with a length up to 39 bytes (the value from constant REDIS_ENCODING_EMBSTR_SIZE_LIMIT object.c). This means that redisObject structure and sds string structure are placed in a single area of memory allocated by allocator. With this in mind, we will be able to calculate the correct alignment. However, it is equally important to understand the problem of memory fragmentation in the Redis and how to live with it. REDIS_ENCODING_RAW used for all strings whose length exceeds REDIS_ENCODING_EMBSTR_SIZE_LIMIT. In this case our ptr * stores a pointer to the memory area with sds string. EMBSTR 在2012年出现，在短字符串方面，带来了大约 60%-70%的性能提升，但目前对内存及其碎片化影响的研究还不多。 7 bytes 的 strings 字符串，使用 EMBSTR 存储结构。构建的存储结构类似这样： 12345+--------------+--------------+------------+--------+----+| robj data... | robj-&gt;ptr | sds header | string | \0 |+--------------+-----+--------+------------+--------+----+ | ^ +-----------------------+ 现在我们可以再次计算 strings 的内存占用情况 1(4 + 4)* + 8(encoding) + 8 (lru) + 8 (refcount) + 8 (ptr) + 16 (sds header) + 7(strig itself) + 1 (terminating zero) = 56 bytes. The type and value in redisObject uses only the 4 lower and higher bits in the same number, so these two aligned fields will take 8 bytes. 让我们检查一下，使用 DEBUG SDSLEN 来debug SDS (http://redis.io/commands/debug-object) 字符串。这个命令在redis2.6 被加入。 123456set key strings+OKdebug object key+Value at:0x7fa037c35dc0 refcount:1 encoding:embstr serializedlength:8 lru:3802212 lru_seconds_idle:14debug sdslen key+key_sds_len:3, key_sds_avail:0, val_sds_len:7, val_sds_avail:0 使用EMBSTR编码，字符串长度 7 bytes（有效SDS长度），那么 hashdin.com 的开发者讨论的 96 bytes 又是关于什么呢？在我的理解中，他们犯了一点小错误，set foo bar 需要分配112 bytes内存（value 56 bytes，key 56 bytes），内存开销 106 bytes。 我承诺会说明使用BITMAP时，节省内存的情况。Redis 2.2 开始出现的 Bit 和 byte 操作 就想一个实时计数的魔法棒，可以节省内存。官方口号是“上亿用户数据，仅占用12M内存“。 理解了redis内存字符串原理，也可以了解bitmap。“是否应该被用于少量数据？”。假设你需要记录一千万人的上网数据： 1234setbit online 10000000 1:0debug sdslen online+key_sds_len:6, key_sds_avail:0, val_sds_len:1250001, val_sds_avail:1048576 你会消费 2,288,577 bytes 内存，对你来说“有用”的部分为 1,250,001 bytes。存储你的一个用户花费 ～2.3 MB，使用 SET 你需要 ～64 bytes（pyaload 为 4 bytes）。使用这种数据结构可以有效减少内存使用量。如果你有10,000～100,000用户，bitmap结构就可以复用内存。 最后，了解一下 字符串 resize，即就是重新分配内存块。内存碎片化是redis的另一个特性，很少有开发者能考虑到这一点： 1234567891011info memory$222# Memoryused_memory:506920used_memory_human:495.04Kused_memory_rss:7565312used_memory_peak:2810024used_memory_peak_human:2.68Mused_memory_lua:36864mem_fragmentation_ratio:14.92mem_allocator:jemalloc-3.6.0 mem_fragmentation_ratio 指标显示了系统分配的内存used_memory_rss 和 redis使用内存used_memory的比率。use_memory 和 use_memory_ree包含了数据和redis存储的内部数据结构所占用内存。 Redis RSS (Resident Set Size) - RAM allocated by the operating system, which in addition to the user data (and the costs of their internal representation) accounted for the cost of fragmentation during the physical allocation of the operating system. 如何理解 mem_fragmentation_ratio ？2.1 意思是需要210%的更多内存。小于1则意味着内存被终止，操作系统正在交换内存。 实际中，如果该数字超过 1-1.5边界意味着有地方出错了，尝试以下解决方法： 重启redis。redis越长时间不重启，这个值就会越大。 检查一下你计划存储的数据量。比方说，如果你使用32位redis存储多达4GB的数据，那么你应该使用64位redis以扩增rdb。 如果你了解内存分配器的不同点，可以考虑更换内存分配器。 其他资料： http://redis.io/topics/memory-optimization http://redis.io/topics/internals-sds http://redislabs.com/blog/redis-ram-ramifications http://github.com/sripathikrishnan/redis-rdb-tools/wiki/Redis-Memory-Optimization]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql优化的一般策略]]></title>
    <url>%2F2019%2F08%2F21%2Fsql%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%80%E8%88%AC%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[sql 优化的一般策略：索引优化，sql改写，参数优化，优化器 索引优化以select * from vvshop_order.vv_order where receive_phone=&#39;151011324532&#39;; 为例分析 1explain select * from vv_order where order_no=23; 结果： 分析：可以看到该sql扫描全表 30 多万记录，可以通过添加索引优化 1alter table vv_order add index orderno_idx(order_no); 注意点： 当传入的数据类型和库表数据类型不一致时，索引会失效 不要为每个查询字段建立单独的索引，应该根据实际需要建立单列索引或者组合索引 通过explain+extended 检查sql的执行计划，是否使用索引，是否发生隐式转换 避免在查询条件中使用函数 sql 改写分页优化原sql select * from buyer where sellerid=100 limit 100000，5000, limit M, N 写法中，M越大，性能越差，可以改写为 123select t1.* from buyer t1,(select id from buyer sellerid=100 limit 100000，5000) t2where t1.id=t2.id; 子查询优化 查询数量较多时，in改为exist，或者优化为如下的形式 1234SELECT first_nameFROM employees emp,(SELECT emp_no FROM salaries_2000 WHERE salary = 5000) salWHERE emp.emp_no = sal.emp_no; 避免查询返回所有字段，只返回需要的字段数据 不使用 select *or 改写为 in or的效率事n，in的效率是log(n)，控制in数量在200以内 不使用函数和触发器，通过应用程序实现少用join，保证字段类型一直再join或比较连续数值 使用 between参数优化优化器其他影响in是否生效的因素 eq_range_index_dive_limit 参数 默认为10 eq_range_index_dive_limit = 0 只能使用index dive 0 &lt; eq_range_index_dive_limit &lt;= N 使用index statistics eq_range_index_dive_limit &gt; N 只能使用index dive 字段 根据实际使用情况设置字段类型 单表不要有太多字段，建议20以内 避免使用null字段，优化较难且额外占用索引空间 用整型来存IP 系统参数调优基准测试工具 sysbench：模块化，跨平台以及多线程的性能测试工具 iibench-mysql：基于java的插入性能测试工具 tpcc-mysql：Percona 开发的TPC-C 测试工具 这里介绍一些比较重要的参数： back_log backlog值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到maxconnections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即backlog，如果等待连接的数量超过backlog，将不被授予连接资源。可以从默认的50升至500 wait_timeout 数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时 maxuserconnection 最大连接数，默认为0无上限，最好设一个合理上限thread_concurrency：并发线程数，设为CPU核数的两倍 skipnameresolve 禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问 keybuffersize 索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show status like’keyread%’，保证keyreads / keyreadrequests在0.1%以下最好 innodbbufferpool_size 缓存数据块和索引块，对InnoDB表性能影响最大。通过查询show status like ‘Innodbbufferpoolread%’，保证 (Innodbbufferpoolreadrequests – Innodbbufferpoolreads)/ Innodbbufferpoolreadrequests 越高越好 innodbadditionalmempoolsize InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小 innodblogbuffer_size InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB querycachesize 缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失。根据命中率(Qcachehits/(Qcachehits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令show status like ‘Qcache_%’查看目前系统Query catch使用大小 readbuffersize MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能 sortbuffersize MySql执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sortbuffersize变量的大小 readrndbuffer_size MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 record_buffer 每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值 threadcachesize 保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的 table_cache 类似于threadcachesize，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM References 阿里云慢SQL优化挑战大赛分析 SQL优化器原理 - 查询优化器综述 MYSQL查询SQL语句性能优化方法 MySQL–eq_range_index_dive_limit参数学习 MySQL SQL优化之in与range查询【转】 MySQL5.7利用虚拟列优化]]></content>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB 查询语法]]></title>
    <url>%2F2019%2F08%2F21%2FSpring%20Data%20MongoDB%20%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Query 和 Criteria 查询 123Query query = new Query();query.addCriteria(Criteria.where("name").is("Eric"));List&lt;User&gt; users = mongoTemplate.find(query, User.class); 支持的查询方法：is, regex, lt, gt, pageable, sort 生成query方法 findByX 1List&lt;User&gt; findByName(String name); startinggWith and endingWith 12List&lt;User&gt; findByNameStartingWith(String regexp);List&lt;User&gt; findByNameEndingWith(String regexp); between 1List&lt;User&gt; findByAgeBetween(int ageGT, int ageLT); like and orderBy 1List&lt;User&gt; users = userRepository.findByNameLikeOrderByAgeAsc("A"); JSON Query methods : @Query 12@Query("&#123; 'name' : ?0 &#125;")List&lt;User&gt; findUsersByName(String name); 支持的查询方法： $regex, $gt, $lt QueryDSL Queries 4.1 maven 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;com.mysema.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-mongodb&lt;/artifactId&gt; &lt;version&gt;3.6.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.mysema.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt; &lt;version&gt;3.6.6&lt;/version&gt;&lt;/dependency&gt;&lt;plugin&gt; &lt;groupId&gt;com.mysema.maven&lt;/groupId&gt; &lt;artifactId&gt;apt-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;process&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/generated-sources/java&lt;/outputDirectory&gt; &lt;processor&gt; org.springframework.data.mongodb.repository.support.MongoAnnotationProcessor &lt;/processor&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 4.2 class 1234567891011@QueryEntity@Documentpublic class User &#123; @Id private String id; private String name; private Integer age; // standard getters and setters&#125; Implement QueryDslPredicateExecutor 123QUser qUser = new QUser("user");Predicate predicate = qUser.name.eq("Eric");List&lt;User&gt; users = (List&lt;User&gt;) userRepository.findAll(predicate); 支持的查询方法：is,startinggWith and endingWith, between ​]]></content>
      <tags>
        <tag>spring data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins API 实践]]></title>
    <url>%2F2019%2F06%2F15%2FJenkins%20API%20%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[获取项目配置 curl -X GET http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/config.xml buildcurl -X POST http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/build -F ‘json={“parameter”: [{“name”: “Project”, “value”: “geek-icem-gateway”}, {“name”: “Branch”, “value”: “ad”}]}’ curl -X POST http://127.0.0.1:37555/job/geek-icem_backend_build-packages/build –user admin:Jenkins -H ‘cache-control: no-cache’ -F ‘json={“parameter”: [{“name”: “Project”, “value”: “geek-icem-gateway”}, {“name”: “Branch”, “value”: “ad”}]}’ 获取构建信息curl -X GET http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/lastCompletedBuild/api/json 获取构建信息curl -X GET http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/lastBuild/api/json 获取控制台日志curl -X GET http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/17/consoleText lastBuild, lastStableBuild, lastSuccessfulBuild, lastFailedBuild, lastUnstableBuild, lastUnsuccessfulBuild, lastCompletedBuild 获取描述curl -X GET http://admin:Jenkins@127.0.0.1:37555/job/geek-icem_backend_build-packages/description References jenkins 出现“Error 403 No valid crumb was included in the request ”的解决方案 python-jenkins 使用 Python 操作 Git 版本库 - GitPython Jenkins获取编译状态 GitPython Tutorial]]></content>
  </entry>
  <entry>
    <title><![CDATA[redission 序列化问题追踪]]></title>
    <url>%2F2019%2F05%2F28%2Fredission%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[背景项目原本是用jedis连接redis，但考虑到需要用redis锁，因此替换为方便快捷的redisson，但是使用redisson之后会报decode error，具体信息如下： 12345678910112019-05-15 13:39:59.973 [redisson-netty-2-3] ERROR o.r.c.h.CommandDecoder [decodeCommand:203] - Unable to decode data. channel: [id: 0x477c5ced, L:/192.168.4.94:57423 - R:10.10.10.43/10.10.10.43:6379], reply: ReplayingDecoderByteBuf(ridx=102, widx=102), command: (GET), params: [Geek:xxxxx:xxxx]java.io.IOException: java.lang.NullPointerException at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:247) at org.redisson.codec.FstCodec$1.decode(FstCodec.java:228) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:368) at org.redisson.client.handler.CommandDecoder.decodeCommand(CommandDecoder.java:200) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:140) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:115) at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:366) at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) 测试代码12RBucket ste = redissonClient.getBucket("Geek:add:ddd");Object re = ste.get(); 考虑可能是由于序列化产生的问题，查到NullPointer 3.10.6，设置codec为StringCodec，即 1redissonClient.getConfig().setCodec(new StringCodec()); 但是并未解决问题，redisson仍然使用默认的FstCodec，通过idea强大的提示功能可以看到 getBucket接受一个codec参数 修改代码为 12RBucket ste = redissonClient.getBucket("Geek:add:ddd", new StringCodec());String re = ste.get(); 完美解决 问题为什么直接设置redisson config 不生效呢，一步步查源码 RedissonObject#RedissonObject 123public RedissonObject(CommandAsyncExecutor commandExecutor, String name) &#123; this(commandExecutor.getConnectionManager().getCodec(), commandExecutor, name);&#125; 可以看出redisson 默认从ConnectionManager里获取codec方式，继续看，以 SingleConnectionManager 为例，SingleConnectionManager是MasterSlaveConnectionManager的子类，具体的类图关系 config.java 12345678public Config(Config oldConf) &#123; setExecutor(oldConf.getExecutor()); if (oldConf.getCodec() == null) &#123; // use it by default oldConf.setCodec(new FstCodec()); &#125;...... &#125; 即检测到原有codec为空时，则设置为FstCodec 看一下 Redisson.java 配置关键部分代码 12345678910111213141516protected Redisson(Config config) &#123; this.config = config; Config configCopy = new Config(config); connectionManager = ConfigSupport.createConnectionManager(configCopy); evictionScheduler = new EvictionScheduler(connectionManager.getCommandExecutor()); writeBehindService = new WriteBehindService(connectionManager.getCommandExecutor());&#125; public static RedissonClient create(Config config) &#123; Redisson redisson = new Redisson(config); if (config.isReferenceEnabled()) &#123; redisson.enableRedissonReferenceSupport(); &#125; return redisson;&#125; 可以看出， config是在redisson初始化的时候传入的 因为我用的是redisson-spring-boot-starter，看一下这个starter里面，是如何初始化的，redisson starter 默认使用 spring-data-redis 配置。 123456789101112131415161718192021222324@Bean(destroyMethod = "shutdown") @ConditionalOnMissingBean(RedissonClient.class) public RedissonClient redisson() throws IOException &#123; Config config = null; .... if (redissonProperties.getConfig() != null) &#123; .... &#125; else &#123; config = new Config(); String prefix = "redis://"; Method method = ReflectionUtils.findMethod(RedisProperties.class, "isSsl"); if (method != null &amp;&amp; (Boolean)ReflectionUtils.invokeMethod(method, redisProperties)) &#123; prefix = "rediss://"; &#125; config.useSingleServer() .setAddress(prefix + redisProperties.getHost() + ":" + redisProperties.getPort()) .setConnectTimeout(timeout) .setDatabase(redisProperties.getDatabase()) .setPassword(redisProperties.getPassword()); &#125; return Redisson.create(config); 回到一开始的问题，直接设置redisson codec为什么不生效？仔细以上分析可以知道，redisson统一设置codec主要是通过初始化的时候传入ConnectionManager使 codec生效，而通过 redissonClient.getConfig().setCodec(...)的方式并不能改变ConnectionManager中的编码方式。 结论： 如果想自定义codec，需要自己初始化redissonClient[调用Redisson.create(config)]， 或者重写redisson-starter 在定制化程度不高时，可直接使用默认codec，或者把特定的codec传入方法体内 Reference]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全局唯一id方案]]></title>
    <url>%2F2019%2F05%2F10%2F%E5%85%A8%E5%B1%80%E5%94%AF%E4%B8%80id%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[mysql自增 优点：简单 缺点：扩容复杂，业务增大时数据迁移困难 基于时间戳和随机字符串 优点：对人比较友好 缺点：随机数生成不易控制 UUID 优点：简单粗暴，性能好，全球唯一，基本不会有性能问题 缺点：占用空间大，无序，查询效率低 变种：COMB算法 redis自增 优点：简单，易实现，数字ID天然有序 缺点：依赖redis，考验redis性能 Twitter-Snowflake算法 优点： 不依赖数据库，性能高，单机有序 灵活，支持多节点部署 缺点： 无法做到全局递增 微信id生成算法 万亿级调用下的优雅：微信序列号生成器架构设计及演变 类似于批量生成多个id，性能好，避免每次访问库的压力 可能会有单点故障，服务重启ID不连续 百度 - UidGenerator 雪花算法变种 Leaf——美团点评分布式ID生成系统- 美团技术团队 wuid — 一个比 UUID 快百倍的唯一 ID 生成器 vesta MongoDB — ObjectId]]></content>
  </entry>
  <entry>
    <title><![CDATA[OKR理解与实施]]></title>
    <url>%2F2019%2F04%2F05%2FOKR%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%96%BD%2F</url>
    <content type="text"><![CDATA[定义目标和关键成果(Objectives and Key Results) 原则 OKRs要是可量化的（时间&amp;数量），比如不能说“使gmail达到成功”而是“在9月上线gmail并在11月有100万用户” 目标要是有野心的，有一些挑战的，有些让你不舒服的。一般来说，1为总分的评分，达到0.6-0.7是较好的了，这样你才会不断为你的目标而奋斗，而不会出现期限不到就完成目标的情况。 每个人的OKRs在全公司都是公开透明的。比如每个人的介绍页里面就放着他们的OKRs的记录，包括内容和评分 两个不同 O和KR的不同：O要是有挑战性的，如果是板上钉钉的事情就是不够的；KRs能很好的支持O的完成，是要明显可量化的，便于评分的。 个人、组、公司OKRs的不同：个人OKRs是你个人展现你将会做什么；组的OKRs不是个人打包，是组优先做的事情；公司OKRs是高层对整个公司的展望。 优势 目标的协调和统一。公司 -&gt; 团队-&gt; 个人目标层层分解，每个人不再只关注自己的工作，而是能够看到”the bigger picture”，了解自己在团队和公司整体目标中发挥的作用，让自己的工作更有意义，从而更有积极性。并且，所有团队的目标都能被统一地联系在一起，执行起来可以相互支持； 判断优先级。通过梳理 OKRs，能找出最重要的事情，让全公司专注在最有价值的事情上，便于做出取舍，最有效地利用资源； 双向沟通。OKRs 的制定由员工和自己的直线上级一起制定，需要双方同意，而不是简单粗暴的由上而下，员工只是被动接受 灵活调整。不像 KPI 是死的，只要目标不变，OKRs 中的关键事件（Key Results）在回顾的过程中可以根据情况随时灵活调整； 鼓励创新。员工有自主权去制定有挑战性的目标，而不是拘泥于公司设定的框架里。往往能激发和产生出一些意想不到的新想法和结果，鼓励公司内部创新的发生。 原则SMART 责任分配矩阵(RAM)效果 清楚界定每项成果 为项目的每项成果设定责任人 让跨职能沟通更为有效 加速项目决策流程，简化审批 分配基本原则(RACI模型) 相互独立，完全穷尽（Mutually Exclusive Collectively Exhaustive，MECE）：分解的任务各部分之间相互独立，所有部分完全覆盖任务的各个部分； 结果导向，而非行动：分解任务时关注结果，而不要陷入对细节行动的过分关注当中，导致分解过于繁琐复杂 References 转：力荐神器级员工考核工具：谷歌OKR 一张图、一颗心、一场仗，阿里巴巴的绩效之道 揭秘阿里leader的绩效管理之道 OKRs 与目标分解 关于 OKR 的常见问题 教你几招，即刻开始实施 OKRs]]></content>
  </entry>
  <entry>
    <title><![CDATA[微服务架构BFF和网关]]></title>
    <url>%2F2018%2F09%2F05%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84BFF%E5%92%8C%E7%BD%91%E5%85%B3%2F</url>
    <content type="text"><![CDATA[BFF 架构图 BFF(Backend for Frontend)也称聚合层或者适配层，上述架构从外到内依次为 端用户体验层-&gt;网关层-&gt;BFF层-&gt;微服务层，主要是讲内部复杂的微服务，适配成对各种不同的用户体验。网关专注解决跨横切面逻辑，包括路由、安全、监控和限流熔断等。 为提高系统的灵活性，在网关层和微服务层之间构建BFF层，这里主要使用GraphQL 构建BFF层。 GraphQL 特点： 定义数据模型：按需获取 数据分层：通过数据分层可以减少客户端请求次数 123456789101112&#123; user(id:1001) &#123; // 第一层 name, friends &#123; // 第二层 name, addr &#123; // 第三层 country, city &#125; &#125; &#125;&#125; 强类型 1234567const Meeting = new GraphQLObjectType(&#123; name: 'Meeting', fields: () =&gt; (&#123; meetingId: &#123;type: new GraphQLNonNull(GraphQLString)&#125;, meetingStatus: &#123;type: new GraphQLNonNull(GraphQLString), defaultValue: ''&#125; &#125;)&#125;) GraphQL 的类型系统定义了包括 Int, Float, String, Boolean, ID, Object, List, Non-Null 等数据类型。所以在开发过程中，利用强大的强类型检查，能够大大节省开发的时间，同时也很方便前后端进行调试。 协议而非存储：GraphQL 本身并不直接提供后端存储的能力，不绑定任何的数据库或者存储引擎。 无需版本化 1234567891011121314const PhotoType = new GraphQLObjectType(&#123; name: 'Photo', fields: () =&gt; (&#123; photoId: &#123;type: new GraphQLNonNull(GraphQLID)&#125;, file: &#123; type: new GraphQLNonNull(FileType), deprecationReason: 'FileModel should be removed after offline app code merged.', resolve: (parent) =&gt; &#123; return parent.file &#125; &#125;, fileId: &#123;type: new GraphQLNonNull(GraphQLID)&#125; &#125;)&#125;) GraphQL 服务端能够通过添加 deprecationReason，自动将某个字段标注为弃用状态。并且基于 GraphQL 高度的可扩展性，如果不需要某个数据，那么只需要使用新的字段或者结构即可，老的弃用字段给老的客户端提供服务，所有新的客户端使用新的字段获取相关信息。并且考虑到所有的 graphql 请求，都是按照 POST /graphql 发送请求，所以在 GraphQL 中是无须进行版本化的。 GraphQL 与 Rest 数据获取：Rest 缺乏可扩展性，GraphQL能够按需获取； API调用：REST针对每种资源的操作都是一个endpoint，GraphQL只需要一个endpoint； 复杂数据请求：REST对于嵌套的复杂数据需要多次调用，GraphQL 一次调用，减少网络开销； 错误码处理：REST能够精确返回HTTP错误码，GraphQL统一返回200，对错误信息进行包装； 版本号：REST通过v1/v2实现，Graph通过Schema扩展实现。 BFF 端技术栈 微服务下使用GraphQL构建BFF中使用node作为BFF主要框架，因自己对node不太熟悉，会尽量采用java来实现相应的功能。GraphQL 使用 query 和 mutation 实现CQRS。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【译】Spring Boot @PropertySource 读取 YAML 文件]]></title>
    <url>%2F2018%2F08%2F31%2FSpring%20Boot%20%40PropertySource%20%E8%AF%BB%E5%8F%96%20YAML%20%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring Boot 默认不支持@PropertySource读取yaml 文件，这也是Stackoverflow 上经常给予的标准答案。 Spring 4.3 通过引入 PropertySourceFactory 接口使之成为可能。PropertySourceFactory 是PropertySource 的工厂类。默认实现是 DefaultPropertySourceFactory，可以构造ResourcePropertySource 实例。 可以通过普通的是实现构造 createPropertySource， 需要做两点: 导入resource 到Properties 对象。 构造 PropertySource 使用Properties。 具体例子： 1234567891011121314151617181920212223public class YamlPropertySourceFactory implements PropertySourceFactory &#123; @Override public PropertySource&lt;?&gt; createPropertySource(@Nullable String name, EncodedResource resource) throws IOException &#123; Properties propertiesFromYaml = loadYamlIntoProperties(resource); String sourceName = name != null ? name : resource.getResource().getFilename(); return new PropertiesPropertySource(sourceName, propertiesFromYaml); &#125; private Properties loadYamlIntoProperties(EncodedResource resource) throws FileNotFoundException &#123; try &#123; YamlPropertiesFactoryBean factory = new YamlPropertiesFactoryBean(); factory.setResources(resource.getResource()); factory.afterPropertiesSet(); return factory.getObject(); &#125; catch (IllegalStateException e) &#123; // for ignoreResourceNotFound Throwable cause = e.getCause(); if (cause instanceof FileNotFoundException) throw (FileNotFoundException) e.getCause(); throw e; &#125; &#125;&#125; 注意：YAML 需要 SnakeYAML 1.18 或者更高版本。 @PropertySource 注解有一个 factory 属性，通过这个属性来注入 PropertySourceFactory，这里给出 YamlPropertySourceFactory的例子。 123456789101112131415@SpringBootApplication@PropertySource(factory = YamlPropertySourceFactory.class, value = "classpath:blog.yaml")public class YamlPropertysourceApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext ctx = SpringApplication.run(YamlPropertysourceApplication.class, args); ConfigurableEnvironment env = ctx.getEnvironment(); env.getPropertySources() .forEach(ps -&gt; System.out.println(ps.getName() + " : " + ps.getClass())); System.out.println("Value of `foo.bar` = " + env.getProperty("foo.bar")); &#125;&#125; 注意：这里使用的是Spring Boot，但是对于Spring 4.3 及其以上版本同样适用。 翻译拙劣，欢迎指正。 References Use @PropertySource with YAML files]]></content>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Explain详解]]></title>
    <url>%2F2018%2F08%2F08%2FMySQL%20Explain%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[日常工作中主要用explain来查看sql语句的执行计划，深入了解是否需要优化，已经索引等信息。 执行explain命令会生成一个QEP(query execution plan) 1234567mysql&gt; explain select * from servers;+----+-------------+---------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | servers | ALL | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+---------+------+---------------+------+---------+------+------+-------+row in set (0.03 sec) 一、 id ​ 我的理解是SQL执行的顺序的标识,SQL从大到小的执行 id相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 3.id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 二、select_type ​ 示查询中每个select子句的类型 (1) SIMPLE(简单SELECT,不使用UNION或子查询等) (2) PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY) (3) UNION(UNION中的第二个或后面的SELECT语句) (4) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) (5) UNION RESULT(UNION的结果) (6) SUBQUERY(子查询中的第一个SELECT) (7) DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询) (8) DERIVED(派生表的SELECT, FROM子句的子查询) (9) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) 三、table 显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果) 12345678mysql&gt; explain select * from (select * from ( select * from t1 where id=2602) a) b;+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| 1 | PRIMARY | &lt;derived2&gt; | system | NULL | NULL | NULL | NULL | 1 | || 2 | DERIVED | &lt;derived3&gt; | system | NULL | NULL | NULL | NULL | 1 | || 3 | DERIVED | t1 | const | PRIMARY,idx_t1_id | PRIMARY | 4 | | 1 | |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ 四、type 表示MySQL在表中找到所需行的方式，又称“访问类型”。 常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好） ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index: Full Index Scan，index与ALL区别为index类型只遍历索引树 range:只检索给定范围的行，使用一个索引来选择行 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 五、possible_keys 指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 六、Key key列显示MySQL实际决定使用的键（索引） 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 七、key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 八、ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 九、rows 表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 十、Extra 该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 Using filesort：MySQL中无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 总结：**• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况• EXPLAIN不考虑各种Cache• EXPLAIN不能显示MySQL在执行查询时所作的优化工作• 部分统计信息是估算的，并非精确值• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。** 转载自 http://www.cnblogs.com/xuanzhi201111/p/4175635.html]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>reprinted</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发布构件到Maven中央仓库]]></title>
    <url>%2F2018%2F06%2F19%2F%E5%8F%91%E5%B8%83%E6%9E%84%E4%BB%B6%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[总结的工具包为使用方便决定发布到maven中央仓库，因为第一次发布，遇到很多问题，简要记录一下。 1Sonatype官网：http://www.sonatype.org/ Step 1: 注册Sonatype用户 注册地址：https://issues.sonatype.org/secure/Signup!default.jspa oss地址：https://oss.sonatype.org ，用于查询构件 Step2: 创建issue create issue: 12345Project：Community Support - Open Source Project Repository Hosting (OSSRH)Issue Type: new Project--- next ---Summary: &lt;简要项目介绍&gt;Group id: me.silloy &lt;需要时自己的域名，同时在工程中使用&gt; 点击create，创建issue 下图查看已创建的issue Step 3: 等待issue审批通过 一般需要1-2天，审批通过后会收到邮件通知，在自己提交的issue下面可以看到Sonatype工作人员的回复。同时issue状态修改为resolved。 Step 4: 发布准备 生成gpg密钥，并发布到PGP密钥服务器，见引用 简要介绍win10环境下gpg密钥生成方法 下载Gpg4win， 安装 依次执行一下命令 版本检查: gpg --version, 我用的是2.2.8 生成key： gpg --gen-key 检查本地key： gpg –list-keys 发布公钥：gpg –keyserver hkp://pool.sks-keyservers.net –send-keys A3434534534 校验是否发布成功：gpg –keyserver hkp://pool.sks-keyservers.net –recv-keys 732796B4 12# List all available gpg servers:$ gpg-connect-agent --dirmngr 'keyserver --hosttable' 修改maven设置 修改maven全局配置文件setting.xml， 增加一下内容 1234567&lt;servers&gt; &lt;server&gt; &lt;id&gt;oss&lt;/id&gt; &lt;username&gt;Harvey.Su&lt;/username&gt; &lt;password&gt;&lt;![CDATA[password]]&gt;&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 修改pom文件，加入需要的信息 1234567891011121314151617181920212223242526&lt;groupId&gt;me.silloy&lt;/groupId&gt; &lt;artifactId&gt;zjtools&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;description&gt;a java tool package&lt;/description&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;developers&gt; &lt;developer&gt; &lt;name&gt;sushaohua&lt;/name&gt; &lt;email&gt;sshzh90@gmail.com&lt;/email&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;scm&gt; &lt;connection&gt;scm:git:git@github.com:silloy/zjtools.git&lt;/connection&gt; &lt;developerConnection&gt;scm:git:git@github.com:silloy/zjtools.git&lt;/developerConnection&gt; &lt;url&gt;git@github.com:silloy/zjtools.git&lt;/url&gt; &lt;/scm&gt; 增加一个名为oss的profile (⭐⭐⭐⭐⭐) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;compilerVersion&gt;$&#123;maven.compiler.source&#125;&lt;/compilerVersion&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.source&#125;&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- Source --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-source-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- Javadoc --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-javadoc-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;additionalOptions&gt; &lt;additionalOption&gt;-Xdoclint:none&lt;/additionalOption&gt; &lt;/additionalOptions&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt;&lt;!-- profiles --&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt; &lt;activation&gt; &lt;!--&lt;activeByDefault&gt;true&lt;/activeByDefault&gt;--&gt; &lt;jdk&gt;[1.8,)&lt;/jdk&gt; &lt;/activation&gt; &lt;!--&lt;properties&gt;--&gt; &lt;!--&lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;--&gt; &lt;!--&lt;/properties&gt;--&gt; &lt;build&gt; &lt;!--&lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;-SNAPSHOT&lt;/finalName&gt;--&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;&lt;/finalName&gt; &lt;plugins&gt; &lt;!-- GPG --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-gpg-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt;&lt;/profiles&gt;&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; javadoc 不规范的情况下，可以把maven-javadoc-plugin注释掉。 特别注意：snapshotRepository 与 repository 中的 id 一定要与 setting.xml 中 server 的 id 保持一致。这里我们都设置为oss。 Step 5: 上传构件到OSS 执行命令： 1mvn clean deploy -P sonar -Darguments="gpg.passphrase=密钥密码" 看到如下提示信息，说明deploy成功 1234567891011Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jar (34 kB at 966 B/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.pomUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.pom (9.7 kB at 985 B/s)Downloading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xmlUploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xmlUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xml (296 B at 5 B/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jar (20 kB at 1.9 kB/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jar (20 kB at 10 kB/s) Step 6: 在OSS中发布构件 登录https://oss.sonatype.org，可以在`Staging Repositories`中查看到已上传的构件，可进行模糊查询，快速定位到自己的构件，状态为open，勾选，然后点击close按钮，接下来系统会自动验证该构件是否满足指定要求，当验证完毕后，状态会变为 Closed，最后，点击 Release 按钮来发布该构件 。 备注：切记是要发布release版本的构件，不能是snapshot，不然不会出现在Staging Repositories里面。 Step 7: 通知 Sonatype“构件已成功发布” 需要在曾经创建的 Issue 下面回复一条“构件已成功发布”的评论，这是为了通知 Sonatype 的工作人员为需要发布的构件做审批( I released the component has been successfully, please approval, thank you!)，发布后会关闭该 Issue 。 Step 8: 等待审批，1~2天，审批通过后会收到邮件通知。 Step 9: 在https://oss.sonatype.org/#stagingRepositories找到自己的构建，点击release Step 9: 从中央仓库搜索自己发布的构件 地址：http://search.maven.org/ Step 10: http://search.maven.org/ 上搜索自己的构件 ，大功告成，可以在项目中引用啦。以后发布就简单了，不需要每次都审核。 问题： gpg错误处理Enter passphrase: gpg: gpg-agent is not available in this session 可能是版本问题，或者没有安装gpg-agent，详见https://askubuntu.com/questions/860370/gpg-agent-cant-be-reached ，具体步骤 linux 系统解决方案 安装gpg2 sudo apt install gpgv2 然后在maven的settings.xml中加入两个属性，主要要在激活的profile里面 123456&lt;profile&gt; &lt;properties&gt; &lt;gpg.executable&gt;gpg2&lt;/gpg.executable&gt; &lt;gpg.useagent&gt;true&lt;/gpg.useagent&gt; &lt;/properties&gt;&lt;/profile&gt; mvn clean deploy -P oss， 参看 Avoid gpg signing prompt when using Maven release plugin windows 解决方案 (Windows下Gpg4win、Git、ssh-pageant配置)[https://www.mjollnir.cc/archives/216.html] .git/config修改 12[gpg] program = gpg Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project zjtools: Failed to deploy artifacts: Could not transfer artifact me.silloy:zjtools:jar:0.0.1 from/to oss (https://oss.sonatype.org/service/local/staging/deploy/maven2/): Access denied to: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jar, ReasonPhrase: Forbidden. -&gt; [Help 1] 答案：引用，提交给sonatype就可以，工作人员会开权限 可以在 http://search.maven.org/ 搜索到，但是不能在 http://mvnrepository.com/ 搜索到，是因为更新频率不一样，等一天左右就好了，参见工作人员回复 gpg --list-keys 出现 unknown 解决 12345678910111213141516gpg --edit-key user@useremail.comgpg&gt; trustPlease decide how far you trust this user to correctly verify other users' keys(by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menuYour decision? 5gpg&gt; save References 发布到中央仓库 上传自己的构件(Jar)到Maven中央仓库 将 Smart 构件发布到 Maven 中央仓库 Maven-008-Nexus 私服部署发布报错 Failed to deploy artifacts: Failed to transfer file: … Return code is: 4XX, ReasonPhrase: … 解决方案 Why am I getting a “401 Unauthorized” error in Maven? 将项目发布到 Maven 中央仓库踩过的坑 [gpg —list-keys command outputs uid [ unknown ] after importing private key onto a clean install]]></content>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-Mybatis通用mapper使用]]></title>
    <url>%2F2018%2F06%2F15%2FSpringBoot-Mybatis%E9%80%9A%E7%94%A8mapper%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[mybatis是一个很好用的工具，但是编写mapper是一件很麻烦的事，自mybatis 3.0开始可以使用注解的方式，极大的简化了xml的编写量，本地想看看mybatis源码，自己扩展写一个工具，在阅读源码过程中发现一个通用mapper的工具包，感觉不用重复造轮子了，简要记录一下spring boot整合通用mapper的使用。 确保可以正常使用mybatis pom引入依赖包，starter需要配合@Mapper注解使用，这里采用这种方式，或者使用@MapperScan注解，@tk.mybatis.spring.annotation.MapperScan(basePackages = &quot;扫描包&quot;)配合原生mapper使用。 12345&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;&#123;version&#125;&lt;/version&gt;&lt;/dependency&gt; 我使用的版本是2.0.2 Mybatis 扫描配置（Deprecated, spring 自动配置） 123456789101112131415161718@Configuration//TODO 注意，由于MapperScannerConfigurer执行的比较早，所以必须有下面的注解@AutoConfigureAfter(MybatisAutoConfiguration.class)public class MyBatisMapperScannerConfig &#123; @Bean public MapperScannerConfigurer mapperScannerConfigurer() &#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setSqlSessionFactoryBeanName("sqlSessionFactory"); mapperScannerConfigurer.setBasePackage("org.springboot.sample.mapper"); Properties properties = new Properties(); // 这里要特别注意，不要把MyMapper放到 basePackage 中，也就是不能同其他Mapper一样被扫描到。 properties.setProperty("mappers", MyMapper.class.getName()); properties.setProperty("notEmpty", "false"); properties.setProperty("IDENTITY", "MYSQL"); mapperScannerConfigurer.setProperties(properties); return mapperScannerConfigurer; &#125;&#125; 新建BaseMapper类，该类不能被当做普通Mapper一样被扫描 ，不加@Mapper注解，或者放在不同文件夹 1234567package com.zj.mapper;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;public interface BaseMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; &#123;&#125; 业务处理dao层，扩展BaseMapper 1234567package com.zj.mapper;import com.zj.model.OrderInfo;import org.apache.ibatis.annotations.Mapper;@Mapperpublic interface OrderInfoMapper extends BaseMapper&lt;OrderInfo&gt; &#123;&#125; 其他和使用普通mybatis一致，service层部分代码 12orderInfoMapper.insertSelective(info);OrderInfo info = orderInfoMapper.selectByPrimaryKey(id); 通用mapper提供常用的一些操作方法: deleteByPrimaryKey, insert, insertSelective, selectByPrimaryKey, updateByPrimaryKeySelective, updateByPrimaryKey, insertList等很多方法，需要你进一步探索😀😀 主键id问题 当使用insert，insertSelective等方法时，希望返回由数据库产生的逐渐，需要在实体类上增加注解 123@Id@GeneratedValue(generator = "JDBC")private Long orderInfoId; generator=”JDBC”表示 MyBatis 使用 JDBC 的 getGeneratedKeys 方法来取出由数据库内部生成的主键 ，适用于MySQL，SQL Server等的自增主键。 或者： 123@Id@KeySql(useGeneratedKeys = true)private Long id; 如果实体字段和数据库字段不一致，可以使用@Column注解，其他注解 参见注解 12@Column(name="SCORE_SUM")private String sumScore; MBG生成参见https://github.com/abel533/Mapper/wiki/4.1.mappergenerator，demo见 git@github.com:silloy/mybatis-generator.git 更多细节参见wiki 通用Mapper极大的简化了xml文件的编写，但仍需要少许xml文件，有待进一步优化。同时因为这是一个个人项目，使用不太熟悉不建议使用。 References http://www.mybatis.tk/ https://github.com/abel533/Mapper/wiki https://github.com/abel533/Mapper https://blog.csdn.net/qq_19260029/article/details/78010369]]></content>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用RabbitMQ实现AMQP和MQTT的协议转换]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%BD%BF%E7%94%A8RabbitMQ%E5%AE%9E%E7%8E%B0AMQP%E5%92%8CMQTT%E7%9A%84%E5%8D%8F%E8%AE%AE%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[基于LBS的业务系统，前台使用MQTT协议推送到后台，后台通过RabbitMQ实现协议的转换，以实现负载消费的功能。主要的框架如图 准备工作：rabbitmq安装以及 rabbitmq_mqtt 插件启用 MQTT client向/drivers/1push消息，AMQP client 使用 topic 模式接受消息， exchange name为固定值 amq.topic，routingKey 为 MQTT 发送使用的Topic，注意/需要替换为.，queue name 可以任意指定，绑定相同queue的customer可以实现负载消费的功能。 Demo示例(以java为例)： RabbitConfig 12345678910111213141516171819202122@Componentpublic class TopicRabbitConfig &#123; public static final String message = "location.broker"; @Bean public Queue queueMessage() &#123; return new Queue(message); &#125; @Bean TopicExchange exchange() &#123; return new TopicExchange("amq.topic", true, false); &#125; //綁定队列 queueMessages() 到 topicExchange 交换机,路由键只接受完全匹配 topic.message 的队列接受者可以收到消息, # 为通配符模式 @Bean Binding bindingExchangeMessage(Queue queueMessage, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessage).to(exchange).with('.drivers.#'); &#125;&#125; RabbitListener: 123456789@Component@RabbitListener(queues = "location.broker")public class TopicReciver &#123; @RabbitHandler public void process(byte[] hello) &#123; System.out.println(new String(hello)); &#125;&#125; 使用java模拟的mqtt client 发送的消息为byte[]， 因此需要使用byte[]接收消息内容。 References Uniting AMQP and MQTT Message Brokering with RabbitMQ 使用rabbitmq做为mqtt服务器，整合spring做推送后台]]></content>
      <tags>
        <tag>amqp</tag>
        <tag>mqtt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo主题升级]]></title>
    <url>%2F2018%2F03%2F01%2Fhexo%E4%B8%BB%E9%A2%98%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[偶然看到最新的next主题，甚佳，决定对自己的博客主题进行升级，简要记录升级过程。 下载最新next 6.0.4, —&gt; 参见11，使用模块管理主题 备份_config.yml，同时对配置进行修改，相关图片位于next/source/images 1234cd themes/nextgit clone https://github.com/theme-next/theme-next-fancybox3 source/lib/fancyboxgit clone https://github.com/theme-next/theme-next-pace source/lib/pacegit clone https://github.com/theme-next/theme-next-algolia-instant-search source/lib/algolia-instant-search 其他插件 增加阅读进度 1git clone https://github.com/theme-next/theme-next-reading-progress source/lib/reading_progress 填补字符间空白 1git clone https://github.com/theme-next/theme-next-pangu.git source/lib/pangu 页面增加3D渲染，next默认提供两种渲染效果，theme-next-three和canvas_nest 123cd themes/nextgit clone https://github.com/theme-next/theme-next-three source/lib/threegit clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest 启用以下任意项： 1three_waves: true 1canvas_lines: true 1canvas_sphere: true 1canvas_nest: true 添加访问人数（6.0已原生支持busuanzi，启用即可） 打开\themes\next\layout\_partials\footer.swig文件,在copyright前加上画红线这句话 1&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 然后再合适的位置添加显示统计的代码，如图： 12345&lt;div class="powered-by"&gt;&lt;i class="fa fa-user-md"&gt;&lt;/i&gt;&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; pv的方式，单个用户连续点击n篇文章，记录n次访问量 uv的方式，单个用户连续点击n篇文章，只记录1次访客数 每篇文章末尾统一添加“本文结束”标记 实现方法 在路径 \themes\next\layout\_macro 中新建 passage-end-tag.swig 文件,并添加以下内容： 12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:15px;"&gt;--------------都看到这了，请我喝杯咖啡吧！&lt;i class="fa fa-coffee"&gt;&lt;/i&gt;--------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 接着打开\themes\next\layout\_macro\post.swig文件，在`` 之后， &lt;footer class=&quot;post-footer&quot;&gt; 之前添加 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 打开主题配置文件，文章末尾添加标记（不用设置） 12passage_end_tag: enabled: true 增加评论系统 gitment 主题配置 12345678910gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide 'Powered by ...' on footer, and more github_user: silloy # MUST HAVE, Your Github Username github_repo: repo # MUST HAVE, The name of the repo you use to store Gitment comments client_id: xxx # MUST HAVE, Github client id for the Gitment client_secret: xxx 问题： Error：validation failed 修改 next/layout/_third-party/comments/gitment.swig中id: window.location.pathname为 1id: &apos;&#123;&#123; page.date &#125;&#125;&apos; valine 文章底部增加版权信息 修改文章底部的那个带#号的标签 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 修改打赏字体不闪动，next 7.0 已支持配置 修改文件next/source/css/_common/components/post/post-reward.styl， 注释wechat:hover 和alipay:hover， 如下： 123456789101112/* 注释文字闪动函数 #wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125; #alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 模块化主题管理（以next主题为例） 备份next主题 mv next next-bak，提交代码 增加子模块 git submodule add git@github.com:silloy/hexo-theme-next.git themes/next 查看状态 git status 12345Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: .gitmodules new file: themes/next 提交 12345$ git commit -m "add next module"[hexo adbe36e] add next module 3 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 .gitmodules create mode 160000 themes/next 更新子模块 git submodule update --remote 拉取含子模块的项目，git clone 后执行以下操作 git submodule init 初始化本地配置文件 git submodule update 从该项目中抓取所有数据并检出父项目中列出的合适的提交 也可在 clone 使用 git clone --recursive 命令, git 就会自动初始化并更新仓库中的每一个子模块. 若子分支仓库中有未同步的更新, 可通过 git submodule update --remote --rebase 来同步最新的内容 同步源主题的修改 增加源 12cd theme/nextgit remote add source git@github.com:theme-next/hexo-theme-next.git 拉取更新 1git pull source master 等同于 123git fetch source mastergit checkout mastergit merge source/master 发布子模块的修改 使用 git push --recurse-submodules=check 命令 检查没有推送的子模块 使用 git push --recurse-submodules=on-demand git 会自动尝试推送变更的子项目 参考文章： NexT 使用文档 利用Gulp来压缩你的Hexo博客 hexo的next主题个性化教程:打造炫酷网站 老高博客 gitment Gitment评论功能接入踩坑教程 实现 Hexo next 主题博客本地站内搜索 我的个人博客之旅：从jekyll到hexo 在 hexo 中使用 git submodules 管理主题]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件预研-rabbitmq, rocketmq]]></title>
    <url>%2F2018%2F01%2F04%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E9%A2%84%E7%A0%94-kafka%2C%20rabbitmq%2C%20rocketmq%2F</url>
    <content type="text"><![CDATA[消息中间件在服务开发中起着重要的作用，应对业务需求，对rabbitmq，rocketmq进行预研，kafka暂时不做深入了解。 消息中间件应用场景 可以做延迟设计比如我们有一些数据，需要过五分钟后再被使用，这时候就需要使用延迟队列设计，比如在RabbitMQ中利用死信队列实现。具体实现在这里：http://www.cnblogs.com/haoxinyue/p/6613706.html 异步处理这个场景主要应用在多任务执行的场景。 应用解耦在大型微服务架构中，有一些无状态的服务经常考虑使用mq做消息通知和转换。 分布式事务最终一致性可以使用基于消息中间件的队列做分布式事务的消息补偿，实现最终一致性。 流量削峰一般在秒杀或团抢活动中使用广泛，可以通过队列实现秒杀的人数和商品控制，还可以缓解短时间压垮应用系统。 日志处理我们在做监控，或者日志采集的时候经常用队列来做消息的传输和暂存。 RocketMQ(Apache 4.2.0)概念Disk Flush (磁盘刷新/同步操作): 就是将内存的数据落地，存储在磁盘中. SYNC_FLUSH（同步刷盘）：生产者发送的每一条消息都在保存到磁盘成功后才返回告诉生产者成功。这种方式不会存在消息丢失的问题，但是有很大的磁盘IO开销，性能有一定影响。 ASYNC_FLUSH（异步刷盘）：生产者发送的每一条消息并不是立即保存到磁盘，而是暂时缓存起来，然后就返回生产者成功。随后再异步的将缓存数据保存到磁盘，有两种情况：1是定期将缓存中更新的数据进行刷盘，2是当缓存中更新的数据条数达到某一设定值后进行刷盘。这种方式会存在消息丢失（在还未来得及同步到磁盘的时候宕机），但是性能很好。默认是这种模式。 Broker Replication (Broker间数据同步/复制): 集群环境下需要部署多个Broker，Broker分为两种角色：一种是master，即可以写也可以读，其brokerId=0，只能有一个；另外一种是slave，只允许读，其brokerId为非0。一个master与多个slave通过指定相同的brokerName被归为一个broker set（broker集）。通常生产环境中，我们至少需要2个broker set。 Broker Replication只的就是slave获取或者是复制master的数据. Sync Broker：生产者发送的每一条消息都至少同步复制到一个slave后才返回告诉生产者成功，即“同步双写”。 Async Broker：生产者发送的每一条消息只要写入master就返回告诉生产者成功。然后再“异步复制”到slave。 start 十分钟入门RocketMQQuickStart: apache-quickstart 集群部署 使用不同配置文件启动nameserv(默认9876) 无状态节点，可集群部署，节点之间无任何信息同步（Broker与每个namesrv连接，可以保证信息同步性） nameserv的所有配置信息 12345678910111213141516rocketmqHome=/usr/local/rocketmqkvConfigPath=/Users/zhangyanghong/namesrv/kvConfig.jsonproductEnvName=centerclusterTest=falseorderMessageEnable=falselistenPort=9876serverWorkerThreads=8serverCallbackExecutorThreads=0serverSelectorThreads=3serverOnewaySemaphoreValue=256serverAsyncSemaphoreValue=64serverChannelMaxIdleTimeSeconds=120serverSocketSndBufSize=4096serverSocketRcvBufSize=4096serverPooledByteBufAllocatorEnable=trueuseEpollNativeSelector=false 通过修改listenPort在一台机器上部署启动两个nameserv nohup sh mqnamesrv -c mqnamesrv-a.conf &amp; 启动broker(默认10911)集群 broker的所用配置项 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122rocketmqHome=/usr/local/rocketmqnamesrvAddr=## 本机ip地址，默认系统自动识别brokerIP1=172.18.48.79brokerIP2=172.18.48.79brokerName=jiexiu’MacbrokerClusterName=DefaultClusterbrokerId=0brokerPermission=6defaultTopicQueueNums=8autoCreateTopicEnable=trueclusterTopicEnable=truebrokerTopicEnable=trueautoCreateSubscriptionGroup=truemessageStorePlugIn=sendMessageThreadPoolNums=32pullMessageThreadPoolNums=24adminBrokerThreadPoolNums=16clientManageThreadPoolNums=16flushConsumerOffsetInterval=5000flushConsumerOffsetHistoryInterval=60000rejectTransactionMessage=falsefetchNamesrvAddrByAddressServer=falsesendThreadPoolQueueCapacity=10000pullThreadPoolQueueCapacity=10000filterServerNums=0longPollingEnable=trueshortPollingTimeMills=1000notifyConsumerIdsChangedEnable=truehighSpeedMode=falsecommercialEnable=truecommercialTimerCount=1commercialTransCount=1commercialBigCount=1transferMsgByHeap=truemaxDelayTime=40regionId=DefaultRegionregisterBrokerTimeoutMills=6000slaveReadEnable=falsedisableConsumeIfConsumerReadSlowly=falseconsumerFallbehindThreshold=0waitTimeMillsInSendQueue=200startAcceptSendRequestTimeStamp=0listenPort=10911serverWorkerThreads=8serverCallbackExecutorThreads=0serverSelectorThreads=3serverOnewaySemaphoreValue=256serverAsyncSemaphoreValue=64serverChannelMaxIdleTimeSeconds=120serverSocketSndBufSize=131072serverSocketRcvBufSize=131072serverPooledByteBufAllocatorEnable=trueuseEpollNativeSelector=falseclientWorkerThreads=4clientCallbackExecutorThreads=4clientOnewaySemaphoreValue=65535clientAsyncSemaphoreValue=65535connectTimeoutMillis=3000channelNotActiveInterval=60000clientChannelMaxIdleTimeSeconds=120clientSocketSndBufSize=131072clientSocketRcvBufSize=131072clientPooledByteBufAllocatorEnable=falseclientCloseSocketIfTimeout=falsestorePathRootDir=/Users/zhangyanghong/storestorePathCommitLog=/Users/zhangyanghong/store/commitlogmapedFileSizeCommitLog=1073741824mapedFileSizeConsumeQueue=6000000flushIntervalCommitLog=1000flushCommitLogTimed=falseflushIntervalConsumeQueue=1000cleanResourceInterval=10000deleteCommitLogFilesInterval=100deleteConsumeQueueFilesInterval=100destroyMapedFileIntervalForcibly=120000redeleteHangedFileInterval=120000## 删除时间点，默认凌晨4点deleteWhen=04diskMaxUsedSpaceRatio=75## 文件保留时间，默认48小时fileReservedTime=72putMsgIndexHightWater=600000maxMessageSize=4194304checkCRCOnRecover=trueflushCommitLogLeastPages=4flushLeastPagesWhenWarmMapedFile=4096flushConsumeQueueLeastPages=2flushCommitLogThoroughInterval=10000flushConsumeQueueThoroughInterval=60000maxTransferBytesOnMessageInMemory=262144maxTransferCountOnMessageInMemory=32maxTransferBytesOnMessageInDisk=65536maxTransferCountOnMessageInDisk=8accessMessageInMemoryMaxRatio=40## 是否开启消息索引功能messageIndexEnable=truemaxHashSlotNum=5000000maxIndexNum=20000000maxMsgsNumBatch=64## 是否提供安全的消息索引机制，索引保证不丢messageIndexSafe=falsehaListenPort=10912haSendHeartbeatInterval=5000haHousekeepingInterval=20000haTransferBatchSize=32768haMasterAddress=haSlaveFallbehindMax=268435456## Broker的角色：ASYNC_MASTER异步复制Master; SYNC_MASTER同步双写MASTER; SLAVEbrokerRole=ASYNC_MASTERflushDiskType=ASYNC_FLUSHsyncFlushTimeout=5000messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2hflushDelayOffsetInterval=10000cleanFileForciblyEnable=truewarmMapedFileEnable=falseoffsetCheckInSlave=falsedebugLockEnable=falseduplicationEnable=falsediskFallRecorded=trueosPageCacheBusyTimeOutMills=1000defaultQueryMaxNum=32 其中重要的配置信息如下： 1234567891011121314151617181920212223242526namesrvAddr=brokerIP1=172.18.48.79brokerName=jiexiu’MacbrokerClusterName=DefaultClusterbrokerId=0autoCreateTopicEnable=trueautoCreateSubscriptionGroup=truerejectTransactionMessage=falsefetchNamesrvAddrByAddressServer=falsestorePathRootDir=/Users/zhangyanghong/storestorePathCommitLog=/Users/zhangyanghong/store/commitlogflushIntervalCommitLog=1000flushCommitLogTimed=falsedeleteWhen=04fileReservedTime=72maxTransferBytesOnMessageInMemory=262144maxTransferCountOnMessageInMemory=32maxTransferBytesOnMessageInDisk=65536maxTransferCountOnMessageInDisk=8accessMessageInMemoryMaxRatio=40messageIndexEnable=truemessageIndexSafe=falsehaMasterAddress=brokerRole=ASYNC_MASTERflushDiskType=ASYNC_FLUSHcleanFileForciblyEnable=true 启动broker sh mqbroker -n &#39;127.0.0.1:9876;127.0.0.1:9877&#39; -c ../conf/2m-noslave/broker-a.properties &gt; /dev/null 2&gt;&amp;1 &amp; 集群验证 sh mqadmin clusterList -n 127.0.0.1:9876 输出信息 123#Cluster Name #Broker Name #BID #Addr #Version #InTPS(LOAD) #OutTPS(LOAD) #PCWait(ms) #Hour #SPACEDefaultCluster broker-a 0 172.18.48.79:10911 V3_5_8 0.00(0,0ms) 0.00(0,0ms) 0 412299.47 0.5476DefaultCluster broker-b 0 172.18.48.79:12911 V3_5_8 0.00(0,0ms) 0.00(0,0ms) 0 412299.47 0.5476 默认的集群配置conf子目录下 1232m-2s-async // 两个master 两个slave异步2m-2s-sync // 两个master 两个slave同步2m-noslave // 两个master 没有slave broker 的master和slave (Slave 不可写，但可读) 单个master: 风险较大, 不建议生产使用 多master: 配置简单，消息可靠，性能最高 单台机器宕机期间，未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响 多master多slave，异步复制: 每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用异步复制方式，主备有短暂消息延迟，毫秒级 优点: 即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样 缺点: Master 宕机，磁盘损坏情况，会丢失少量消息 多master多slave，同步双写: 每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用同步双写方式，主备都写成功，向应用返回成功 优点: 数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高 缺点: 性能比异步复制模式略低，大约低 10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。 问题 出现 Lock failed,MQ already started 解决方案: 修改配置文件中的storePathRootDir项 启动 mqnamesrv / mqbroker服务报错，显示内存不够，需大于2G？具体表现: “VM warning: INFO: OS::commit_memory(0x00000006c0000000, 2147483648, 0) faild; error=’Cannot allocate memory’ (errno=12)” 解决方案：修改/RocketMQ/devnev/bin/ 下的服务启动脚本 runserver.sh 、runbroker.sh 中对于内存的限制，改成如下示例： JAVA_OPT=&quot;${JAVA_OPT} -server -Xms128m -Xmx128m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=128m&quot; 启动rocketmq-consolemvn spring-boot:run or java -jar rocketmq-console-ng-1.0.0.jar --server.port=12581 --rocketmq.config.namesrvAddr=10.89.0.64:9876;10.89.0.65:9876 topicrocketmq中一个broker-name其实就相当于kafka-broker中的一个partition，而rocketmq每一个slave就相当于kafka中的一个replication，这种情况，所以rocketmq的特点相当于单个partition支持多队列，大致的原理图如下： 默认: 一个topic的队列数是8 问题 producer生产消息过多，customer来不及消费? 消息的重试机制 broker和client/producer版本不一致问题 解决: 会导致已消费消息堆积，重启customer会重复消费，更换一致版本 队列个数设置 producer发送消息时候设置，特别注意：同一个topic仅当第一次创建的时候设置有效，以后修改无效，除非修改broker服务器上的consume.json文件， demo：mqProducer.setDefaultTopicQueueNums(5) 参考：http://www.mamicode.com/info-detail-327693.html 其他常见问题3.2.4 RabbitMQ安装erlang 使用kerl安装和管理erlang，参考 Erlang版本管理工具: Kerl , 安装Erlang/OTP的简单方法, 其他安装方法 在CentOS上安装erlang 设置环境变量 安装 RabbitMQ 下载rabbit rpm包 错误 1234Error: Package: rabbitmq-server-3.7.2-1.el7.noarch (/rabbitmq-server-3.7.2-1.el7.noarch) Requires: erlang &gt;= 19.3 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest 执行 rpm --nodeps -ivh rabbitmq-server-3.7.2-1.el7.noarch.rpm 启动命令(/usr/lib/rabbitmq /etc/rabbitmq —- /usr/share/doc/rabbitmq-server-3.7.2) 12345service rabbitmq-server startservice rabbitmq-server stopservice rabbitmq-server restartservice rabbitmq-server status./rabbitmqctl stop 开启管理功能 rabbitmq-plugins enable rabbitmq_management 启动服务 rabbitmq-server -detached 添加用户权限 12rabbitmqctl add_user albert passwordrabbitmqctl set_user_tags albert administrator 开机自启动 chkconfig rabbitmq-server on 修改配置文件，开启远程用户访问 12cp /usr/share/doc/rabbitmq-server-3.7.2/rabbitmq.config.example /etc/rabbitmq/ mv rabbitmq.config.example rabbitmq.config 增加 {loopback_users, []} 集群部署 RabbitMQ 配置初步 Centos7 安装rabbitmq reference How-to-install-rabbitmq-on-centos-7 重要概念: 左侧 P 代表 生产者，也就是往 RabbitMQ 发消息的程序。 中间即是 RabbitMQ，其中包括了 交换机 和 队列。 右侧 C 代表 消费者，也就是往 RabbitMQ 拿消息的程序。 重要的概念：虚拟主机，交换机，队列，和绑定 虚拟主机：一个虚拟主机持有一组交换机、队列和绑定。为什么需要多个虚拟主机呢？很简单，RabbitMQ当中，用户只能在虚拟主机的粒度进行权限控制。 因此，如果需要禁止A组访问B组的交换机/队列/绑定，必须为A和B分别创建一个虚拟主机。每一个RabbitMQ服务器都有一个默认的虚拟主机“/”。 交换机：Exchange 用于转发消息，但是它不会做存储 ，如果没有 Queue bind 到 Exchange 的话，它会直接丢弃掉 Producer 发送过来的消息。这里有一个比较重要的概念：路由键 。消息到交换机的时候，交互机会转发到对应的队列中，那么究竟转发到哪个队列，就要根据该路由键。 绑定：也就是交换机需要和队列相绑定，这其中如上图所示，是多对多的关系。 交换机(Exchange)交换机的功能主要是接收消息并且转发到绑定的队列，交换机不存储消息，在启用ack模式后，交换机找不到队列会返回错误。交换机有四种类型：Direct, topic, Headers and Fanout Direct：direct 类型的行为是”先匹配, 再投送”. 即在绑定时设定一个 routing_key, 消息的routing_key 匹配时, 才会被交换器投送到绑定的队列中去. Topic：按规则转发消息（最灵活） Headers：设置header attribute参数类型的交换机 Fanout：转发消息到所有绑定队列 性能比较: RabbitMQ三种Exchange模式(fanout,direct,topic)的性能比较 理解rabbitmq的概念 : http://tryrabbitmq.com/ 示例代码: boot-in-action References 消息队列中间件调研文档 几款消息中间的调研 消息队列及常见消息队列介绍 Understanding When to use RabbitMQ or Apache Kafka 高吞吐、高可用MQ对比分析 Kafka、RabbitMQ、RocketMQ消息中间件的对比 —— 消息发送性能 消息队列设计精要 分布式开放消息系统(RocketMQ)的原理与实践 RabbitMQ详解 消息队列探秘-RabbitMQ消息队列介绍 RabbitMq延迟、重试队列及Spring Boot的黑科技 rabbitmq可靠发送的自动重试机制]]></content>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链路监控浅析---以sleuth-zipkin和skywalking为例]]></title>
    <url>%2F2018%2F01%2F02%2F%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7%E6%B5%85%E6%9E%90---%E4%BB%A5sleuth-zipkin%E5%92%8Cskywalking%E4%B8%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[APM (Application Performance Management &amp; Monitoring)四种实现思路 基于日志系统，探针只负责对日志加上编号，又类似ELK的系统进行收集、处理、展示。这方面没有很成熟的产品，一般都属于公司内部封装的框架。 自动探针，适用语言：Java、C#、PHP、Node.js等等存在VM的语言。绝对大多数的商业产品和热门的开源产品都属于这个系列。 全手动探针，优势是适用范围广，最有名的就是Zipkin的整个生态系统，分布式追踪几乎无处不在。也是现在全球运用最广泛的分布式监控系统。 同时支持自动和手动模式的探针，适用语言同样是Java、C#、PHP、Node.js等等存在VM的语言，由于技术复杂性提高，运用的较少。优点是入门方便，同时使用灵活。商业上主要是Instana，开源主要是sky-walking提供了技术解决方案。 三大模块: 探针或sdk ：负责数据采集和发送。探针或 SDK 是应用程序的收集端。一般使用插件的模式，自动探针一般是不需要修改程序，而 SDK 则是需要修改部分配置或者代码。skywalking 就是自动探针为主，zipkin-brave 就是 Zipkin 的 Java 手动探针 collector模块 ：负责数据收集、分析、汇总、告警和存储。Collector 模块，这个根据不同的 APM 实现，可能由一个或者多个子系统构成。Collector 负责对探针和 SDK 提供网络接口（TCP、UDP、HTTP 不同形式接口） UI ，负责高实时性展现。包括但不限于 Trace 的查询，统计数据展现，拓扑图展现，VM 或进程相关信息等，监控关键数据的展现 监控概述-sleuth实现 Sleuth-Zipkinsleuth用来和zipkin(twitter)集成，自动完成span, trace等信息的生成，接入http request, 以及向zipkin server 发送采集信息，实现分布式服务跟踪能力。全链路spring cloud sleuth+zipkin 结构图 搭建zipkin-server 引入 123456789&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 添加注解 @EnableZipkinServer rest服务调用建立两个基本的rest服务，不再赘述 引入 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置 spring.zipkin.base-url=http://127.0.0.1:9418 启动 LOG: aplication-name.TraceId.SpanId.BOOL 其他配置1spring.sleuth.sampler.percentage=1 数据持久化 zipkin-server引入 12345678910111213141516&lt;!--此依赖会自动引入spring-cloud-sleuth-stream并且引入zipkin的依赖包(可以去除zipkin-server) --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--持久化数据到elasticsearch--&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch-http&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; client引入 none 注解修改 @EnableZipkinStreamServer 问题 4.1. 引入es后不能显示dependency tree zipkin-dependencies 12wget -O zipkin-dependencies.jar 'https://search.maven.org/remote_content?g=io.zipkin.dependencies&amp;a=zipkin-dependencies&amp;v=LATEST'STORAGE_TYPE=elasticsearch ES_HOSTS=host1,host2 java -jar zipkin-dependencies.jar 概念 Span ：基本工作单元，发送一个远程调度任务 就会产生一个Span，Span是一个64位ID唯一标识的，Trace是用另一个64位ID唯一标识的，Span还有其他数据信息，比如摘要、时间戳事件、Span的ID、以及进度ID Trace ：一系列Span组成的一个树状结构。请求一个微服务系统的API接口，这个API接口，需要调用多个微服务，调用每个微服务都会产生一个新的Span，所有由这个请求产生的Span组成了这个Trace。 Annotation ：用来及时记录一个事件的，一些核心注解用来定义一个请求的开始和结束 。这些注解包括以下： cs - Client Sent -客户端发送一个请求，这个注解描述了这个Span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络传输的时间 ss - Server Sent （服务端发送响应）–该注解表明请求处理的完成(当请求返回客户端)，如果ss的时间戳减去sr时间戳，就可以得到服务器请求的时间。 cr - Client Received （客户端接收响应）-此时Span的结束，如果cr的时间戳减去cs时间戳便可以得到整个请求所消耗的时间。 Log: span跨度内发生的事件，异常等，包含tag, annotation, log, event等 sky-walking提供分布式事务跟踪，以及APM性能监控。 skywalking， 使用javaagent技术使得应用监控0耦合 架构 部署 下载agent, collector, ui 三个组件 启动collector, ui 修改application-name, 启动agent -javaagent:/path/to/skywalking-agent/skywalking-agent.jar 其他的一些APM工具: Pinpoint, CAT, Xhprof/Xhgui]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS linux(AMI) 搭建VPN]]></title>
    <url>%2F2016%2F11%2F02%2FAWS%20linux(AMI)%20%E6%90%AD%E5%BB%BAVPN%2F</url>
    <content type="text"><![CDATA[AWS提供一年的免费试用($2.00)，试着在amazon linux(AMI) 上搭建vpn，简要记录一下搭建过程。 安装过程如下： ssh配置 给pem权限 chmod 400 data_korea.pem 登录ec2 ssh -i &quot;data_korea.pem&quot; ec2-user@ec2-52-78-70-0.ap-northeast-2.compute.amazonaws.com 修改默认用户和root密码 sudo passwd ec2-user suod paddwd root 切换root用户，修改文件 su root vim /etc/ssh/sshd_config 123PermitRootLogin yesPubkeyAuthentication noPasswordAuthentication yes reboot或者重启ssh /etc/init.d/sshd restart vpn安装 安装ppp yum install ppp 下载并安装pptpd wget http://poptop.sourceforge.net/yum/stable/packages/ppp-2.4.5-33.0.fc21.x86_64.rpm wget http://poptop.sourceforge.net/yum/stable/packages/pptpd-1.4.0-1.el6.x86_64.rpm rpm -Uhv pptpd*.rpm 添加DNS服务器 (可选) 打开vim /etc/ppp/options.pptpd 文件并添加入如下内容： ms-dns 8.8.8.8 ms-dns 8.8.4.4 ms-dns 4.4.4.4 以上2个是Google提供的免费DNS 添加 VPN 帐号 在/etc/ppp/chap-secrets文件中添加VPN用户，格式为“用户名 服务器 密码 IP地址”： vpnuser pptpd myVPN$99 * 打开IP转发(IP Forward)功能 在 /etc/sysctl.conf 文件中修改： net.ipv4.ip_forward = 1 保存设置 sysctl -p 在 IP Tables 中开启 IP 伪装(IP Masquerade) iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 如果你需要这个设置在重启之后依然有效，则需要把这一行添加到 /etc/rc.local 的末尾。 把 pptpd 设置成自动运行的 chkconfig pptpd on 重启pptpd服务 service pptpd restart ec2控制台打开TCP的1723端口，这是pptpd的默认连接端口。 References：https://leonax.net/p/3274/install-vpn-server-on-amazon-ec2/ AMI 软件更新 jdk 更新 12345rpm -qa|grep java //查询系统jdkrpm -e --allmatches --nodeps java-1.6.0-openjdk-1.6.0.37-1.13.9.4.el5_11 //删除老版本yum -y list java* (yum search jdk) //查询软件包内的jdkyum install java-1.8.0-openjdk.x86_64 //安装新版本java -version //验证 jdk的安装路径加入到JAVA_HOME vi /etc/profile 12345#set java environmentJAVA_HOME=/usr/lib/jvm/jre-1.6.0-openjdk.x86_64PATH=$PATH:$JAVA_HOME/binCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME CLASSPATH PATH . /etc/profile]]></content>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL定时任务]]></title>
    <url>%2F2016%2F08%2F04%2FMySQL%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[简介数据定时更新非常有必要，自MySQL5.1.6起，增加了事件调度器，可以用来执行某些定时任务。简要记录一下创建过程。 配置 windows10 MySQL5.6 过程开启event_scheduler set global event_scheduler = 1; my.cnf 加上 event_scheduler = 1 set global event_scheduler = ON; mysqld --event_scheduler=1; 查看是否开启了event_scheduler show varuables like &#39;event_scheduler&#39;; select @@event_scheduler; show processlist; 创建事件(create event)语法123456CREATE EVENT [IFNOT EXISTS] event_nameONSCHEDULE schedule[ONCOMPLETION [NOT] PRESERVE][ENABLE | DISABLE][COMMENT &apos;comment&apos;]DO sql_statement; schedual: 12AT TIMESTAMP [+ INTERVAL INTERVAL]| EVERY INTERVAL [STARTS TIMESTAMP] [ENDS TIMESTAMP] INTERVAL: 123quantity &#123;YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE |WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE |DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND&#125; 示例 每秒插入一条记录 12345USE tarena;CREATE TABLE aaa (timeline TIMESTAMP);CREAT EEVENT e_test_insertON SCHEDULE EVERY 1 SECONDDO INSERTINTO tarena.aaa VALUES(CURRENT_TIMESTAMP); 5天后清空表 123CREATE EVENT e_testON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 5 DAYDO TRUNCATETABLE tarena.aaa; 预约清空 123CREATE EVENT e_testON SCHEDULE AT TIMESTAMP &apos;2007-07-20 12:00:00&apos;DO TRUNCATETABLE tarena.aaa; 定时清空 123CREATE EVENT e_testON SCHEDULE EVERY 1 DAYDO TRUNCATETABLE tarena.aaa; 预约定时清空 1234CREATE EVENT e_testONSCHEDULE EVERY 1 DAYSTARTS CURRENT_TIMESTAMP+ INTERVAL 5 DAYDO TRUNCATETABLE tarena.aaa; 定时清空，一段时间后停止 1234CREATE EVENT e_testON SCHEDULE EVERY 1 DAYENDS CURRENT_TIMESTAMP+ INTERVAL 5 DAYDO TRUNCATETABLE test.aaa; 预约定时清空，一段时间后停止 12345CREATE EVENT e_testON SCHEDULE EVERY 1 DAYSTARTS CURRENT_TIMESTAMP+ INTERVAL 5 DAYENDS CURRENT_TIMESTAMP+ INTERVAL 1 MONTHDO TRUNCATETABLE test.aaa; 定时清空，执行一次后终止[ON COMPLETION [NOT] PRESERVE]可以设置这个事件是执行一次还是持久执行，默认为NOT PRESERVE。 1234CREATE EVENT e_testON SCHEDULE EVERY 1 DAYON COMPLETION NOT PRESERVEDO TRUNCATETABLE test.aaa; [ENABLE | DISABLE]可是设置该事件创建后状态是否开启或关闭，默认为ENABLE。 [COMMENT ‘comment’]可以给该事件加上注释。 修改事件(ALTER EVENT)语法1234567ALTER EVENT event_name[ONSCHEDULE schedule][RENAME TOnew_event_name][ONCOMPLETION [NOT] PRESERVE][COMMENT &apos;comment&apos;][ENABLE | DISABLE][DO sql_statement] 临时关闭事件ALTER EVENT e_test DISABLE; 开启事件ALTER EVENT e_test ENABLE; 时间点修改ALTER EVENT e_test ON SCHEDULE EVERY 5 DAY; 删除事件(DROP EVENT)语法DROP EVENT [IF EXISTS] event_name 案例123456789101112delimiter //create procedure `Slave_Monitor`()beginSELECT VARIABLE_VALUE INTO @SLAVE_STATUSFROM information_schema.GLOBAL_STATUSWHERE VARIABLE_NAME=&apos;SLAVE_RUNNING&apos;;IF (&apos;ON&apos;!= @SLAVE_STATUS) THENSET GLOBAL SQL_SLAVE_SKIP_COUNTER=0;SLAVE START;END IF;end; //delimiter ; 1234CREATE EVENT IFNOT EXISTS `Slave_Monitor`ON SCHEDULE EVERY 5 SECONDON COMPLETION PRESERVEDO CALL Slave_Monitor();]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Linux命令查看硬件信息]]></title>
    <url>%2F2016%2F07%2F20%2F%E4%BD%BF%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[在linux检查和查看硬件信息有分很多命令，这里列出一些命令快速查看linux的cpu和内存等信息。 lscpu 直接使用即可，没有多余的选项和功能 lspci 可以列出所有连接到PCI总线的详细信息，例如：显卡，网卡，USB接口及SATA控制器等设备。 ​ 可以使用类似如下命令过滤出特定的设备信息 1lspci -v | grep &quot;VGA&quot; -A 12 lshw 通用工具，可以执行多个硬件如CPU，硬件，USB控制器及磁盘等详细信息。在执行之后会自动提取不同”/proc”文件中的信息。 lsusb 显示连接到此计算机的USB控制器的详细信息，可以使用-v选项来输出每个usb端口的详细信息。 lnxi 用来获取多项目硬件信息的脚本工具，可以为用户输入一个详细的硬件报告，默认未安装在ubuntu系统当中，可以使用如下命令安装： sudo apt-get install inxi使用 inxi -Fx 输出硬件报告 df 输出当前Linux系统中个各种分区及其挂载点，可以使用-H参数df -H free 查看当前系统的内存信息free -m dmidecode 主要通过读取DMI表中数据来提取硬件信息。查看CPU信息sudo dmidecode -t processor 查看内存信息sudo dmidecode -t memory 查看BIOS信息 sudo dmidecode -t bios hdparm 读取SATA设备（eg.硬盘）的相关信息sudo hdparm]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window添加sublime右键菜单]]></title>
    <url>%2F2016%2F06%2F28%2Fwindow%E6%B7%BB%E5%8A%A0sublime%E5%8F%B3%E9%94%AE%E8%8F%9C%E5%8D%95%2F</url>
    <content type="text"><![CDATA[1.打开注册表编辑器，开始-&gt;运行-&gt;regedit。 2.在HKEY_CLASSSES_ROOT→ * → Shell 下，在Shell下，新建项命名为Open With Sublime Text，在该新建项的右边窗口新建字符串值（右键–新建–字符串值）。名称：Icon；值：D:\Program Files\Sublime Text 3\sublime_text.exe,0 【注：使用您自己的安装文件目录】。 3.在新建的项Open With Sublime Text下面新建项Command（必须这个名称）.修改Command项右侧窗口的默认值，修改为：”D:\Program Files\Sublime Text 3\sublime_text.exe” “%1”【注：使用您自己的安装文件目录】，双引号一定要加，否则无法打开路径带空格的文件，这样就大功告成了。]]></content>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL修改密码]]></title>
    <url>%2F2016%2F06%2F27%2FMySQL%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[记录MySQL修改密码的几种方式 用SET PASSWORD命令首先登录MySQL。格式：mysql&gt; set password for 用户名@localhost = password(‘新密码’);例子：mysql&gt; set password for root@localhost = password(‘123’); 用mysqladmin格式：mysqladmin -u用户名 -p旧密码 password 新密码例子：mysqladmin -uroot -p123456 password 123 用UPDATE直接编辑user表首先登录MySQL。mysql&gt; use mysql;mysql&gt; update user set password=password(‘123’) where user=’root’ and host=’localhost’;mysql&gt; flush privileges; 在忘记root密码的时候，可以这样以windows为例： 关闭正在运行的MySQL服务。 打开DOS窗口，转到mysql\bin目录。 输入mysqld –skip-grant-tables 回车。–skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\bin目录。 输入mysql回车，如果成功，将出现MySQL提示符 &gt;。 连接权限数据库： use mysql; 。 改密码：update user set password=password(“123”) where user=”root”;（别忘了最后加分号） 。 刷新权限（必须步骤）：flush privileges; 。 退出 quit。 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo同步]]></title>
    <url>%2F2016%2F06%2F22%2Fhexo%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[简要介绍一下hexo搭建的github page同步过程。 我已经在github上建立了hexo的源码分支hexo，以及主页分支master。过程： 准备工作 git(Cygwin)，nodejs(win10)安装 依次执行的命令 git clone -b hexo git@github.com:silloy/silloy.github.io.git hexo cd hexo npm install -g hexo-cli npm install npm install hexo-deployer-git npm install -g npm-check npm-check -u （npm-check -u -g） 安装其他依赖包 https://github.com/theme-next/theme-next-canvas-nest https://github.com/theme-next/theme-next-fancybox3 https://github.com/theme-next/theme-next-pace 参考资料： 使用hexo，如果换了电脑怎么更新博客？]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vm环境安装linuxmint]]></title>
    <url>%2F2016%2F05%2F14%2Fvm%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85linuxmint%2F</url>
    <content type="text"><![CDATA[选择轻量级Linux系统linuxmint搭建需要，做本地测试 准备工作 linux mint下载：linuxmint-17.3-kde-64bit.iso vmvare workstation 12 player 安装 安装过程不再叙述，全程傻瓜式安装 基本设置 分辨率，壁纸， 语言 卸载装机软件 libreoffice sudo apt-get purge libreoffice?or sudo aptitude purge libreoffice?or sudo apt-get remove --purge libreoffice* 设置软件源，并更新系统 sudo apt-get update sudo apt-get upgrade main选择： ustc base选择： aliyun 浏览器主页，搜索引擎设置安装GuakeTerminal sudo apt-get install guake docs: GuakeTerminal──linux下完美帅气的终端 安装VMWARE tools安装输入法(不适用于linuxmint18) sudo add-apt-repository ppa:fcitx-team/nightly sudo aptitude update sudo aptitude install fcitx fcitx-sogoupinyin fcitx-config-gtk fcitx-frontend-all fcitx-module-cloudpinyin fcitx-ui-classic 下载sogou for linux , 打开im-config设置fcitx，安装sougou reboot 配置文件（注意取消only show current language） 安装markdown编辑器 推荐 cmd_markdown 开发常用软件 docs：Linux mint 安装JAVA jdk jre sudo apt-get install default-jre open-jdk sudo apt-get install default-jdk jdk sudo add-apt-repository ppa:webupd8team/java sudo apt-get update sudo apt-get install oracle-java8-installer sudo apt-get install oracle-java8-set-default docs： 怎样在Ubuntu 14.04中安装JavaLinux下配置Java环境变量]]></content>
      <tags>
        <tag>linux-mint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle基础命令]]></title>
    <url>%2F2016%2F05%2F09%2FOracle%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[主流关系型数据库: Oracle，DB2，Sybase，SQL server，MySQL这里主要记录一下oracle数据库初步学习过程，涉及少量与mysql的不同讨论。基本的SQL知识 SQL（Structured Query Language）常用关键字 SQL可分为 数据定义语言DDL: CREATE, ALTER, DROP, TRUNCATE 数据操纵语言DML: INSERT, UPDATE, DELETE 事务控制语言TCL: COMMIT, ROLLBACK, SAVEPOINT 数据查询语言DQL: SELECT 数据控制语言DCL: GRANT, REVOKE, CREATE USER NUMBER: 数字类型 定义: NUMBER（P，S） FYI: MySQL里没有number类型，对应的是int，float…… CHAR: 固定长度的字符类型 定义: CHAR(N) 最多保存2000字节 FYI: 对应MySQL里的CHAR VARCHAR2: 变长的字符类型 定义: VARCHAR2(N) 最多保存4000字节 FYI: 对应MySQL里的VARCHAR DATE: 日期时间数据 默认格式: DD-MON-RR DEFAULT: 指定默认值 NOT NULL: 非空约束 DROP 删除列或表 LONG: VARCHAR2加长版 限制: 每个表只能有一个long列，不能作为主键，不能建立索引，不能出现在查询条件中 FYI: MySQL没有long型 CLOB: 存储定长或变长字符串，oracle建议使用clob替代long型 执行DML操作后，需要在执行commit，才算真正确认了此操作 常用函数 UPPER, LOWER, INITCAP: 大小写转换 TRIM, LTRIM, RTRIM: 截去子串 LPAD, RPAD: 补位函数 LPAD(char1, n, char2) RPAD(char1, n, char2) SUBSTR: 获取子串 SUBSTR(char, m[, n]) INSTR: 返回子串在父串中的位置 INSTR(char1, char2[, n[, m]]) n: 开始搜索的位置，默认为1 m: 指定子串第m次出现，默认为1 没找到，返回0 ROUND：四舍五入 ROUND(n[, m]) n: 可以是任何数字，指要被处理的数字 m：必须是整数，四舍五入的位数，可以是负数，默认是0 MOD: 取余 MOD(m, n) n为0直接返回m CEIL CEIL(n) FLOOR FLOOR(n) CONCAT, “||” 日期操作 DATE 范围：公元前4712年1月1日至公元9999年12月31日 占用7个字节 byte 1: 世纪+100 byte 2: 年 byte 3: 月 byte 4: 日 byte 5: 小时 + 1 byte 6: 分 + 1 byte 7: 秒 + 1 TIMESTAMP 最高精度可以到ns 占用7或者11个字节，精度为0，用7字节存储，精度大于0用11字节存储 精度：第8-11字节，内部运算类型为整型 SYSDATE: SystemDate SYSTIMESTAMP: 返回当前系统日期和时间，精确到毫秒 TO_DATE：字符串转换为日期类型 TO_DATE(char[, fmt[, nlsparams]]) fmt: 格式 - nlsparams: 指定日期语言 TO_CHAR TO_CHAR(date[, fmt[, nlsparams]]) LAST_DAY LAST_DAY(date): 返回日期date所在月的最后一天 ADD_MONTHS ADD_MONTHS(date, i) MONTHS_BETWEEN MONTHS_BETWEEN(date1, date2): 计算月份差 NEXT_DAY NEXT_DAY(date, char): 返回date日期的下一个周几 LEAST, GREATEST GREATEST(expr1[,expr2[, expr3]]…) EXTRACT EXTRACT(date from datetime): 提取日期中的年、月、日等 空值操作 NVL NVL(expr1, expr2): expr1为null则转变为expr2，数据类型必须一致 NVL2 NVL2(expr1, expr2, expr3): expr1非null返回我想expr2，为null返回expr3 高级查询子查询分页查询 ROWNUM：伪列，用于返回标识行数据顺序的数字 只能从1计数，不能从结果集中直接截取 部分数据需要用到行内视图 PageN: (n - 1) * pageSize + 1 ~ n * pageSize DECODE函数 DECODE(expr, search1, result1[,search2, result2….][, default]) 用途： 比较expr的值，若匹配到哪一个search，返回对应result，类似于case语句 应用场景： 分组查询 12SELECT DECODE(job, &apos;ANALYST&apos;, &apos;VIP&apos;, &apos;MANAGER&apos;, &apos;VIP&apos;, &apos;OPERATION&apos;)job, COUNT(1)job_count FROM emp GROUP BY DECODE(job, &apos;ANALYST&apos;, &apos;VIP&apos;, &apos;MANAGER&apos;, &apos;VIP&apos;, &apos;OPERATION&apos;); 字段内容排序ORDER BY DECODE(job, &#39;ANALYST&#39;, &#39;1&#39;, &#39;MANAGER&#39;, &#39;2&#39;, &#39;OPERATION&#39;, &#39;3&#39;) ROW_NUMBER函数 ROW_NUMBER() OVER(PARTITION BY col1 ORDER BY col2): 根据col1分组，在分组内部根据col2排序 此函数计算的值标识每组内部排序后的顺序编号，组内连续且唯一 RANK_NUMBER函数 类似于ROW_NUMBER, 跳跃排序，可以有重复值 DENSE_RANK 类似于RANK_NUMBER, 连续排序，有重复，无跳跃 UNION UNION: 自动合并去重，排序 UINION ALL：合并不去重，不排序 INTERSECT：交集 MINUS：差集 高级分组函数 ROLLUP: 从右向左以一次少一列的方式组合直到所有列都去掉 GROUP BY ROLLUP(a, b, c) CUBE: 所以维度的取值集合 GROUP BY CUBE(a, b, c) GROUPING SETS: GROUP BY GROUPING SETS(a, b, c) 视图 视图(VIEW) 也称虚表，是一组数据的逻辑表示，对应于一条select语句 分类： 简单视图 复杂视图 连接视图 作用： 简化复杂查询 限制数据访问 GRANT CREATE VIEW TO user; 限制约束 [WITH CHECK OPTION] [WITH READ ONLY] 序列 创建序列 CREATE SEQUENCE[schema.]sequence_name[START WITH i][INCREMENT BY j][MAXVALUE m | NOMAXVALUE][MINVALUE n | NOMINVALUE][CYCLE | NOCYCLE][CACHE p | NOCACHE] 使用序列 NEXTVAL: 获取序列的下个值 CURRVAL：获取序列的当前值 索引 索引(INDEX)是一种允许直接访问数据表中某一数据行的数据结构，为了提高查询效率而引入，是独立于表的对象，可以存放与表不同的表空间中 Syntax： CREATE [UNIQUE] INDEX index_name ON table(column[, column…]); UNIQUE表示唯一索引 约束 非空约束 NOT NULL 唯一性约束 UNIQUE 主键约束 PRIMARY KEY 外键约束 FOREIGN KEY 检查约束 CHECK]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
</search>
