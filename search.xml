<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java8之扩展函数式接口]]></title>
    <url>%2F2019%2F09%2F29%2FJava8%E4%B9%8B%E6%89%A9%E5%B1%95%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[函数式接口先看一下官方定义 Functional interfaces provide target types for lambda expressions and method references. 可以看出函数式接口主要用于lambda表达式，这类接口只定义了唯一的抽象方法的接口（除了隐含的Object对象的公共方法），一开始也称SAM类型接口(Single Abstract Method)。 简单使用1234List&lt;Integer&gt; list = Lists.newArrayList(1,2,3); list.forEach(r -&gt; &#123; System.out.println("re = " + Math.sqrt(r)); &#125;); 看一下 foreach 实现，在Iterable.java中 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125; &#125; 这里出现的Consumer就是一个函数式接口， java8 提供了一些常用的函数式接口 Predicate – 传入一个参数，返回一个bool结果， 方法为boolean test(T t) Consumer – 传入一个参数，无返回值，纯消费。 方法为void accept(T t) Function – 传入一个参数，返回一个结果，方法为R apply(T t) Supplier – 无参数传入，返回一个结果，方法为T get() UnaryOperator – 一元操作符， 继承Function,传入参数的类型和返回类型相同。 BinaryOperator – 二元操作符， 传入的两个参数的类型和返回类型相同， 继承BiFunction 这里就不一一列举了，具体请见 java.util.function 包 都很简单，不太清楚的自行google 扩展但是jdk提供的有时候不一定能满足需求，这个时候就需要我们自定义函数式接口 普通的 Function 或者 Consumer 只能就收一个参数，BiFuntion 和 BiConsumer 也只能接受连个参数，参数更多的情况就无法满足了 以 consumer 为例，先自定义一个接口 12345678910111213@FunctionalInterface public interface TriConsumer&lt;T, U, W&gt; &#123; void accept(T t, U u, W w); default TriConsumer&lt;T, U, W&gt; andThen(TriConsumer&lt;? super T, ? super U, ? super W&gt; after) &#123; Objects.requireNonNull(after); return (l, r, w) -&gt; &#123; accept(l, r, w); after.accept(l, r, w); &#125;; &#125; &#125; 函数式接口一般使用 @FunctionalInterface 注解注释，以申明该接口是一个函数式接口， 这里提供一个 andThen 方法以支持连续调用 使用方法 1234TriConsumer&lt;Integer, Integer, Integer&gt; consumer = (a, b, c) -&gt; &#123; System.out.println(a + b + c); &#125;; consumer.accept(5,6,7); funtion类似，这里就不举例了 异常捕获 FunctionalInterface 提供的接口一般是不抛出异常的，意味着我们在使用的时候需要在方法体内部捕获异常，这里定义一种可以抛出异常的接口 1234@FunctionalInterface public interface InterfaceException&lt;T&gt; &#123; void apply(T t) throws Exception; &#125; References Java 8函数式接口functional interface的秘密]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis底层原理-Strings]]></title>
    <url>%2F2019%2F08%2F21%2Fredis%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86-Strings%2F</url>
    <content type="text"><![CDATA[翻译自 Under the Hood of Redis: Strings 你知道简单string strings 在redis里占用了56 bytes的内存吗？ 我会试图告诉你为什么，了解redis运行原理时非常重要的。当你试图构建一个高负载的应用显的尤为重要，同时，你很快就会理解你的redis实例为什么会消费大量的内存？ 这篇文章主要介绍以下几个主题： strings 在redis里如何存储 strings 的内部结构是什么样的 redis 使用的优化机制 依据不同场景，如何有效的使用strings或者以此为基础的结构 Strings是redis里最常用的数据结构。 HSET/ZSET/LIST 在内部结构上都会增加一定的开销。过去一年，我在 stackoverflow 上浏览了大量关于redis的答案， 让我意识到大量的开发者并不理解reids的内存结构以及redis为高速所付出的代价。这是该系列的第一篇文章，讲解redis内部构造。redis数据结构会占用多少内存的问题实际和编译器，CPU以及redis使用的内存分配器相关（redis默认使用jemalloc）。以下的计算依赖64位 centos 上的redis 3.0.5 版本。 对于不编写或者不熟悉C/C++的开发者而言，理解可能上不太容易。在此我会简化概念以让你能理解计算过程。在C/C++语言里，当你声明unsigned int (4 bytes) 变量，编译器会分配8 bytes内存（64位架构）。jemalloc 内存分配器会优化查找新的内存块的速度，并对齐分配的内存。jemalloc 的内存分配策略运行良好，然而接下来我认为我应该使用简化的概念来描述。你请求24 bytes，分配32。你请求61，分配64。我做了深度的简化，希望你理解的更清楚。 Salvatore Sanfilippo’s (aka antirez）通过一种SDS的结构来解释strings： 12345+--------+-------------------------------+-----------+| Header | Binary safe C alike string... | Null term |+--------+-------------------------------+-----------+ | `-&gt; Pointer returned to the user. 这是一种简单的C结构，header 部分包含string数据部分和末尾0的实际大小和内存占用空间的信息。我们感兴趣的事sds strings header结构的成本，resize策略和内存分配的代价。 2015年7月4号，pull request a long history with the optimization of sds strings, 被引入Redis 3.2，使sds headers部分内存占用大幅度降低（从16%到200%不等）。移除了redis里关于redis string 最大512MB的限制。所有的这些可能性都归功于string长度变化时，header的动态变更。strings长度在256 bytes以下时，header仅占用3 bytes，65kb以下时占用5 bytes，512MB以下时占用9 bytes，uint64_t(64 bit unsigned integer)以下时占用17 bytes。而这种变化可以减少redis server farm 19.3%的内存（～42 GB）。然而，在Redis 3.0.x 中简化为 8 bytes 加 末端零占用的1 byte。让我们评估一下string strings的内存占用： 116 (header) + 7 (string length) + 1(trailing zero) = 24 bytes (16 bytes in the header, because the compiler will align 2 unsigned int for you). jemalloc 会分配32 bytes。Let’s take as long as it will not be taken into account （我希望你售后能理解为什么）。 当一个字符串大小变化时会引起什么变化？当你增加字符串长度，同时发现已分配的内存不足，redis 会将新长度和常量SDS_MAX_PREALLOC（sds.h中定义，值为1,048,576 bytes）比较。如果新长度比该值小，则会分配两倍的请求大小。如过请求长度大于 SDS_MAX_PREALLOC ，新增加的长度会增加到这个常量上。 这个特性对于主题-bitmaps使用中内存减少 这个问题非常重要。分配的内存通常是需要的两倍，是因为setbit实现的需要（参见 setbit 命令，bitops.c）。 现在你可以说 strings 会占用32 bytes（包括已分配的）。浏览过 hashedin.com (redis memory optimization guide) 的读者可能会想起他们被强烈建议不要使用少于100 bytes 的字符串，比如 set foo bar 会占用 ～96 bytes，其中 90 bytes 的开销（64位机器）。讲道理，让我们看一下为什么。 reids里所有的值都被命名为 redisObject， 内部结构如下： 123+------+----------+-----+----------+-------------------------+| Type | Encoding | LRU | RefCount | Pointer to data (ptr*) |+------+----------+-----+----------+-------------------------+ 稍后我们会计算字符串的大小，了解账户编译器和jemalloc特性。了解存储字符串的编码是非常重要的，redis会使用三种不同的存储策略： REDIS_ENCODING_INT. Strings can be stored in this form, if the value is cast to long value in the range LONG_MIN, LONG_MAX. For example, the string «dict» it will be stored in the form of this encoding, and will be the number 1952672100 (0x74636964). This encoding is also used for pre-selected range of special values in the range REDIS_SHARED_INTEGERS (defined in redis.h and the default is 10000). The values of this range are allocated immediately at the start of Redis. REDIS_ENCODING_EMBSTR used for strings with a length up to 39 bytes (the value from constant REDIS_ENCODING_EMBSTR_SIZE_LIMIT object.c). This means that redisObject structure and sds string structure are placed in a single area of memory allocated by allocator. With this in mind, we will be able to calculate the correct alignment. However, it is equally important to understand the problem of memory fragmentation in the Redis and how to live with it. REDIS_ENCODING_RAW used for all strings whose length exceeds REDIS_ENCODING_EMBSTR_SIZE_LIMIT. In this case our ptr * stores a pointer to the memory area with sds string. EMBSTR 在2012年出现，在短字符串方面，带来了大约 60%-70%的性能提升，但目前对内存及其碎片化影响的研究还不多。 7 bytes 的 strings 字符串，使用 EMBSTR 存储结构。构建的存储结构类似这样： 12345+--------------+--------------+------------+--------+----+| robj data... | robj-&gt;ptr | sds header | string | \0 |+--------------+-----+--------+------------+--------+----+ | ^ +-----------------------+ 现在我们可以再次计算 strings 的内存占用情况 1(4 + 4)* + 8(encoding) + 8 (lru) + 8 (refcount) + 8 (ptr) + 16 (sds header) + 7(strig itself) + 1 (terminating zero) = 56 bytes. The type and value in redisObject uses only the 4 lower and higher bits in the same number, so these two aligned fields will take 8 bytes. 让我们检查一下，使用 DEBUG SDSLEN 来debug SDS (http://redis.io/commands/debug-object) 字符串。这个命令在redis2.6 被加入。 123456set key strings+OKdebug object key+Value at:0x7fa037c35dc0 refcount:1 encoding:embstr serializedlength:8 lru:3802212 lru_seconds_idle:14debug sdslen key+key_sds_len:3, key_sds_avail:0, val_sds_len:7, val_sds_avail:0 使用EMBSTR编码，字符串长度 7 bytes（有效SDS长度），那么 hashdin.com 的开发者讨论的 96 bytes 又是关于什么呢？在我的理解中，他们犯了一点小错误，set foo bar 需要分配112 bytes内存（value 56 bytes，key 56 bytes），内存开销 106 bytes。 我承诺会说明使用BITMAP时，节省内存的情况。Redis 2.2 开始出现的 Bit 和 byte 操作 就想一个实时计数的魔法棒，可以节省内存。官方口号是“上亿用户数据，仅占用12M内存“。 理解了redis内存字符串原理，也可以了解bitmap。“是否应该被用于少量数据？”。假设你需要记录一千万人的上网数据： 1234setbit online 10000000 1:0debug sdslen online+key_sds_len:6, key_sds_avail:0, val_sds_len:1250001, val_sds_avail:1048576 你会消费 2,288,577 bytes 内存，对你来说“有用”的部分为 1,250,001 bytes。存储你的一个用户花费 ～2.3 MB，使用 SET 你需要 ～64 bytes（pyaload 为 4 bytes）。使用这种数据结构可以有效减少内存使用量。如果你有10,000～100,000用户，bitmap结构就可以复用内存。 最后，了解一下 字符串 resize，即就是重新分配内存块。内存碎片化是redis的另一个特性，很少有开发者能考虑到这一点： 1234567891011info memory$222# Memoryused_memory:506920used_memory_human:495.04Kused_memory_rss:7565312used_memory_peak:2810024used_memory_peak_human:2.68Mused_memory_lua:36864mem_fragmentation_ratio:14.92mem_allocator:jemalloc-3.6.0 mem_fragmentation_ratio 指标显示了系统分配的内存used_memory_rss 和 redis使用内存used_memory的比率。use_memory 和 use_memory_ree包含了数据和redis存储的内部数据结构所占用内存。 Redis RSS (Resident Set Size) - RAM allocated by the operating system, which in addition to the user data (and the costs of their internal representation) accounted for the cost of fragmentation during the physical allocation of the operating system. 如何理解 mem_fragmentation_ratio ？2.1 意思是需要210%的更多内存。小于1则意味着内存被终止，操作系统正在交换内存。 实际中，如果该数字超过 1-1.5边界意味着有地方出错了，尝试以下解决方法： 重启redis。redis越长时间不重启，这个值就会越大。 检查一下你计划存储的数据量。比方说，如果你使用32位redis存储多达4GB的数据，那么你应该使用64位redis以扩增rdb。 如果你了解内存分配器的不同点，可以考虑更换内存分配器。 其他资料： http://redis.io/topics/memory-optimization http://redis.io/topics/internals-sds http://redislabs.com/blog/redis-ram-ramifications http://github.com/sripathikrishnan/redis-rdb-tools/wiki/Redis-Memory-Optimization]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql优化的一般策略]]></title>
    <url>%2F2019%2F08%2F21%2Fsql%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%80%E8%88%AC%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[sql 优化的一般策略：索引优化，sql改写，参数优化，优化器 索引优化以select * from vvshop_order.vv_order where receive_phone=&#39;151011324532&#39;; 为例分析 1explain select * from vv_order where order_no=23; 结果： 分析：可以看到该sql扫描全表 30 多万记录，可以通过添加索引优化 1alter table vv_order add index orderno_idx(order_no); 注意点： 当传入的数据类型和库表数据类型不一致时，索引会失效 不要为每个查询字段建立单独的索引，应该根据实际需要建立单列索引或者组合索引 通过explain+extended 检查sql的执行计划，是否使用索引，是否发生隐式转换 避免在查询条件中使用函数 sql 改写分页优化原sql select * from buyer where sellerid=100 limit 100000，5000, limit M, N 写法中，M越大，性能越差，可以改写为 123select t1.* from buyer t1,(select id from buyer sellerid=100 limit 100000，5000) t2where t1.id=t2.id; 子查询优化 查询数量较多时，in改为exist，或者优化为如下的形式 1234SELECT first_nameFROM employees emp,(SELECT emp_no FROM salaries_2000 WHERE salary = 5000) salWHERE emp.emp_no = sal.emp_no; 避免查询返回所有字段，只返回需要的字段数据 不使用 select *or 改写为 in or的效率事n，in的效率是log(n)，控制in数量在200以内 不使用函数和触发器，通过应用程序实现少用join，保证字段类型一直再join或比较连续数值 使用 between参数优化优化器其他影响in是否生效的因素 eq_range_index_dive_limit 参数 默认为10 eq_range_index_dive_limit = 0 只能使用index dive 0 &lt; eq_range_index_dive_limit &lt;= N 使用index statistics eq_range_index_dive_limit &gt; N 只能使用index dive 字段 根据实际使用情况设置字段类型 单表不要有太多字段，建议20以内 避免使用null字段，优化较难且额外占用索引空间 用整型来存IP 系统参数调优基准测试工具 sysbench：模块化，跨平台以及多线程的性能测试工具 iibench-mysql：基于java的插入性能测试工具 tpcc-mysql：Percona 开发的TPC-C 测试工具 这里介绍一些比较重要的参数： back_log backlog值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到maxconnections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即backlog，如果等待连接的数量超过backlog，将不被授予连接资源。可以从默认的50升至500 wait_timeout 数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时 maxuserconnection 最大连接数，默认为0无上限，最好设一个合理上限thread_concurrency：并发线程数，设为CPU核数的两倍 skipnameresolve 禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问 keybuffersize 索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show status like’keyread%’，保证keyreads / keyreadrequests在0.1%以下最好 innodbbufferpool_size 缓存数据块和索引块，对InnoDB表性能影响最大。通过查询show status like ‘Innodbbufferpoolread%’，保证 (Innodbbufferpoolreadrequests – Innodbbufferpoolreads)/ Innodbbufferpoolreadrequests 越高越好 innodbadditionalmempoolsize InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小 innodblogbuffer_size InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB querycachesize 缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失。根据命中率(Qcachehits/(Qcachehits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令show status like ‘Qcache_%’查看目前系统Query catch使用大小 readbuffersize MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能 sortbuffersize MySql执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sortbuffersize变量的大小 readrndbuffer_size MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 record_buffer 每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值 threadcachesize 保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的 table_cache 类似于threadcachesize，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM reference: 阿里云慢SQL优化挑战大赛分析 SQL优化器原理 - 查询优化器综述 MYSQL查询SQL语句性能优化方法 MySQL–eq_range_index_dive_limit参数学习 MySQL SQL优化之in与range查询【转】 MySQL5.7利用虚拟列优化]]></content>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB 查询语法]]></title>
    <url>%2F2019%2F08%2F21%2FSpring%20Data%20MongoDB%20%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Query 和 Criteria 查询 123Query query = new Query();query.addCriteria(Criteria.where("name").is("Eric"));List&lt;User&gt; users = mongoTemplate.find(query, User.class); 支持的查询方法：is, regex, lt, gt, pageable, sort 生成query方法 findByX 1List&lt;User&gt; findByName(String name); startinggWith and endingWith 12List&lt;User&gt; findByNameStartingWith(String regexp);List&lt;User&gt; findByNameEndingWith(String regexp); between 1List&lt;User&gt; findByAgeBetween(int ageGT, int ageLT); like and orderBy 1List&lt;User&gt; users = userRepository.findByNameLikeOrderByAgeAsc("A"); JSON Query methods : @Query 12@Query("&#123; 'name' : ?0 &#125;")List&lt;User&gt; findUsersByName(String name); 支持的查询方法： $regex, $gt, $lt QueryDSL Queries 4.1 maven 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;com.mysema.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-mongodb&lt;/artifactId&gt; &lt;version&gt;3.6.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.mysema.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt; &lt;version&gt;3.6.6&lt;/version&gt;&lt;/dependency&gt;&lt;plugin&gt; &lt;groupId&gt;com.mysema.maven&lt;/groupId&gt; &lt;artifactId&gt;apt-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;process&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/generated-sources/java&lt;/outputDirectory&gt; &lt;processor&gt; org.springframework.data.mongodb.repository.support.MongoAnnotationProcessor &lt;/processor&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 4.2 class 1234567891011@QueryEntity@Documentpublic class User &#123; @Id private String id; private String name; private Integer age; // standard getters and setters&#125; Implement QueryDslPredicateExecutor 123QUser qUser = new QUser("user");Predicate predicate = qUser.name.eq("Eric");List&lt;User&gt; users = (List&lt;User&gt;) userRepository.findAll(predicate); 支持的查询方法：is,startinggWith and endingWith, between ​]]></content>
      <tags>
        <tag>spring data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redission 序列化问题追踪]]></title>
    <url>%2F2019%2F05%2F28%2Fredission%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[背景项目原本是用jedis连接redis，但考虑到需要用redis锁，因此替换为方便快捷的redisson，但是使用redisson之后会报decode error，具体信息如下： 12345678910112019-05-15 13:39:59.973 [redisson-netty-2-3] ERROR o.r.c.h.CommandDecoder [decodeCommand:203] - Unable to decode data. channel: [id: 0x477c5ced, L:/192.168.4.94:57423 - R:10.10.10.43/10.10.10.43:6379], reply: ReplayingDecoderByteBuf(ridx=102, widx=102), command: (GET), params: [Geek:xxxxx:xxxx]java.io.IOException: java.lang.NullPointerException at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:247) at org.redisson.codec.FstCodec$1.decode(FstCodec.java:228) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:368) at org.redisson.client.handler.CommandDecoder.decodeCommand(CommandDecoder.java:200) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:140) at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:115) at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:366) at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) 测试代码12RBucket ste = redissonClient.getBucket("Geek:add:ddd");Object re = ste.get(); 考虑可能是由于序列化产生的问题，查到NullPointer 3.10.6，设置codec为StringCodec，即 1redissonClient.getConfig().setCodec(new StringCodec()); 但是并未解决问题，redisson仍然使用默认的FstCodec，通过idea强大的提示功能可以看到 getBucket接受一个codec参数 修改代码为 12RBucket ste = redissonClient.getBucket("Geek:add:ddd", new StringCodec());String re = ste.get(); 完美解决 问题为什么直接设置redisson config 不生效呢，一步步查源码 RedissonObject#RedissonObject 123public RedissonObject(CommandAsyncExecutor commandExecutor, String name) &#123; this(commandExecutor.getConnectionManager().getCodec(), commandExecutor, name);&#125; 可以看出redisson 默认从ConnectionManager里获取codec方式，继续看，以 SingleConnectionManager 为例，SingleConnectionManager是MasterSlaveConnectionManager的子类，具体的类图关系 config.java 12345678public Config(Config oldConf) &#123; setExecutor(oldConf.getExecutor()); if (oldConf.getCodec() == null) &#123; // use it by default oldConf.setCodec(new FstCodec()); &#125;...... &#125; 即检测到原有codec为空时，则设置为FstCodec 看一下 Redisson.java 配置关键部分代码 12345678910111213141516protected Redisson(Config config) &#123; this.config = config; Config configCopy = new Config(config); connectionManager = ConfigSupport.createConnectionManager(configCopy); evictionScheduler = new EvictionScheduler(connectionManager.getCommandExecutor()); writeBehindService = new WriteBehindService(connectionManager.getCommandExecutor());&#125; public static RedissonClient create(Config config) &#123; Redisson redisson = new Redisson(config); if (config.isReferenceEnabled()) &#123; redisson.enableRedissonReferenceSupport(); &#125; return redisson;&#125; 可以看出， config是在redisson初始化的时候传入的 因为我用的是redisson-spring-boot-starter，看一下这个starter里面，是如何初始化的，redisson starter 默认使用 spring-data-redis 配置。 123456789101112131415161718192021222324@Bean(destroyMethod = "shutdown") @ConditionalOnMissingBean(RedissonClient.class) public RedissonClient redisson() throws IOException &#123; Config config = null; .... if (redissonProperties.getConfig() != null) &#123; .... &#125; else &#123; config = new Config(); String prefix = "redis://"; Method method = ReflectionUtils.findMethod(RedisProperties.class, "isSsl"); if (method != null &amp;&amp; (Boolean)ReflectionUtils.invokeMethod(method, redisProperties)) &#123; prefix = "rediss://"; &#125; config.useSingleServer() .setAddress(prefix + redisProperties.getHost() + ":" + redisProperties.getPort()) .setConnectTimeout(timeout) .setDatabase(redisProperties.getDatabase()) .setPassword(redisProperties.getPassword()); &#125; return Redisson.create(config); 回到一开始的问题，直接设置redisson codec为什么不生效？仔细以上分析可以知道，redisson统一设置codec主要是通过初始化的时候传入ConnectionManager使 codec生效，而通过 redissonClient.getConfig().setCodec(...)的方式并不能改变ConnectionManager中的编码方式。 结论： 如果想自定义codec，需要自己初始化redissonClient[调用Redisson.create(config)]， 或者重写redisson-starter 在定制化程度不高时，可直接使用默认codec，或者把特定的codec传入方法体内 Reference]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构BFF和网关]]></title>
    <url>%2F2018%2F09%2F05%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84BFF%E5%92%8C%E7%BD%91%E5%85%B3%2F</url>
    <content type="text"><![CDATA[BFF 架构图 BFF(Backend for Frontend)也称聚合层或者适配层，上述架构从外到内依次为 端用户体验层-&gt;网关层-&gt;BFF层-&gt;微服务层，主要是讲内部复杂的微服务，适配成对各种不同的用户体验。网关专注解决跨横切面逻辑，包括路由、安全、监控和限流熔断等。 为提高系统的灵活性，在网关层和微服务层之间构建BFF层，这里主要使用GraphQL 构建BFF层。 GraphQL 特点： 定义数据模型：按需获取 数据分层：通过数据分层可以减少客户端请求次数 123456789101112&#123; user(id:1001) &#123; // 第一层 name, friends &#123; // 第二层 name, addr &#123; // 第三层 country, city &#125; &#125; &#125;&#125; 强类型 1234567const Meeting = new GraphQLObjectType(&#123; name: 'Meeting', fields: () =&gt; (&#123; meetingId: &#123;type: new GraphQLNonNull(GraphQLString)&#125;, meetingStatus: &#123;type: new GraphQLNonNull(GraphQLString), defaultValue: ''&#125; &#125;)&#125;) GraphQL 的类型系统定义了包括 Int, Float, String, Boolean, ID, Object, List, Non-Null 等数据类型。所以在开发过程中，利用强大的强类型检查，能够大大节省开发的时间，同时也很方便前后端进行调试。 协议而非存储：GraphQL 本身并不直接提供后端存储的能力，不绑定任何的数据库或者存储引擎。 无需版本化 1234567891011121314const PhotoType = new GraphQLObjectType(&#123; name: 'Photo', fields: () =&gt; (&#123; photoId: &#123;type: new GraphQLNonNull(GraphQLID)&#125;, file: &#123; type: new GraphQLNonNull(FileType), deprecationReason: 'FileModel should be removed after offline app code merged.', resolve: (parent) =&gt; &#123; return parent.file &#125; &#125;, fileId: &#123;type: new GraphQLNonNull(GraphQLID)&#125; &#125;)&#125;) GraphQL 服务端能够通过添加 deprecationReason，自动将某个字段标注为弃用状态。并且基于 GraphQL 高度的可扩展性，如果不需要某个数据，那么只需要使用新的字段或者结构即可，老的弃用字段给老的客户端提供服务，所有新的客户端使用新的字段获取相关信息。并且考虑到所有的 graphql 请求，都是按照 POST /graphql 发送请求，所以在 GraphQL 中是无须进行版本化的。 GraphQL 与 Rest 数据获取：Rest 缺乏可扩展性，GraphQL能够按需获取； API调用：REST针对每种资源的操作都是一个endpoint，GraphQL只需要一个endpoint； 复杂数据请求：REST对于嵌套的复杂数据需要多次调用，GraphQL 一次调用，减少网络开销； 错误码处理：REST能够精确返回HTTP错误码，GraphQL统一返回200，对错误信息进行包装； 版本号：REST通过v1/v2实现，Graph通过Schema扩展实现。 BFF 端技术栈 微服务下使用GraphQL构建BFF中使用node作为BFF主要框架，因自己对node不太熟悉，会尽量采用java来实现相应的功能。GraphQL 使用 query 和 mutation 实现CQRS。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【译】Spring Boot @PropertySource 读取 YAML 文件]]></title>
    <url>%2F2018%2F08%2F31%2FSpring%20Boot%20%40PropertySource%20%E8%AF%BB%E5%8F%96%20YAML%20%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring Boot 默认不支持@PropertySource读取yaml 文件，这也是Stackoverflow 上经常给予的标准答案。 Spring 4.3 通过引入 PropertySourceFactory 接口使之成为可能。PropertySourceFactory 是PropertySource 的工厂类。默认实现是 DefaultPropertySourceFactory，可以构造ResourcePropertySource 实例。 可以通过普通的是实现构造 createPropertySource， 需要做两点: 导入resource 到Properties 对象。 构造 PropertySource 使用Properties。 具体例子： 1234567891011121314151617181920212223public class YamlPropertySourceFactory implements PropertySourceFactory &#123; @Override public PropertySource&lt;?&gt; createPropertySource(@Nullable String name, EncodedResource resource) throws IOException &#123; Properties propertiesFromYaml = loadYamlIntoProperties(resource); String sourceName = name != null ? name : resource.getResource().getFilename(); return new PropertiesPropertySource(sourceName, propertiesFromYaml); &#125; private Properties loadYamlIntoProperties(EncodedResource resource) throws FileNotFoundException &#123; try &#123; YamlPropertiesFactoryBean factory = new YamlPropertiesFactoryBean(); factory.setResources(resource.getResource()); factory.afterPropertiesSet(); return factory.getObject(); &#125; catch (IllegalStateException e) &#123; // for ignoreResourceNotFound Throwable cause = e.getCause(); if (cause instanceof FileNotFoundException) throw (FileNotFoundException) e.getCause(); throw e; &#125; &#125;&#125; 注意：YAML 需要 SnakeYAML 1.18 或者更高版本。 @PropertySource 注解有一个 factory 属性，通过这个属性来注入 PropertySourceFactory，这里给出 YamlPropertySourceFactory的例子。 123456789101112131415@SpringBootApplication@PropertySource(factory = YamlPropertySourceFactory.class, value = "classpath:blog.yaml")public class YamlPropertysourceApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext ctx = SpringApplication.run(YamlPropertysourceApplication.class, args); ConfigurableEnvironment env = ctx.getEnvironment(); env.getPropertySources() .forEach(ps -&gt; System.out.println(ps.getName() + " : " + ps.getClass())); System.out.println("Value of `foo.bar` = " + env.getProperty("foo.bar")); &#125;&#125; 注意：这里使用的是Spring Boot，但是对于Spring 4.3 及其以上版本同样适用。 翻译拙劣，欢迎指正。 reference: Use @PropertySource with YAML files]]></content>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Explain详解]]></title>
    <url>%2F2018%2F08%2F08%2FMySQL%20Explain%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[日常工作中主要用explain来查看sql语句的执行计划，深入了解是否需要优化，已经索引等信息。 执行explain命令会生成一个QEP(query execution plan) 1234567mysql&gt; explain select * from servers;+----+-------------+---------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | servers | ALL | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+---------+------+---------------+------+---------+------+------+-------+row in set (0.03 sec) 一、 id ​ 我的理解是SQL执行的顺序的标识,SQL从大到小的执行 id相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 3.id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 二、select_type ​ 示查询中每个select子句的类型 (1) SIMPLE(简单SELECT,不使用UNION或子查询等) (2) PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY) (3) UNION(UNION中的第二个或后面的SELECT语句) (4) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) (5) UNION RESULT(UNION的结果) (6) SUBQUERY(子查询中的第一个SELECT) (7) DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询) (8) DERIVED(派生表的SELECT, FROM子句的子查询) (9) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) 三、table 显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果) 12345678mysql&gt; explain select * from (select * from ( select * from t1 where id=2602) a) b;+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| 1 | PRIMARY | &lt;derived2&gt; | system | NULL | NULL | NULL | NULL | 1 | || 2 | DERIVED | &lt;derived3&gt; | system | NULL | NULL | NULL | NULL | 1 | || 3 | DERIVED | t1 | const | PRIMARY,idx_t1_id | PRIMARY | 4 | | 1 | |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ 四、type 表示MySQL在表中找到所需行的方式，又称“访问类型”。 常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好） ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index: Full Index Scan，index与ALL区别为index类型只遍历索引树 range:只检索给定范围的行，使用一个索引来选择行 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 五、possible_keys 指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 六、Key key列显示MySQL实际决定使用的键（索引） 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 七、key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 八、ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 九、rows 表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 十、Extra 该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 Using filesort：MySQL中无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 总结：**• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况• EXPLAIN不考虑各种Cache• EXPLAIN不能显示MySQL在执行查询时所作的优化工作• 部分统计信息是估算的，并非精确值• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。** 转载自 http://www.cnblogs.com/xuanzhi201111/p/4175635.html]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>reprinted</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发布构件到Maven中央仓库]]></title>
    <url>%2F2018%2F06%2F19%2F%E5%8F%91%E5%B8%83%E6%9E%84%E4%BB%B6%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[总结的工具包为使用方便决定发布到maven中央仓库，因为第一次发布，遇到很多问题，简要记录一下。 1Sonatype官网：http://www.sonatype.org/ Step 1: 注册Sonatype用户 注册地址：https://issues.sonatype.org/secure/Signup!default.jspa oss地址：https://oss.sonatype.org ，用于查询构件 Step2: 创建issue create issue: 12345Project：Community Support - Open Source Project Repository Hosting (OSSRH)Issue Type: new Project--- next ---Summary: &lt;简要项目介绍&gt;Group id: me.silloy &lt;需要时自己的域名，同时在工程中使用&gt; 点击create，创建issue 下图查看已创建的issue Step 3: 等待issue审批通过 一般需要1-2天，审批通过后会收到邮件通知，在自己提交的issue下面可以看到Sonatype工作人员的回复。同时issue状态修改为resolved。 Step 4: 发布准备 生成gpg密钥，并发布到PGP密钥服务器，见引用 简要介绍win10环境下gpg密钥生成方法 下载Gpg4win， 安装 依次执行一下命令 版本检查: gpg --version, 我用的是2.2.8 生成key： gpg --gen-key 检查本地key： gpg –list-keys 发布公钥：gpg –keyserver hkp://pool.sks-keyservers.net –send-keys A3434534534 校验是否发布成功：gpg –keyserver hkp://pool.sks-keyservers.net –recv-keys 732796B4 12# List all available gpg servers:$ gpg-connect-agent --dirmngr 'keyserver --hosttable' 修改maven设置 修改maven全局配置文件setting.xml， 增加一下内容 1234567&lt;servers&gt; &lt;server&gt; &lt;id&gt;oss&lt;/id&gt; &lt;username&gt;Harvey.Su&lt;/username&gt; &lt;password&gt;&lt;![CDATA[password]]&gt;&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 修改pom文件，加入需要的信息 1234567891011121314151617181920212223242526&lt;groupId&gt;me.silloy&lt;/groupId&gt; &lt;artifactId&gt;zjtools&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;description&gt;a java tool package&lt;/description&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;developers&gt; &lt;developer&gt; &lt;name&gt;sushaohua&lt;/name&gt; &lt;email&gt;sshzh90@gmail.com&lt;/email&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;scm&gt; &lt;connection&gt;scm:git:git@github.com:silloy/zjtools.git&lt;/connection&gt; &lt;developerConnection&gt;scm:git:git@github.com:silloy/zjtools.git&lt;/developerConnection&gt; &lt;url&gt;git@github.com:silloy/zjtools.git&lt;/url&gt; &lt;/scm&gt; 增加一个名为oss的profile (⭐⭐⭐⭐⭐) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;compilerVersion&gt;$&#123;maven.compiler.source&#125;&lt;/compilerVersion&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.source&#125;&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- Source --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-source-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- Javadoc --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-javadoc-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;additionalOptions&gt; &lt;additionalOption&gt;-Xdoclint:none&lt;/additionalOption&gt; &lt;/additionalOptions&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt;&lt;!-- profiles --&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt; &lt;activation&gt; &lt;!--&lt;activeByDefault&gt;true&lt;/activeByDefault&gt;--&gt; &lt;jdk&gt;[1.8,)&lt;/jdk&gt; &lt;/activation&gt; &lt;!--&lt;properties&gt;--&gt; &lt;!--&lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;--&gt; &lt;!--&lt;/properties&gt;--&gt; &lt;build&gt; &lt;!--&lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;-SNAPSHOT&lt;/finalName&gt;--&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;&lt;/finalName&gt; &lt;plugins&gt; &lt;!-- GPG --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-gpg-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt;&lt;/profiles&gt;&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; javadoc 不规范的情况下，可以把maven-javadoc-plugin注释掉。 特别注意：snapshotRepository 与 repository 中的 id 一定要与 setting.xml 中 server 的 id 保持一致。这里我们都设置为oss。 Step 5: 上传构件到OSS 执行命令： 1mvn clean deploy -P sonar -Darguments="gpg.passphrase=密钥密码" 看到如下提示信息，说明deploy成功 1234567891011Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jar (34 kB at 966 B/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.pomUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.pom (9.7 kB at 985 B/s)Downloading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xmlUploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xmlUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/maven-metadata.xml (296 B at 5 B/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jar (20 kB at 1.9 kB/s)Uploading: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jarUploaded: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1-sources.jar (20 kB at 10 kB/s) Step 6: 在OSS中发布构件 登录https://oss.sonatype.org，可以在`Staging Repositories`中查看到已上传的构件，可进行模糊查询，快速定位到自己的构件，状态为open，勾选，然后点击close按钮，接下来系统会自动验证该构件是否满足指定要求，当验证完毕后，状态会变为 Closed，最后，点击 Release 按钮来发布该构件 。 备注：切记是要发布release版本的构件，不能是snapshot，不然不会出现在Staging Repositories里面。 Step 7: 通知 Sonatype“构件已成功发布” 需要在曾经创建的 Issue 下面回复一条“构件已成功发布”的评论，这是为了通知 Sonatype 的工作人员为需要发布的构件做审批( I released the component has been successfully, please approval, thank you!)，发布后会关闭该 Issue 。 Step 8: 等待审批，1~2天，审批通过后会收到邮件通知。 Step 9: 在https://oss.sonatype.org/#stagingRepositories找到自己的构建，点击release Step 9: 从中央仓库搜索自己发布的构件 地址：http://search.maven.org/ Step 10: http://search.maven.org/ 上搜索自己的构件 ，大功告成，可以在项目中引用啦。以后发布就简单了，不需要每次都审核。 问题： gpg错误处理Enter passphrase: gpg: gpg-agent is not available in this session 可能是版本问题，或者没有安装gpg-agent，详见https://askubuntu.com/questions/860370/gpg-agent-cant-be-reached ，具体步骤 linux 系统解决方案 安装gpg2 sudo apt install gpgv2 然后在maven的settings.xml中加入两个属性，主要要在激活的profile里面 123456&lt;profile&gt; &lt;properties&gt; &lt;gpg.executable&gt;gpg2&lt;/gpg.executable&gt; &lt;gpg.useagent&gt;true&lt;/gpg.useagent&gt; &lt;/properties&gt;&lt;/profile&gt; mvn clean deploy -P oss， 参看 Avoid gpg signing prompt when using Maven release plugin windows 解决方案 (Windows下Gpg4win、Git、ssh-pageant配置)[https://www.mjollnir.cc/archives/216.html] .git/config修改 12[gpg] program = gpg Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project zjtools: Failed to deploy artifacts: Could not transfer artifact me.silloy:zjtools:jar:0.0.1 from/to oss (https://oss.sonatype.org/service/local/staging/deploy/maven2/): Access denied to: https://oss.sonatype.org/service/local/staging/deploy/maven2/me/silloy/zjtools/0.0.1/zjtools-0.0.1.jar, ReasonPhrase: Forbidden. -&gt; [Help 1] 答案：引用，提交给sonatype就可以，工作人员会开权限 可以在 http://search.maven.org/ 搜索到，但是不能在 http://mvnrepository.com/ 搜索到，是因为更新频率不一样，等一天左右就好了，参见工作人员回复 gpg --list-keys 出现 unknown 解决 12345678910111213141516gpg --edit-key user@useremail.comgpg&gt; trustPlease decide how far you trust this user to correctly verify other users' keys(by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menuYour decision? 5gpg&gt; save reference： 发布到中央仓库 上传自己的构件(Jar)到Maven中央仓库 将 Smart 构件发布到 Maven 中央仓库 Maven-008-Nexus 私服部署发布报错 Failed to deploy artifacts: Failed to transfer file: … Return code is: 4XX, ReasonPhrase: … 解决方案 Why am I getting a “401 Unauthorized” error in Maven? 将项目发布到 Maven 中央仓库踩过的坑 [gpg —list-keys command outputs uid [ unknown ] after importing private key onto a clean install]]></content>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-Mybatis通用mapper使用]]></title>
    <url>%2F2018%2F06%2F15%2FSpringBoot-Mybatis%E9%80%9A%E7%94%A8mapper%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[mybatis是一个很好用的工具，但是编写mapper是一件很麻烦的事，自mybatis 3.0开始可以使用注解的方式，极大的简化了xml的编写量，本地想看看mybatis源码，自己扩展写一个工具，在阅读源码过程中发现一个通用mapper的工具包，感觉不用重复造轮子了，简要记录一下spring boot整合通用mapper的使用。 确保可以正常使用mybatis pom引入依赖包，starter需要配合@Mapper注解使用，这里采用这种方式，或者使用@MapperScan注解，@tk.mybatis.spring.annotation.MapperScan(basePackages = &quot;扫描包&quot;)配合原生mapper使用。 12345&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;&#123;version&#125;&lt;/version&gt;&lt;/dependency&gt; 我使用的版本是2.0.2 Mybatis 扫描配置（Deprecated, spring 自动配置） 123456789101112131415161718@Configuration//TODO 注意，由于MapperScannerConfigurer执行的比较早，所以必须有下面的注解@AutoConfigureAfter(MybatisAutoConfiguration.class)public class MyBatisMapperScannerConfig &#123; @Bean public MapperScannerConfigurer mapperScannerConfigurer() &#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setSqlSessionFactoryBeanName("sqlSessionFactory"); mapperScannerConfigurer.setBasePackage("org.springboot.sample.mapper"); Properties properties = new Properties(); // 这里要特别注意，不要把MyMapper放到 basePackage 中，也就是不能同其他Mapper一样被扫描到。 properties.setProperty("mappers", MyMapper.class.getName()); properties.setProperty("notEmpty", "false"); properties.setProperty("IDENTITY", "MYSQL"); mapperScannerConfigurer.setProperties(properties); return mapperScannerConfigurer; &#125;&#125; 新建BaseMapper类，该类不能被当做普通Mapper一样被扫描 ，不加@Mapper注解，或者放在不同文件夹 1234567package com.zj.mapper;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;public interface BaseMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; &#123;&#125; 业务处理dao层，扩展BaseMapper 1234567package com.zj.mapper;import com.zj.model.OrderInfo;import org.apache.ibatis.annotations.Mapper;@Mapperpublic interface OrderInfoMapper extends BaseMapper&lt;OrderInfo&gt; &#123;&#125; 其他和使用普通mybatis一致，service层部分代码 12orderInfoMapper.insertSelective(info);OrderInfo info = orderInfoMapper.selectByPrimaryKey(id); 通用mapper提供常用的一些操作方法: deleteByPrimaryKey, insert, insertSelective, selectByPrimaryKey, updateByPrimaryKeySelective, updateByPrimaryKey, insertList等很多方法，需要你进一步探索😀😀 主键id问题 当使用insert，insertSelective等方法时，希望返回由数据库产生的逐渐，需要在实体类上增加注解 123@Id@GeneratedValue(generator = "JDBC")private Long orderInfoId; generator=”JDBC”表示 MyBatis 使用 JDBC 的 getGeneratedKeys 方法来取出由数据库内部生成的主键 ，适用于MySQL，SQL Server等的自增主键。 或者： 123@Id@KeySql(useGeneratedKeys = true)private Long id; 如果实体字段和数据库字段不一致，可以使用@Column注解，其他注解 参见注解 12@Column(name="SCORE_SUM")private String sumScore; MBG生成参见https://github.com/abel533/Mapper/wiki/4.1.mappergenerator，demo见 git@github.com:silloy/mybatis-generator.git 更多细节参见wiki 通用Mapper极大的简化了xml文件的编写，但仍需要少许xml文件，有待进一步优化。同时因为这是一个个人项目，使用不太熟悉不建议使用。 reference: http://www.mybatis.tk/ https://github.com/abel533/Mapper/wiki https://github.com/abel533/Mapper https://blog.csdn.net/qq_19260029/article/details/78010369]]></content>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用RabbitMQ实现AMQP和MQTT的协议转换]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%BD%BF%E7%94%A8RabbitMQ%E5%AE%9E%E7%8E%B0AMQP%E5%92%8CMQTT%E7%9A%84%E5%8D%8F%E8%AE%AE%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[基于LBS的业务系统，前台使用MQTT协议推送到后台，后台通过RabbitMQ实现协议的转换，以实现负载消费的功能。主要的框架如图 准备工作：rabbitmq安装以及 rabbitmq_mqtt 插件启用 MQTT client向/drivers/1push消息，AMQP client 使用 topic 模式接受消息， exchange name为固定值 amq.topic，routingKey 为 MQTT 发送使用的Topic，注意/需要替换为.，queue name 可以任意指定，绑定相同queue的customer可以实现负载消费的功能。 Demo示例(以java为例)： RabbitConfig 12345678910111213141516171819202122@Componentpublic class TopicRabbitConfig &#123; public static final String message = "location.broker"; @Bean public Queue queueMessage() &#123; return new Queue(message); &#125; @Bean TopicExchange exchange() &#123; return new TopicExchange("amq.topic", true, false); &#125; //綁定队列 queueMessages() 到 topicExchange 交换机,路由键只接受完全匹配 topic.message 的队列接受者可以收到消息, # 为通配符模式 @Bean Binding bindingExchangeMessage(Queue queueMessage, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessage).to(exchange).with('.drivers.#'); &#125;&#125; RabbitListener: 123456789@Component@RabbitListener(queues = "location.broker")public class TopicReciver &#123; @RabbitHandler public void process(byte[] hello) &#123; System.out.println(new String(hello)); &#125;&#125; 使用java模拟的mqtt client 发送的消息为byte[]， 因此需要使用byte[]接收消息内容。 reference: Uniting AMQP and MQTT Message Brokering with RabbitMQ 使用rabbitmq做为mqtt服务器，整合spring做推送后台]]></content>
      <tags>
        <tag>amqp</tag>
        <tag>mqtt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo主题升级]]></title>
    <url>%2F2018%2F03%2F01%2Fhexo%E4%B8%BB%E9%A2%98%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[偶然看到最新的next主题，甚佳，决定对自己的博客主题进行升级，简要记录升级过程。 下载最新next 6.0.4, —&gt; 参见11，使用模块管理主题 备份_config.yml，同时对配置进行修改，相关图片位于next/source/images 1234cd themes/nextgit clone https://github.com/theme-next/theme-next-fancybox3 source/lib/fancyboxgit clone https://github.com/theme-next/theme-next-pace source/lib/pacegit clone https://github.com/theme-next/theme-next-algolia-instant-search source/lib/algolia-instant-search 其他插件 增加阅读进度 1git clone https://github.com/theme-next/theme-next-reading-progress source/lib/reading_progress 填补字符间空白 1git clone https://github.com/theme-next/theme-next-pangu.git source/lib/pangu 页面增加3D渲染，next默认提供两种渲染效果，theme-next-three和canvas_nest 123cd themes/nextgit clone https://github.com/theme-next/theme-next-three source/lib/threegit clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest 启用以下任意项： 1three_waves: true 1canvas_lines: true 1canvas_sphere: true 1canvas_nest: true 添加访问人数（6.0已原生支持busuanzi，启用即可） 打开\themes\next\layout\_partials\footer.swig文件,在copyright前加上画红线这句话 1&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 然后再合适的位置添加显示统计的代码，如图： 12345&lt;div class="powered-by"&gt;&lt;i class="fa fa-user-md"&gt;&lt;/i&gt;&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; pv的方式，单个用户连续点击n篇文章，记录n次访问量 uv的方式，单个用户连续点击n篇文章，只记录1次访客数 每篇文章末尾统一添加“本文结束”标记 实现方法 在路径 \themes\next\layout\_macro 中新建 passage-end-tag.swig 文件,并添加以下内容： 12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:15px;"&gt;--------------都看到这了，请我喝杯咖啡吧！&lt;i class="fa fa-coffee"&gt;&lt;/i&gt;--------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 接着打开\themes\next\layout\_macro\post.swig文件，在`` 之后， &lt;footer class=&quot;post-footer&quot;&gt; 之前添加 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 打开主题配置文件，文章末尾添加标记（不用设置） 12passage_end_tag: enabled: true 增加评论系统 gitment 主题配置 12345678910gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide 'Powered by ...' on footer, and more github_user: silloy # MUST HAVE, Your Github Username github_repo: repo # MUST HAVE, The name of the repo you use to store Gitment comments client_id: xxx # MUST HAVE, Github client id for the Gitment client_secret: xxx 问题： Error：validation failed 修改 next/layout/_third-party/comments/gitment.swig中id: window.location.pathname为 1id: &apos;&#123;&#123; page.date &#125;&#125;&apos; valine 文章底部增加版权信息 修改文章底部的那个带#号的标签 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 修改打赏字体不闪动，next 7.0 已支持配置 修改文件next/source/css/_common/components/post/post-reward.styl， 注释wechat:hover 和alipay:hover， 如下： 123456789101112/* 注释文字闪动函数 #wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125; #alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 模块化主题管理（以next主题为例） 备份next主题 mv next next-bak，提交代码 增加子模块 git submodule add git@github.com:silloy/hexo-theme-next.git themes/next 查看状态 git status 12345Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: .gitmodules new file: themes/next 提交 12345$ git commit -m "add next module"[hexo adbe36e] add next module 3 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 .gitmodules create mode 160000 themes/next 更新子模块 git submodule update --remote 拉取含子模块的项目，git clone 后执行以下操作 git submodule init 初始化本地配置文件 git submodule update 从该项目中抓取所有数据并检出父项目中列出的合适的提交 也可在 clone 使用 git clone --recursive 命令, git 就会自动初始化并更新仓库中的每一个子模块. 若子分支仓库中有未同步的更新, 可通过 git submodule update --remote --rebase 来同步最新的内容 同步源主题的修改 增加源 12cd theme/nextgit remote add source git@github.com:theme-next/hexo-theme-next.git 拉取更新 1git pull source master 等同于 123git fetch source mastergit checkout mastergit merge source/master 发布子模块的修改 使用 git push --recurse-submodules=check 命令 检查没有推送的子模块 使用 git push --recurse-submodules=on-demand git 会自动尝试推送变更的子项目 参考文章： NexT 使用文档 利用Gulp来压缩你的Hexo博客 hexo的next主题个性化教程:打造炫酷网站 老高博客 gitment Gitment评论功能接入踩坑教程 实现 Hexo next 主题博客本地站内搜索 我的个人博客之旅：从jekyll到hexo 在 hexo 中使用 git submodules 管理主题]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件预研-rabbitmq, rocketmq]]></title>
    <url>%2F2018%2F01%2F04%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E9%A2%84%E7%A0%94-kafka%2C%20rabbitmq%2C%20rocketmq%2F</url>
    <content type="text"><![CDATA[消息中间件在服务开发中起着重要的作用，应对业务需求，对rabbitmq，rocketmq进行预研，kafka暂时不做深入了解。 消息中间件应用场景 可以做延迟设计比如我们有一些数据，需要过五分钟后再被使用，这时候就需要使用延迟队列设计，比如在RabbitMQ中利用死信队列实现。具体实现在这里：http://www.cnblogs.com/haoxinyue/p/6613706.html 异步处理这个场景主要应用在多任务执行的场景。 应用解耦在大型微服务架构中，有一些无状态的服务经常考虑使用mq做消息通知和转换。 分布式事务最终一致性可以使用基于消息中间件的队列做分布式事务的消息补偿，实现最终一致性。 流量削峰一般在秒杀或团抢活动中使用广泛，可以通过队列实现秒杀的人数和商品控制，还可以缓解短时间压垮应用系统。 日志处理我们在做监控，或者日志采集的时候经常用队列来做消息的传输和暂存。 RocketMQ(Apache 4.2.0)概念Disk Flush (磁盘刷新/同步操作): 就是将内存的数据落地，存储在磁盘中. SYNC_FLUSH（同步刷盘）：生产者发送的每一条消息都在保存到磁盘成功后才返回告诉生产者成功。这种方式不会存在消息丢失的问题，但是有很大的磁盘IO开销，性能有一定影响。 ASYNC_FLUSH（异步刷盘）：生产者发送的每一条消息并不是立即保存到磁盘，而是暂时缓存起来，然后就返回生产者成功。随后再异步的将缓存数据保存到磁盘，有两种情况：1是定期将缓存中更新的数据进行刷盘，2是当缓存中更新的数据条数达到某一设定值后进行刷盘。这种方式会存在消息丢失（在还未来得及同步到磁盘的时候宕机），但是性能很好。默认是这种模式。 Broker Replication (Broker间数据同步/复制): 集群环境下需要部署多个Broker，Broker分为两种角色：一种是master，即可以写也可以读，其brokerId=0，只能有一个；另外一种是slave，只允许读，其brokerId为非0。一个master与多个slave通过指定相同的brokerName被归为一个broker set（broker集）。通常生产环境中，我们至少需要2个broker set。 Broker Replication只的就是slave获取或者是复制master的数据. Sync Broker：生产者发送的每一条消息都至少同步复制到一个slave后才返回告诉生产者成功，即“同步双写”。 Async Broker：生产者发送的每一条消息只要写入master就返回告诉生产者成功。然后再“异步复制”到slave。 start 十分钟入门RocketMQQuickStart: apache-quickstart 集群部署 使用不同配置文件启动nameserv(默认9876) 无状态节点，可集群部署，节点之间无任何信息同步（Broker与每个namesrv连接，可以保证信息同步性） nameserv的所有配置信息 12345678910111213141516rocketmqHome=/usr/local/rocketmqkvConfigPath=/Users/zhangyanghong/namesrv/kvConfig.jsonproductEnvName=centerclusterTest=falseorderMessageEnable=falselistenPort=9876serverWorkerThreads=8serverCallbackExecutorThreads=0serverSelectorThreads=3serverOnewaySemaphoreValue=256serverAsyncSemaphoreValue=64serverChannelMaxIdleTimeSeconds=120serverSocketSndBufSize=4096serverSocketRcvBufSize=4096serverPooledByteBufAllocatorEnable=trueuseEpollNativeSelector=false 通过修改listenPort在一台机器上部署启动两个nameserv nohup sh mqnamesrv -c mqnamesrv-a.conf &amp; 启动broker(默认10911)集群 broker的所用配置项 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122rocketmqHome=/usr/local/rocketmqnamesrvAddr=## 本机ip地址，默认系统自动识别brokerIP1=172.18.48.79brokerIP2=172.18.48.79brokerName=jiexiu’MacbrokerClusterName=DefaultClusterbrokerId=0brokerPermission=6defaultTopicQueueNums=8autoCreateTopicEnable=trueclusterTopicEnable=truebrokerTopicEnable=trueautoCreateSubscriptionGroup=truemessageStorePlugIn=sendMessageThreadPoolNums=32pullMessageThreadPoolNums=24adminBrokerThreadPoolNums=16clientManageThreadPoolNums=16flushConsumerOffsetInterval=5000flushConsumerOffsetHistoryInterval=60000rejectTransactionMessage=falsefetchNamesrvAddrByAddressServer=falsesendThreadPoolQueueCapacity=10000pullThreadPoolQueueCapacity=10000filterServerNums=0longPollingEnable=trueshortPollingTimeMills=1000notifyConsumerIdsChangedEnable=truehighSpeedMode=falsecommercialEnable=truecommercialTimerCount=1commercialTransCount=1commercialBigCount=1transferMsgByHeap=truemaxDelayTime=40regionId=DefaultRegionregisterBrokerTimeoutMills=6000slaveReadEnable=falsedisableConsumeIfConsumerReadSlowly=falseconsumerFallbehindThreshold=0waitTimeMillsInSendQueue=200startAcceptSendRequestTimeStamp=0listenPort=10911serverWorkerThreads=8serverCallbackExecutorThreads=0serverSelectorThreads=3serverOnewaySemaphoreValue=256serverAsyncSemaphoreValue=64serverChannelMaxIdleTimeSeconds=120serverSocketSndBufSize=131072serverSocketRcvBufSize=131072serverPooledByteBufAllocatorEnable=trueuseEpollNativeSelector=falseclientWorkerThreads=4clientCallbackExecutorThreads=4clientOnewaySemaphoreValue=65535clientAsyncSemaphoreValue=65535connectTimeoutMillis=3000channelNotActiveInterval=60000clientChannelMaxIdleTimeSeconds=120clientSocketSndBufSize=131072clientSocketRcvBufSize=131072clientPooledByteBufAllocatorEnable=falseclientCloseSocketIfTimeout=falsestorePathRootDir=/Users/zhangyanghong/storestorePathCommitLog=/Users/zhangyanghong/store/commitlogmapedFileSizeCommitLog=1073741824mapedFileSizeConsumeQueue=6000000flushIntervalCommitLog=1000flushCommitLogTimed=falseflushIntervalConsumeQueue=1000cleanResourceInterval=10000deleteCommitLogFilesInterval=100deleteConsumeQueueFilesInterval=100destroyMapedFileIntervalForcibly=120000redeleteHangedFileInterval=120000## 删除时间点，默认凌晨4点deleteWhen=04diskMaxUsedSpaceRatio=75## 文件保留时间，默认48小时fileReservedTime=72putMsgIndexHightWater=600000maxMessageSize=4194304checkCRCOnRecover=trueflushCommitLogLeastPages=4flushLeastPagesWhenWarmMapedFile=4096flushConsumeQueueLeastPages=2flushCommitLogThoroughInterval=10000flushConsumeQueueThoroughInterval=60000maxTransferBytesOnMessageInMemory=262144maxTransferCountOnMessageInMemory=32maxTransferBytesOnMessageInDisk=65536maxTransferCountOnMessageInDisk=8accessMessageInMemoryMaxRatio=40## 是否开启消息索引功能messageIndexEnable=truemaxHashSlotNum=5000000maxIndexNum=20000000maxMsgsNumBatch=64## 是否提供安全的消息索引机制，索引保证不丢messageIndexSafe=falsehaListenPort=10912haSendHeartbeatInterval=5000haHousekeepingInterval=20000haTransferBatchSize=32768haMasterAddress=haSlaveFallbehindMax=268435456## Broker的角色：ASYNC_MASTER异步复制Master; SYNC_MASTER同步双写MASTER; SLAVEbrokerRole=ASYNC_MASTERflushDiskType=ASYNC_FLUSHsyncFlushTimeout=5000messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2hflushDelayOffsetInterval=10000cleanFileForciblyEnable=truewarmMapedFileEnable=falseoffsetCheckInSlave=falsedebugLockEnable=falseduplicationEnable=falsediskFallRecorded=trueosPageCacheBusyTimeOutMills=1000defaultQueryMaxNum=32 其中重要的配置信息如下： 1234567891011121314151617181920212223242526namesrvAddr=brokerIP1=172.18.48.79brokerName=jiexiu’MacbrokerClusterName=DefaultClusterbrokerId=0autoCreateTopicEnable=trueautoCreateSubscriptionGroup=truerejectTransactionMessage=falsefetchNamesrvAddrByAddressServer=falsestorePathRootDir=/Users/zhangyanghong/storestorePathCommitLog=/Users/zhangyanghong/store/commitlogflushIntervalCommitLog=1000flushCommitLogTimed=falsedeleteWhen=04fileReservedTime=72maxTransferBytesOnMessageInMemory=262144maxTransferCountOnMessageInMemory=32maxTransferBytesOnMessageInDisk=65536maxTransferCountOnMessageInDisk=8accessMessageInMemoryMaxRatio=40messageIndexEnable=truemessageIndexSafe=falsehaMasterAddress=brokerRole=ASYNC_MASTERflushDiskType=ASYNC_FLUSHcleanFileForciblyEnable=true 启动broker sh mqbroker -n &#39;127.0.0.1:9876;127.0.0.1:9877&#39; -c ../conf/2m-noslave/broker-a.properties &gt; /dev/null 2&gt;&amp;1 &amp; 集群验证 sh mqadmin clusterList -n 127.0.0.1:9876 输出信息 123#Cluster Name #Broker Name #BID #Addr #Version #InTPS(LOAD) #OutTPS(LOAD) #PCWait(ms) #Hour #SPACEDefaultCluster broker-a 0 172.18.48.79:10911 V3_5_8 0.00(0,0ms) 0.00(0,0ms) 0 412299.47 0.5476DefaultCluster broker-b 0 172.18.48.79:12911 V3_5_8 0.00(0,0ms) 0.00(0,0ms) 0 412299.47 0.5476 ​ 默认的集群配置conf子目录下 1232m-2s-async // 两个master 两个slave异步2m-2s-sync // 两个master 两个slave同步2m-noslave // 两个master 没有slave broker 的master和slave (Slave 不可写，但可读) 单个master: 风险较大, 不建议生产使用 多master: 配置简单，消息可靠，性能最高 单台机器宕机期间，未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响 多master多slave，异步复制: 每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用异步复制方式，主备有短暂消息延迟，毫秒级 优点: 即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样 缺点: Master 宕机，磁盘损坏情况，会丢失少量消息 多master多slave，同步双写: 每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用同步双写方式，主备都写成功，向应用返回成功 优点: 数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高 缺点: 性能比异步复制模式略低，大约低 10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。 问题 出现 Lock failed,MQ already started 解决方案: 修改配置文件中的storePathRootDir项 启动 mqnamesrv / mqbroker服务报错，显示内存不够，需大于2G？具体表现: “VM warning: INFO: OS::commit_memory(0x00000006c0000000, 2147483648, 0) faild; error=’Cannot allocate memory’ (errno=12)” 解决方案：修改/RocketMQ/devnev/bin/ 下的服务启动脚本 runserver.sh 、runbroker.sh 中对于内存的限制，改成如下示例： JAVA_OPT=&quot;${JAVA_OPT} -server -Xms128m -Xmx128m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=128m&quot; ​ 启动rocketmq-consolemvn spring-boot:run or java -jar rocketmq-console-ng-1.0.0.jar --server.port=12581 --rocketmq.config.namesrvAddr=10.89.0.64:9876;10.89.0.65:9876 topicrocketmq中一个broker-name其实就相当于kafka-broker中的一个partition，而rocketmq每一个slave就相当于kafka中的一个replication，这种情况，所以rocketmq的特点相当于单个partition支持多队列，大致的原理图如下： 默认: 一个topic的队列数是8 问题 producer生产消息过多，customer来不及消费? 消息的重试机制 broker和client/producer版本不一致问题 解决: 会导致已消费消息堆积，重启customer会重复消费，更换一致版本 队列个数设置 producer发送消息时候设置，特别注意：同一个topic仅当第一次创建的时候设置有效，以后修改无效，除非修改broker服务器上的consume.json文件， demo：mqProducer.setDefaultTopicQueueNums(5) 参考：http://www.mamicode.com/info-detail-327693.html 其他常见问题3.2.4 RabbitMQ安装erlang 使用kerl安装和管理erlang，参考 Erlang版本管理工具: Kerl , 安装Erlang/OTP的简单方法, 其他安装方法 在CentOS上安装erlang 设置环境变量 安装 RabbitMQ 下载rabbit rpm包 错误 1234Error: Package: rabbitmq-server-3.7.2-1.el7.noarch (/rabbitmq-server-3.7.2-1.el7.noarch) Requires: erlang &gt;= 19.3 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest 执行 rpm --nodeps -ivh rabbitmq-server-3.7.2-1.el7.noarch.rpm 启动命令(/usr/lib/rabbitmq /etc/rabbitmq —- /usr/share/doc/rabbitmq-server-3.7.2) 12345service rabbitmq-server startservice rabbitmq-server stopservice rabbitmq-server restartservice rabbitmq-server status./rabbitmqctl stop 开启管理功能 rabbitmq-plugins enable rabbitmq_management 启动服务 rabbitmq-server -detached 添加用户权限 12rabbitmqctl add_user albert passwordrabbitmqctl set_user_tags albert administrator 开机自启动 chkconfig rabbitmq-server on 修改配置文件，开启远程用户访问 12cp /usr/share/doc/rabbitmq-server-3.7.2/rabbitmq.config.example /etc/rabbitmq/ mv rabbitmq.config.example rabbitmq.config 增加 {loopback_users, []} 集群部署 RabbitMQ 配置初步 Centos7 安装rabbitmq reference How-to-install-rabbitmq-on-centos-7 ​ 重要概念: 左侧 P 代表 生产者，也就是往 RabbitMQ 发消息的程序。 中间即是 RabbitMQ，其中包括了 交换机 和 队列。 右侧 C 代表 消费者，也就是往 RabbitMQ 拿消息的程序。 重要的概念：虚拟主机，交换机，队列，和绑定 虚拟主机：一个虚拟主机持有一组交换机、队列和绑定。为什么需要多个虚拟主机呢？很简单，RabbitMQ当中，用户只能在虚拟主机的粒度进行权限控制。 因此，如果需要禁止A组访问B组的交换机/队列/绑定，必须为A和B分别创建一个虚拟主机。每一个RabbitMQ服务器都有一个默认的虚拟主机“/”。 交换机：Exchange 用于转发消息，但是它不会做存储 ，如果没有 Queue bind 到 Exchange 的话，它会直接丢弃掉 Producer 发送过来的消息。这里有一个比较重要的概念：路由键 。消息到交换机的时候，交互机会转发到对应的队列中，那么究竟转发到哪个队列，就要根据该路由键。 绑定：也就是交换机需要和队列相绑定，这其中如上图所示，是多对多的关系。 交换机(Exchange)交换机的功能主要是接收消息并且转发到绑定的队列，交换机不存储消息，在启用ack模式后，交换机找不到队列会返回错误。交换机有四种类型：Direct, topic, Headers and Fanout Direct：direct 类型的行为是”先匹配, 再投送”. 即在绑定时设定一个 routing_key, 消息的routing_key 匹配时, 才会被交换器投送到绑定的队列中去. Topic：按规则转发消息（最灵活） Headers：设置header attribute参数类型的交换机 Fanout：转发消息到所有绑定队列 性能比较: RabbitMQ三种Exchange模式(fanout,direct,topic)的性能比较 理解rabbitmq的概念 : http://tryrabbitmq.com/ 示例代码: boot-in-action reference: 消息队列中间件调研文档 几款消息中间的调研 消息队列及常见消息队列介绍 Understanding When to use RabbitMQ or Apache Kafka 高吞吐、高可用MQ对比分析 Kafka、RabbitMQ、RocketMQ消息中间件的对比 —— 消息发送性能 消息队列设计精要 分布式开放消息系统(RocketMQ)的原理与实践 RabbitMQ详解 消息队列探秘-RabbitMQ消息队列介绍 RabbitMq延迟、重试队列及Spring Boot的黑科技 rabbitmq可靠发送的自动重试机制 ​]]></content>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链路监控浅析---以sleuth-zipkin和skywalking为例]]></title>
    <url>%2F2018%2F01%2F02%2F%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7%E6%B5%85%E6%9E%90---%E4%BB%A5sleuth-zipkin%E5%92%8Cskywalking%E4%B8%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[APM (Application Performance Management &amp; Monitoring)四种实现思路 基于日志系统，探针只负责对日志加上编号，又类似ELK的系统进行收集、处理、展示。这方面没有很成熟的产品，一般都属于公司内部封装的框架。 自动探针，适用语言：Java、C#、PHP、Node.js等等存在VM的语言。绝对大多数的商业产品和热门的开源产品都属于这个系列。 全手动探针，优势是适用范围广，最有名的就是Zipkin的整个生态系统，分布式追踪几乎无处不在。也是现在全球运用最广泛的分布式监控系统。 同时支持自动和手动模式的探针，适用语言同样是Java、C#、PHP、Node.js等等存在VM的语言，由于技术复杂性提高，运用的较少。优点是入门方便，同时使用灵活。商业上主要是Instana，开源主要是sky-walking提供了技术解决方案。 三大模块: 探针或sdk ：负责数据采集和发送。探针或 SDK 是应用程序的收集端。一般使用插件的模式，自动探针一般是不需要修改程序，而 SDK 则是需要修改部分配置或者代码。skywalking 就是自动探针为主，zipkin-brave 就是 Zipkin 的 Java 手动探针 collector模块 ：负责数据收集、分析、汇总、告警和存储。Collector 模块，这个根据不同的 APM 实现，可能由一个或者多个子系统构成。Collector 负责对探针和 SDK 提供网络接口（TCP、UDP、HTTP 不同形式接口） UI ，负责高实时性展现。包括但不限于 Trace 的查询，统计数据展现，拓扑图展现，VM 或进程相关信息等，监控关键数据的展现 监控概述-sleuth实现 Sleuth-Zipkinsleuth用来和zipkin(twitter)集成，自动完成span, trace等信息的生成，接入http request, 以及向zipkin server 发送采集信息，实现分布式服务跟踪能力。全链路spring cloud sleuth+zipkin 结构图 搭建zipkin-server 引入 123456789&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 添加注解 @EnableZipkinServer rest服务调用建立两个基本的rest服务，不再赘述 引入 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置 spring.zipkin.base-url=http://127.0.0.1:9418 启动 LOG: aplication-name.TraceId.SpanId.BOOL 其他配置1spring.sleuth.sampler.percentage=1 数据持久化 zipkin-server引入 12345678910111213141516&lt;!--此依赖会自动引入spring-cloud-sleuth-stream并且引入zipkin的依赖包(可以去除zipkin-server) --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--持久化数据到elasticsearch--&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch-http&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; client引入 none 注解修改 @EnableZipkinStreamServer ​ 问题 4.1. 引入es后不能显示dependency tree zipkin-dependencies 12wget -O zipkin-dependencies.jar 'https://search.maven.org/remote_content?g=io.zipkin.dependencies&amp;a=zipkin-dependencies&amp;v=LATEST'STORAGE_TYPE=elasticsearch ES_HOSTS=host1,host2 java -jar zipkin-dependencies.jar 概念 Span ：基本工作单元，发送一个远程调度任务 就会产生一个Span，Span是一个64位ID唯一标识的，Trace是用另一个64位ID唯一标识的，Span还有其他数据信息，比如摘要、时间戳事件、Span的ID、以及进度ID Trace ：一系列Span组成的一个树状结构。请求一个微服务系统的API接口，这个API接口，需要调用多个微服务，调用每个微服务都会产生一个新的Span，所有由这个请求产生的Span组成了这个Trace。 Annotation ：用来及时记录一个事件的，一些核心注解用来定义一个请求的开始和结束 。这些注解包括以下： cs - Client Sent -客户端发送一个请求，这个注解描述了这个Span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络传输的时间 ss - Server Sent （服务端发送响应）–该注解表明请求处理的完成(当请求返回客户端)，如果ss的时间戳减去sr时间戳，就可以得到服务器请求的时间。 cr - Client Received （客户端接收响应）-此时Span的结束，如果cr的时间戳减去cs时间戳便可以得到整个请求所消耗的时间。 Log: span跨度内发生的事件，异常等，包含tag, annotation, log, event等 sky-walking提供分布式事务跟踪，以及APM性能监控。 skywalking， 使用javaagent技术使得应用监控0耦合 架构 部署 下载agent, collector, ui 三个组件 启动collector, ui 修改application-name, 启动agent -javaagent:/path/to/skywalking-agent/skywalking-agent.jar 其他的一些APM工具: Pinpoint, CAT, Xhprof/Xhgui]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS linux(AMI) 搭建VPN]]></title>
    <url>%2F2016%2F11%2F02%2FAWS%20linux(AMI)%20%E6%90%AD%E5%BB%BAVPN%2F</url>
    <content type="text"><![CDATA[AWS提供一年的免费试用($2.00)，试着在amazon linux(AMI) 上搭建vpn，简要记录一下搭建过程。 安装过程如下： ssh配置 给pem权限 chmod 400 data_korea.pem 登录ec2 ssh -i &quot;data_korea.pem&quot; ec2-user@ec2-52-78-70-0.ap-northeast-2.compute.amazonaws.com 修改默认用户和root密码 sudo passwd ec2-user suod paddwd root 切换root用户，修改文件 su root vim /etc/ssh/sshd_config 123PermitRootLogin yesPubkeyAuthentication noPasswordAuthentication yes reboot或者重启ssh /etc/init.d/sshd restart vpn安装 安装ppp yum install ppp 下载并安装pptpd wget http://poptop.sourceforge.net/yum/stable/packages/ppp-2.4.5-33.0.fc21.x86_64.rpm wget http://poptop.sourceforge.net/yum/stable/packages/pptpd-1.4.0-1.el6.x86_64.rpm rpm -Uhv pptpd*.rpm 添加DNS服务器 (可选) 打开vim /etc/ppp/options.pptpd 文件并添加入如下内容： ms-dns 8.8.8.8 ms-dns 8.8.4.4 ms-dns 4.4.4.4 以上2个是Google提供的免费DNS 添加 VPN 帐号 在/etc/ppp/chap-secrets文件中添加VPN用户，格式为“用户名 服务器 密码 IP地址”： vpnuser pptpd myVPN$99 * 打开IP转发(IP Forward)功能 在 /etc/sysctl.conf 文件中修改： net.ipv4.ip_forward = 1 保存设置 sysctl -p 在 IP Tables 中开启 IP 伪装(IP Masquerade) iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 如果你需要这个设置在重启之后依然有效，则需要把这一行添加到 /etc/rc.local 的末尾。 把 pptpd 设置成自动运行的 chkconfig pptpd on 重启pptpd服务 service pptpd restart ec2控制台打开TCP的1723端口，这是pptpd的默认连接端口。 Reference：https://leonax.net/p/3274/install-vpn-server-on-amazon-ec2/ AMI 软件更新 jdk 更新 12345rpm -qa|grep java //查询系统jdkrpm -e --allmatches --nodeps java-1.6.0-openjdk-1.6.0.37-1.13.9.4.el5_11 //删除老版本yum -y list java* (yum search jdk) //查询软件包内的jdkyum install java-1.8.0-openjdk.x86_64 //安装新版本java -version //验证 jdk的安装路径加入到JAVA_HOME vi /etc/profile 12345#set java environmentJAVA_HOME=/usr/lib/jvm/jre-1.6.0-openjdk.x86_64PATH=$PATH:$JAVA_HOME/binCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME CLASSPATH PATH . /etc/profile]]></content>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL定时任务]]></title>
    <url>%2F2016%2F08%2F04%2FMySQL%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[简介数据定时更新非常有必要，自MySQL5.1.6起，增加了事件调度器，可以用来执行某些定时任务。简要记录一下创建过程。 配置 windows10 MySQL5.6 过程开启event_scheduler set global event_scheduler = 1; my.cnf 加上 event_scheduler = 1 set global event_scheduler = ON; mysqld --event_scheduler=1; 查看是否开启了event_scheduler show varuables like &#39;event_scheduler&#39;; select @@event_scheduler; show processlist; 创建事件(create event)语法123456CREATE EVENT [IFNOT EXISTS] event_nameONSCHEDULE schedule[ONCOMPLETION [NOT] PRESERVE][ENABLE | DISABLE][COMMENT &apos;comment&apos;]DO sql_statement; schedual: 12AT TIMESTAMP [+ INTERVAL INTERVAL]| EVERY INTERVAL [STARTS TIMESTAMP] [ENDS TIMESTAMP] INTERVAL: 123quantity &#123;YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE |WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE |DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND&#125; 示例 每秒插入一条记录 12345USE tarena;CREATE TABLE aaa (timeline TIMESTAMP);CREAT EEVENT e_test_insertON SCHEDULE EVERY 1 SECONDDO INSERTINTO tarena.aaa VALUES(CURRENT_TIMESTAMP); 5天后清空表 123CREATE EVENT e_testON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 5 DAYDO TRUNCATETABLE tarena.aaa; 预约清空 123CREATE EVENT e_testON SCHEDULE AT TIMESTAMP &apos;2007-07-20 12:00:00&apos;DO TRUNCATETABLE tarena.aaa; 定时清空 123CREATE EVENT e_testON SCHEDULE EVERY 1 DAYDO TRUNCATETABLE tarena.aaa; 预约定时清空 1234CREATE EVENT e_testONSCHEDULE EVERY 1 DAYSTARTS CURRENT_TIMESTAMP+ INTERVAL 5 DAYDO TRUNCATETABLE tarena.aaa; 定时清空，一段时间后停止 1234CREATE EVENT e_testON SCHEDULE EVERY 1 DAYENDS CURRENT_TIMESTAMP+ INTERVAL 5 DAYDO TRUNCATETABLE test.aaa; 预约定时清空，一段时间后停止 12345CREATE EVENT e_testON SCHEDULE EVERY 1 DAYSTARTS CURRENT_TIMESTAMP+ INTERVAL 5 DAYENDS CURRENT_TIMESTAMP+ INTERVAL 1 MONTHDO TRUNCATETABLE test.aaa; 定时清空，执行一次后终止[ON COMPLETION [NOT] PRESERVE]可以设置这个事件是执行一次还是持久执行，默认为NOT PRESERVE。 1234CREATE EVENT e_testON SCHEDULE EVERY 1 DAYON COMPLETION NOT PRESERVEDO TRUNCATETABLE test.aaa; [ENABLE | DISABLE]可是设置该事件创建后状态是否开启或关闭，默认为ENABLE。 [COMMENT ‘comment’]可以给该事件加上注释。 修改事件(ALTER EVENT)语法1234567ALTER EVENT event_name[ONSCHEDULE schedule][RENAME TOnew_event_name][ONCOMPLETION [NOT] PRESERVE][COMMENT &apos;comment&apos;][ENABLE | DISABLE][DO sql_statement] 临时关闭事件ALTER EVENT e_test DISABLE; 开启事件ALTER EVENT e_test ENABLE; 时间点修改ALTER EVENT e_test ON SCHEDULE EVERY 5 DAY; 删除事件(DROP EVENT)语法DROP EVENT [IF EXISTS] event_name 案例123456789101112delimiter //create procedure `Slave_Monitor`()beginSELECT VARIABLE_VALUE INTO @SLAVE_STATUSFROM information_schema.GLOBAL_STATUSWHERE VARIABLE_NAME=&apos;SLAVE_RUNNING&apos;;IF (&apos;ON&apos;!= @SLAVE_STATUS) THENSET GLOBAL SQL_SLAVE_SKIP_COUNTER=0;SLAVE START;END IF;end; //delimiter ; 1234CREATE EVENT IFNOT EXISTS `Slave_Monitor`ON SCHEDULE EVERY 5 SECONDON COMPLETION PRESERVEDO CALL Slave_Monitor();]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Linux命令查看硬件信息]]></title>
    <url>%2F2016%2F07%2F20%2F%E4%BD%BF%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[在linux检查和查看硬件信息有分很多命令，这里列出一些命令快速查看linux的cpu和内存等信息。 lscpu 直接使用即可，没有多余的选项和功能 lspci 可以列出所有连接到PCI总线的详细信息，例如：显卡，网卡，USB接口及SATA控制器等设备。 ​ 可以使用类似如下命令过滤出特定的设备信息 1lspci -v | grep &quot;VGA&quot; -A 12 lshw 通用工具，可以执行多个硬件如CPU，硬件，USB控制器及磁盘等详细信息。在执行之后会自动提取不同”/proc”文件中的信息。 lsusb 显示连接到此计算机的USB控制器的详细信息，可以使用-v选项来输出每个usb端口的详细信息。 lnxi 用来获取多项目硬件信息的脚本工具，可以为用户输入一个详细的硬件报告，默认未安装在ubuntu系统当中，可以使用如下命令安装： sudo apt-get install inxi使用 inxi -Fx 输出硬件报告 df 输出当前Linux系统中个各种分区及其挂载点，可以使用-H参数df -H free 查看当前系统的内存信息free -m dmidecode 主要通过读取DMI表中数据来提取硬件信息。查看CPU信息sudo dmidecode -t processor 查看内存信息sudo dmidecode -t memory 查看BIOS信息 sudo dmidecode -t bios hdparm 读取SATA设备（eg.硬盘）的相关信息sudo hdparm]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window添加sublime右键菜单]]></title>
    <url>%2F2016%2F06%2F28%2Fwindow%E6%B7%BB%E5%8A%A0sublime%E5%8F%B3%E9%94%AE%E8%8F%9C%E5%8D%95%2F</url>
    <content type="text"><![CDATA[1.打开注册表编辑器，开始-&gt;运行-&gt;regedit。 2.在HKEY_CLASSSES_ROOT→ * → Shell 下，在Shell下，新建项命名为Open With Sublime Text，在该新建项的右边窗口新建字符串值（右键–新建–字符串值）。名称：Icon；值：D:\Program Files\Sublime Text 3\sublime_text.exe,0 【注：使用您自己的安装文件目录】。 3.在新建的项Open With Sublime Text下面新建项Command（必须这个名称）.修改Command项右侧窗口的默认值，修改为：”D:\Program Files\Sublime Text 3\sublime_text.exe” “%1”【注：使用您自己的安装文件目录】，双引号一定要加，否则无法打开路径带空格的文件，这样就大功告成了。]]></content>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL修改密码]]></title>
    <url>%2F2016%2F06%2F27%2FMySQL%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[记录MySQL修改密码的几种方式 用SET PASSWORD命令首先登录MySQL。格式：mysql&gt; set password for 用户名@localhost = password(‘新密码’);例子：mysql&gt; set password for root@localhost = password(‘123’); 用mysqladmin格式：mysqladmin -u用户名 -p旧密码 password 新密码例子：mysqladmin -uroot -p123456 password 123 用UPDATE直接编辑user表首先登录MySQL。mysql&gt; use mysql;mysql&gt; update user set password=password(‘123’) where user=’root’ and host=’localhost’;mysql&gt; flush privileges; 在忘记root密码的时候，可以这样以windows为例： 关闭正在运行的MySQL服务。 打开DOS窗口，转到mysql\bin目录。 输入mysqld –skip-grant-tables 回车。–skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\bin目录。 输入mysql回车，如果成功，将出现MySQL提示符 &gt;。 连接权限数据库： use mysql; 。 改密码：update user set password=password(“123”) where user=”root”;（别忘了最后加分号） 。 刷新权限（必须步骤）：flush privileges; 。 退出 quit。 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo同步]]></title>
    <url>%2F2016%2F06%2F22%2Fhexo%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[简要介绍一下hexo搭建的github page同步过程。 我已经在github上建立了hexo的源码分支hexo，以及主页分支master。过程： 准备工作 git(Cygwin)，nodejs(win10)安装 依次执行的命令 git clone -b hexo git@github.com:silloy/silloy.github.io.git hexo cd hexo npm install -g hexo-cli npm install npm install hexo-deployer-git npm install -g npm-check npm-check -u （npm-check -u -g） 安装其他依赖包 https://github.com/theme-next/theme-next-canvas-nest https://github.com/theme-next/theme-next-fancybox3 https://github.com/theme-next/theme-next-pace 参考资料： 使用hexo，如果换了电脑怎么更新博客？]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vm环境安装linuxmint]]></title>
    <url>%2F2016%2F05%2F14%2Fvm%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85linuxmint%2F</url>
    <content type="text"><![CDATA[选择轻量级Linux系统linuxmint搭建需要，做本地测试 准备工作 linux mint下载：linuxmint-17.3-kde-64bit.iso vmvare workstation 12 player 安装 安装过程不再叙述，全程傻瓜式安装 基本设置 分辨率，壁纸， 语言 卸载装机软件 libreoffice sudo apt-get purge libreoffice?or sudo aptitude purge libreoffice?or sudo apt-get remove --purge libreoffice* 设置软件源，并更新系统 sudo apt-get update sudo apt-get upgrade main选择： ustc base选择： aliyun 浏览器主页，搜索引擎设置安装GuakeTerminal sudo apt-get install guake docs: GuakeTerminal──linux下完美帅气的终端 安装VMWARE tools安装输入法(不适用于linuxmint18) sudo add-apt-repository ppa:fcitx-team/nightly sudo aptitude update sudo aptitude install fcitx fcitx-sogoupinyin fcitx-config-gtk fcitx-frontend-all fcitx-module-cloudpinyin fcitx-ui-classic 下载sogou for linux , 打开im-config设置fcitx，安装sougou reboot 配置文件（注意取消only show current language） 安装markdown编辑器 推荐 cmd_markdown 开发常用软件 docs：Linux mint 安装JAVA jdk jre sudo apt-get install default-jre open-jdk sudo apt-get install default-jdk jdk sudo add-apt-repository ppa:webupd8team/java sudo apt-get update sudo apt-get install oracle-java8-installer sudo apt-get install oracle-java8-set-default docs： 怎样在Ubuntu 14.04中安装JavaLinux下配置Java环境变量]]></content>
      <tags>
        <tag>linux-mint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle基础命令]]></title>
    <url>%2F2016%2F05%2F09%2FOracle%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[主流关系型数据库: Oracle，DB2，Sybase，SQL server，MySQL这里主要记录一下oracle数据库初步学习过程，涉及少量与mysql的不同讨论。基本的SQL知识 SQL（Structured Query Language）常用关键字 SQL可分为 数据定义语言DDL: CREATE, ALTER, DROP, TRUNCATE 数据操纵语言DML: INSERT, UPDATE, DELETE 事务控制语言TCL: COMMIT, ROLLBACK, SAVEPOINT 数据查询语言DQL: SELECT 数据控制语言DCL: GRANT, REVOKE, CREATE USER NUMBER: 数字类型 定义: NUMBER（P，S） FYI: MySQL里没有number类型，对应的是int，float…… CHAR: 固定长度的字符类型 定义: CHAR(N) 最多保存2000字节 FYI: 对应MySQL里的CHAR VARCHAR2: 变长的字符类型 定义: VARCHAR2(N) 最多保存4000字节 FYI: 对应MySQL里的VARCHAR DATE: 日期时间数据 默认格式: DD-MON-RR DEFAULT: 指定默认值 NOT NULL: 非空约束 DROP 删除列或表 LONG: VARCHAR2加长版 限制: 每个表只能有一个long列，不能作为主键，不能建立索引，不能出现在查询条件中 FYI: MySQL没有long型 CLOB: 存储定长或变长字符串，oracle建议使用clob替代long型 执行DML操作后，需要在执行commit，才算真正确认了此操作 常用函数 UPPER, LOWER, INITCAP: 大小写转换 TRIM, LTRIM, RTRIM: 截去子串 LPAD, RPAD: 补位函数 LPAD(char1, n, char2) RPAD(char1, n, char2) SUBSTR: 获取子串 SUBSTR(char, m[, n]) INSTR: 返回子串在父串中的位置 INSTR(char1, char2[, n[, m]]) n: 开始搜索的位置，默认为1 m: 指定子串第m次出现，默认为1 没找到，返回0 ROUND：四舍五入 ROUND(n[, m]) n: 可以是任何数字，指要被处理的数字 m：必须是整数，四舍五入的位数，可以是负数，默认是0 MOD: 取余 MOD(m, n) n为0直接返回m CEIL CEIL(n) FLOOR FLOOR(n) CONCAT, “||” 日期操作 DATE 范围：公元前4712年1月1日至公元9999年12月31日 占用7个字节 byte 1: 世纪+100 byte 2: 年 byte 3: 月 byte 4: 日 byte 5: 小时 + 1 byte 6: 分 + 1 byte 7: 秒 + 1 TIMESTAMP 最高精度可以到ns 占用7或者11个字节，精度为0，用7字节存储，精度大于0用11字节存储 精度：第8-11字节，内部运算类型为整型 SYSDATE: SystemDate SYSTIMESTAMP: 返回当前系统日期和时间，精确到毫秒 TO_DATE：字符串转换为日期类型 TO_DATE(char[, fmt[, nlsparams]]) fmt: 格式 nlsparams: 指定日期语言 TO_CHAR TO_CHAR(date[, fmt[, nlsparams]]) LAST_DAY LAST_DAY(date): 返回日期date所在月的最后一天 ADD_MONTHS ADD_MONTHS(date, i) MONTHS_BETWEEN MONTHS_BETWEEN(date1, date2): 计算月份差 NEXT_DAY NEXT_DAY(date, char): 返回date日期的下一个周几 LEAST, GREATEST GREATEST(expr1[,expr2[, expr3]]…) EXTRACT EXTRACT(date from datetime): 提取日期中的年、月、日等 空值操作 NVL NVL(expr1, expr2): expr1为null则转变为expr2，数据类型必须一致 NVL2 NVL2(expr1, expr2, expr3): expr1非null返回我想expr2，为null返回expr3 高级查询子查询分页查询 ROWNUM：伪列，用于返回标识行数据顺序的数字 只能从1计数，不能从结果集中直接截取 部分数据需要用到行内视图 PageN: (n - 1) * pageSize + 1 ~ n * pageSize DECODE函数 DECODE(expr, search1, result1[,search2, result2….][, default]) 用途： 比较expr的值，若匹配到哪一个search，返回对应result，类似于case语句 应用场景： 分组查询 12SELECT DECODE(job, &apos;ANALYST&apos;, &apos;VIP&apos;, &apos;MANAGER&apos;, &apos;VIP&apos;, &apos;OPERATION&apos;)job, COUNT(1)job_count FROM emp GROUP BY DECODE(job, &apos;ANALYST&apos;, &apos;VIP&apos;, &apos;MANAGER&apos;, &apos;VIP&apos;, &apos;OPERATION&apos;); 字段内容排序ORDER BY DECODE(job, &#39;ANALYST&#39;, &#39;1&#39;, &#39;MANAGER&#39;, &#39;2&#39;, &#39;OPERATION&#39;, &#39;3&#39;) ROW_NUMBER函数 ROW_NUMBER() OVER(PARTITION BY col1 ORDER BY col2): 根据col1分组，在分组内部根据col2排序 此函数计算的值标识每组内部排序后的顺序编号，组内连续且唯一 RANK_NUMBER函数 类似于ROW_NUMBER, 跳跃排序，可以有重复值 DENSE_RANK 类似于RANK_NUMBER, 连续排序，有重复，无跳跃 UNION UNION: 自动合并去重，排序 UINION ALL：合并不去重，不排序 INTERSECT：交集 MINUS：差集 高级分组函数 ROLLUP: 从右向左以一次少一列的方式组合直到所有列都去掉 GROUP BY ROLLUP(a, b, c) CUBE: 所以维度的取值集合 GROUP BY CUBE(a, b, c) GROUPING SETS: GROUP BY GROUPING SETS(a, b, c) 视图 视图(VIEW) 也称虚表，是一组数据的逻辑表示，对应于一条select语句 分类： 简单视图 复杂视图 连接视图 作用： 简化复杂查询 限制数据访问 GRANT CREATE VIEW TO user; 限制约束 [WITH CHECK OPTION] [WITH READ ONLY] 序列 创建序列 CREATE SEQUENCE[schema.]sequence_name[START WITH i][INCREMENT BY j][MAXVALUE m | NOMAXVALUE][MINVALUE n | NOMINVALUE][CYCLE | NOCYCLE][CACHE p | NOCACHE] 使用序列 NEXTVAL: 获取序列的下个值 CURRVAL：获取序列的当前值 索引 索引(INDEX)是一种允许直接访问数据表中某一数据行的数据结构，为了提高查询效率而引入，是独立于表的对象，可以存放与表不同的表空间中 Syntax： CREATE [UNIQUE] INDEX index_name ON table(column[, column…]); UNIQUE表示唯一索引 约束 非空约束 NOT NULL 唯一性约束 UNIQUE 主键约束 PRIMARY KEY 外键约束 FOREIGN KEY 检查约束 CHECK]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
</search>
